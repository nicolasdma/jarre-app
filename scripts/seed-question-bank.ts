/**
 * Jarre - Seed Question Bank
 *
 * ~270 curated factual questions across 66 concepts.
 * Generated by Claude 4.6, not via DeepSeek API.
 *
 * Run with: npx tsx scripts/seed-question-bank.ts
 *
 * Requires SUPABASE_URL and SUPABASE_SERVICE_ROLE_KEY in .env.local
 */

import { createClient } from '@supabase/supabase-js';
import * as dotenv from 'dotenv';
import { resolve } from 'path';

dotenv.config({ path: resolve(__dirname, '../.env.local') });

const supabaseUrl = process.env.NEXT_PUBLIC_SUPABASE_URL;
const supabaseKey = process.env.SUPABASE_SECRET_KEY;

if (!supabaseUrl || !supabaseKey) {
  console.error('Missing NEXT_PUBLIC_SUPABASE_URL or SUPABASE_SECRET_KEY in .env.local');
  process.exit(1);
}

const supabase = createClient(supabaseUrl, supabaseKey);

// ============================================================================
// TYPES
// ============================================================================

interface SeedQuestion {
  concept_id: string;
  type: 'definition' | 'fact' | 'property' | 'guarantee' | 'complexity' | 'comparison';
  question_text: string;
  expected_answer: string;
  difficulty: 1 | 2 | 3;
  related_concept_id?: string;
}

// ============================================================================
// QUESTIONS
// ============================================================================

const questions: SeedQuestion[] = [
  // ==========================================================================
  // PHASE 1: Distributed Systems Fundamentals
  // ==========================================================================

  // --- reliability ---
  {
    concept_id: 'reliability',
    type: 'definition',
    question_text: '¿Qué significa que un sistema sea "reliable"? ¿Es lo mismo que ser "fault-free"?',
    expected_answer: 'Un sistema reliable sigue funcionando correctamente incluso cuando ocurren fallas (faults). No es lo mismo que fault-free: un sistema reliable es fault-tolerant, acepta que las fallas ocurren y las maneja.',
    difficulty: 1,
  },
  {
    concept_id: 'reliability',
    type: 'property',
    question_text: '¿Cuáles son los tres tipos principales de faults que afectan la reliability?',
    expected_answer: 'Hardware faults (disco falla, RAM corrupta), software faults (bugs, cascading failures), y faults humanos (errores de configuración, deploys malos). Los faults humanos son la causa más común.',
    difficulty: 2,
  },
  {
    concept_id: 'reliability',
    type: 'fact',
    question_text: '¿Cuál es la diferencia entre fault, error y failure?',
    expected_answer: 'Un fault es el componente que se desvía de su spec. Un error es el estado incorrecto del sistema causado por un fault. Un failure es cuando el sistema como un todo deja de proveer el servicio esperado al usuario.',
    difficulty: 2,
  },
  {
    concept_id: 'reliability',
    type: 'comparison',
    question_text: '¿En qué se diferencia la reliability de la availability?',
    expected_answer: 'Reliability es la probabilidad de que el sistema funcione correctamente durante un período. Availability es la fracción de tiempo que el sistema está operativo. Un sistema puede estar available (responde) pero no reliable (responde incorrectamente).',
    difficulty: 2,
    related_concept_id: 'slos-slis',
  },

  // --- scalability ---
  {
    concept_id: 'scalability',
    type: 'definition',
    question_text: '¿Qué es scalability y cómo se mide?',
    expected_answer: 'Scalability es la capacidad de un sistema de manejar carga creciente agregando recursos. Se mide en términos de load parameters específicos al sistema: requests/segundo, volumen de datos, usuarios concurrentes, etc.',
    difficulty: 1,
  },
  {
    concept_id: 'scalability',
    type: 'property',
    question_text: '¿Cuál es la diferencia entre scaling vertical y horizontal?',
    expected_answer: 'Scaling vertical (scale up) es usar una máquina más potente. Scaling horizontal (scale out) es distribuir la carga entre múltiples máquinas. Horizontal es más flexible pero introduce complejidad de sistemas distribuidos.',
    difficulty: 1,
  },
  {
    concept_id: 'scalability',
    type: 'fact',
    question_text: '¿Qué son los percentiles de latencia y por qué p99 importa más que el promedio?',
    expected_answer: 'Los percentiles muestran la distribución de tiempos de respuesta. p99 indica que el 99% de requests son más rápidos que ese valor. Importa más que el promedio porque los usuarios más lentos suelen ser los que tienen más datos (más valiosos) y porque un request lento en un servicio fan-out afecta toda la respuesta.',
    difficulty: 2,
  },
  {
    concept_id: 'scalability',
    type: 'comparison',
    question_text: '¿Cuál es la diferencia entre throughput y latency como load parameters?',
    expected_answer: 'Throughput es la cantidad de trabajo procesada por unidad de tiempo (requests/seg, records/seg). Latency es el tiempo que tarda un request individual. Pueden ser inversamente correlacionados: optimizar para throughput (batching) puede aumentar la latency individual.',
    difficulty: 2,
    related_concept_id: 'tail-latency',
  },

  // --- maintainability ---
  {
    concept_id: 'maintainability',
    type: 'definition',
    question_text: '¿Cuáles son los tres principios de diseño de la maintainability según DDIA?',
    expected_answer: 'Operability (fácil de operar para el equipo de ops), Simplicity (manejo de complejidad, abstracciones claras), y Evolvability (facilidad de hacer cambios futuros, también llamada extensibility).',
    difficulty: 1,
  },
  {
    concept_id: 'maintainability',
    type: 'property',
    question_text: '¿Por qué la simplicidad es crucial para la maintainability de un sistema?',
    expected_answer: 'La complejidad accidental dificulta entender el sistema, aumenta bugs y hace los cambios más riesgosos. La simplicidad se logra con buenas abstracciones que esconden detalles de implementación sin ocultar semántica importante.',
    difficulty: 2,
  },

  // --- data-models ---
  {
    concept_id: 'data-models',
    type: 'definition',
    question_text: '¿Cuáles son los tres modelos de datos principales y qué operaciones facilita cada uno?',
    expected_answer: 'Relacional: bueno para joins, datos normalizados, relaciones many-to-many. Document: bueno para datos jerárquicos, self-contained, schema flexible. Graph: bueno para relaciones complejas y queries de conectividad (N grados de separación).',
    difficulty: 1,
  },
  {
    concept_id: 'data-models',
    type: 'property',
    question_text: '¿Qué es el impedance mismatch entre modelo de datos y código de aplicación?',
    expected_answer: 'Es la desconexión entre cómo se almacenan los datos (tablas relacionales) y cómo se usan en código (objetos/structs). Los ORMs intentan resolver esto pero agregan complejidad. Los document DBs reducen el mismatch para datos jerárquicos.',
    difficulty: 2,
  },
  {
    concept_id: 'data-models',
    type: 'comparison',
    question_text: '¿Cuándo conviene un modelo de documentos vs relacional?',
    expected_answer: 'Documentos: cuando los datos son self-contained (un perfil de usuario completo), se acceden juntos, y no hay muchos joins entre documentos. Relacional: cuando hay muchas relaciones many-to-many, se necesitan joins complejos, o los datos se acceden de formas impredecibles.',
    difficulty: 2,
    related_concept_id: 'storage-engines',
  },

  // --- storage-engines ---
  {
    concept_id: 'storage-engines',
    type: 'definition',
    question_text: '¿Cuáles son los dos tipos principales de storage engines y su trade-off fundamental?',
    expected_answer: 'Log-structured (LSM trees): optimizados para writes, append-only. Page-oriented (B-trees): optimizados para reads, in-place updates. El trade-off es write amplification vs read amplification.',
    difficulty: 1,
  },
  {
    concept_id: 'storage-engines',
    type: 'property',
    question_text: '¿Qué es un LSM tree y cómo funciona la compactación?',
    expected_answer: 'LSM tree escribe primero a un memtable en memoria, luego flush a disco como SSTable ordenada. La compactación merge-sortea múltiples SSTables para eliminar duplicados y deletes. Trade-off: mejor write throughput pero reads pueden necesitar buscar en múltiples SSTables.',
    difficulty: 2,
  },
  {
    concept_id: 'storage-engines',
    type: 'complexity',
    question_text: '¿Cuál es la complejidad de búsqueda en un B-tree vs un LSM tree?',
    expected_answer: 'B-tree: O(log n) para lectura, garantizado. LSM tree: O(log n) por cada SSTable, y puede haber múltiples niveles de SSTables. En el peor caso, LSM lee de todos los niveles, pero Bloom filters mitigan esto significativamente.',
    difficulty: 3,
  },
  {
    concept_id: 'storage-engines',
    type: 'fact',
    question_text: '¿Qué es write amplification y por qué importa?',
    expected_answer: 'Write amplification es la ratio entre bytes escritos a disco vs bytes escritos por la aplicación. En B-trees, un update puede reescribir una página entera. En LSM trees, la compactación reescribe datos múltiples veces. Afecta el desgaste de SSDs y throughput.',
    difficulty: 2,
  },

  // --- replication ---
  {
    concept_id: 'replication',
    type: 'definition',
    question_text: '¿Cuáles son los tres modelos principales de replicación?',
    expected_answer: 'Leader-based (single leader): un nodo acepta writes, los réplicas copian. Multi-leader: múltiples nodos aceptan writes. Leaderless: cualquier nodo acepta writes, se usan quorums para consistencia.',
    difficulty: 1,
  },
  {
    concept_id: 'replication',
    type: 'property',
    question_text: '¿Qué es la replicación síncrona vs asíncrona y sus trade-offs?',
    expected_answer: 'Síncrona: el leader espera confirmación del follower antes de confirmar al cliente. Garantiza durabilidad pero aumenta latencia. Asíncrona: el leader confirma sin esperar followers. Menor latencia pero riesgo de data loss si el leader falla.',
    difficulty: 2,
  },
  {
    concept_id: 'replication',
    type: 'fact',
    question_text: '¿Qué problemas causa el replication lag en sistemas eventualmente consistentes?',
    expected_answer: 'Read-after-write inconsistency (escribes algo y no lo ves al releer), monotonic read violations (lees un valor nuevo y luego uno viejo), y consistent prefix violations (ves efectos antes de sus causas).',
    difficulty: 2,
  },
  {
    concept_id: 'replication',
    type: 'comparison',
    question_text: '¿Cuándo usarías multi-leader vs single-leader replication?',
    expected_answer: 'Multi-leader cuando hay múltiples datacenters (latencia de escritura), apps offline-first (cada dispositivo es un leader), o collaborative editing. Single-leader para la mayoría de los casos: más simple, sin conflictos de escritura. Multi-leader requiere conflict resolution.',
    difficulty: 3,
    related_concept_id: 'consistency-models',
  },

  // --- partitioning ---
  {
    concept_id: 'partitioning',
    type: 'definition',
    question_text: '¿Qué es partitioning y cuáles son las dos estrategias principales?',
    expected_answer: 'Partitioning (sharding) divide un dataset grande entre múltiples máquinas. Estrategias: por key range (permite range queries eficientes pero riesgo de hot spots) y por hash (distribución más uniforme pero pierde orden).',
    difficulty: 1,
  },
  {
    concept_id: 'partitioning',
    type: 'property',
    question_text: '¿Qué es un hot spot en partitioning y cómo se mitiga?',
    expected_answer: 'Un hot spot es una partición que recibe desproporcionadamente más tráfico. Se mitiga con hash partitioning, splitting de particiones, o agregando un random suffix a keys populares para distribuir la carga.',
    difficulty: 2,
  },
  {
    concept_id: 'partitioning',
    type: 'fact',
    question_text: '¿Qué problema introduce el rebalancing de particiones?',
    expected_answer: 'Al redistribuir datos entre nodos, se genera tráfico de red y I/O masivo. Técnicas: fixed number of partitions, dynamic splitting, proportional to nodes. Importante: no usar hash mod N porque mover un nodo redistribuye casi todos los datos.',
    difficulty: 2,
  },

  // --- distributed-failures ---
  {
    concept_id: 'distributed-failures',
    type: 'definition',
    question_text: '¿Qué tipos de fallas son característicos de los sistemas distribuidos?',
    expected_answer: 'Network partitions (nodos no pueden comunicarse), node crashes, clock skew (relojes desincronizados), Byzantine faults (nodos que mienten). La diferencia clave: las fallas parciales son la norma, no la excepción.',
    difficulty: 1,
  },
  {
    concept_id: 'distributed-failures',
    type: 'property',
    question_text: '¿Por qué no se puede distinguir entre un nodo caído y uno con red lenta?',
    expected_answer: 'En una red asíncrona, no hay límite de tiempo garantizado para la entrega de mensajes. Un timeout solo indica que no hubo respuesta a tiempo, no si el nodo procesó el request, lo ignoró, o nunca lo recibió.',
    difficulty: 2,
  },
  {
    concept_id: 'distributed-failures',
    type: 'fact',
    question_text: '¿Qué es un Byzantine fault y cuándo importa?',
    expected_answer: 'Es cuando un nodo actúa de forma arbitraria o maliciosa (envía respuestas incorrectas). Importa en sistemas donde no confías en todos los participantes: blockchain, aviación, sistemas financieros. Para la mayoría de sistemas web, basta asumir crash-stop faults.',
    difficulty: 3,
  },

  // --- consistency-models ---
  {
    concept_id: 'consistency-models',
    type: 'definition',
    question_text: '¿Qué es linearizability y por qué es considerada "consistencia fuerte"?',
    expected_answer: 'Linearizability garantiza que una vez que un write completa, todos los reads posteriores ven ese valor. Es como si hubiera una sola copia de los datos. Es fuerte porque da las mismas garantías que un sistema no distribuido.',
    difficulty: 2,
  },
  {
    concept_id: 'consistency-models',
    type: 'fact',
    question_text: '¿Qué dice el teorema CAP?',
    expected_answer: 'Ante una network partition (P), debes elegir entre Consistency (C, todos los reads ven el write más reciente) y Availability (A, todos los nodos responden). No puedes tener ambas durante una partición. En la práctica, P es inevitable, así que la elección real es CP vs AP.',
    difficulty: 1,
  },
  {
    concept_id: 'consistency-models',
    type: 'comparison',
    question_text: '¿Cuál es la diferencia entre causal consistency y eventual consistency?',
    expected_answer: 'Eventual consistency solo garantiza que eventualmente todos verán el mismo valor, sin orden. Causal consistency preserva el orden de operaciones causalmente relacionadas: si A causó B, todos ven A antes que B. Causal es más fuerte que eventual pero más débil que linearizability.',
    difficulty: 3,
    related_concept_id: 'replication',
  },

  // --- consensus ---
  {
    concept_id: 'consensus',
    type: 'definition',
    question_text: '¿Qué problema resuelve el consenso en sistemas distribuidos?',
    expected_answer: 'Que múltiples nodos se pongan de acuerdo sobre un valor. Se usa para leader election, atomic commit, total order broadcast. Imposible de resolver en presencia de fallas arbitrarias (FLP impossibility en sistemas asíncronos).',
    difficulty: 1,
  },
  {
    concept_id: 'consensus',
    type: 'fact',
    question_text: '¿Cómo funciona Raft a alto nivel?',
    expected_answer: 'Raft elige un leader mediante elecciones con timeouts. El leader recibe todos los writes y los replica al log de los followers. Un write se compromete cuando la mayoría confirma. Si el leader falla, un follower con timeout dispara nueva elección.',
    difficulty: 2,
  },
  {
    concept_id: 'consensus',
    type: 'property',
    question_text: '¿Por qué los algoritmos de consenso requieren una mayoría (quorum)?',
    expected_answer: 'Porque dos quorums siempre se solapan en al menos un nodo, garantizando que la información más reciente está presente. Con N nodos, el quorum es floor(N/2)+1. Tolera hasta floor((N-1)/2) fallas.',
    difficulty: 2,
  },

  // --- stream-processing ---
  {
    concept_id: 'stream-processing',
    type: 'definition',
    question_text: '¿Qué es stream processing y en qué se diferencia del batch processing?',
    expected_answer: 'Stream processing procesa datos continuamente a medida que llegan, con baja latencia. Batch processing acumula datos y los procesa juntos periódicamente. Stream: latencia baja, procesamiento continuo. Batch: throughput alto, procesamiento completo del dataset.',
    difficulty: 1,
  },
  {
    concept_id: 'stream-processing',
    type: 'property',
    question_text: '¿Cuál es la diferencia entre event time y processing time en streams?',
    expected_answer: 'Event time es cuando el evento ocurrió (timestamp del evento). Processing time es cuando el sistema lo procesa. La diferencia importa para windowing correcto: eventos pueden llegar tarde o fuera de orden. Los sistemas robustos usan event time con watermarks para manejar late arrivals.',
    difficulty: 2,
  },
  {
    concept_id: 'stream-processing',
    type: 'guarantee',
    question_text: '¿Qué significa "exactly-once" semantics en stream processing y es realmente posible?',
    expected_answer: 'Exactly-once garantiza que cada evento se procese exactamente una vez, sin duplicados ni pérdida. En la práctica se logra con idempotency + at-least-once delivery, o transacciones distribuidas. No es "magia": requiere que el sistema completo (source, processor, sink) coopere.',
    difficulty: 3,
  },

  // --- slos-slis ---
  {
    concept_id: 'slos-slis',
    type: 'definition',
    question_text: '¿Cuál es la diferencia entre SLI, SLO y SLA?',
    expected_answer: 'SLI (Service Level Indicator): la métrica que mides (ej: latencia p99). SLO (Service Level Objective): el target para esa métrica (ej: p99 < 200ms). SLA (Service Level Agreement): contrato con consecuencias si no se cumple el SLO.',
    difficulty: 1,
  },
  {
    concept_id: 'slos-slis',
    type: 'property',
    question_text: '¿Qué es un error budget y cómo se usa?',
    expected_answer: 'Es la cantidad de downtime o errores permitidos según el SLO. Si el SLO es 99.9% disponibilidad, el error budget es 0.1% (43 min/mes). Si queda budget, puedes hacer cambios riesgosos (deploys). Si se agota, priorizas estabilidad.',
    difficulty: 2,
  },

  // --- monitoring ---
  {
    concept_id: 'monitoring',
    type: 'definition',
    question_text: '¿Cuáles son las cuatro señales doradas del monitoring según Google SRE?',
    expected_answer: 'Latency (tiempo de respuesta), Traffic (demanda al sistema), Errors (tasa de respuestas fallidas), y Saturation (cuán lleno está el recurso más limitado). Si monitoreas estas cuatro, cubres la mayoría de problemas.',
    difficulty: 1,
  },
  {
    concept_id: 'monitoring',
    type: 'comparison',
    question_text: '¿Cuál es la diferencia entre metrics, logs y traces?',
    expected_answer: 'Metrics: valores numéricos agregados en el tiempo (CPU %, requests/sec). Logs: eventos discretos detallados (texto). Traces: el recorrido de un request a través de múltiples servicios. Los tres son complementarios: metrics para alertar, logs para investigar, traces para diagnosticar.',
    difficulty: 2,
    related_concept_id: 'llm-observability',
  },

  // --- embracing-risk ---
  {
    concept_id: 'embracing-risk',
    type: 'definition',
    question_text: '¿Por qué Google SRE dice que 100% reliability es un objetivo incorrecto?',
    expected_answer: 'Porque 100% reliability es infinitamente costoso e imposible de lograr. Además, los usuarios no notan la diferencia entre 99.99% y 100% dado que hay otros factores de falla (su red, su dispositivo). Mejor definir error budgets y gastarlos en velocidad de innovación.',
    difficulty: 1,
  },
  {
    concept_id: 'embracing-risk',
    type: 'fact',
    question_text: '¿Cómo se calcula el costo incremental de cada "nueve" de reliability?',
    expected_answer: 'Cada nueve adicional es ~10x más costoso. Ir de 99% a 99.9% puede requerir redundancia. De 99.9% a 99.99% requiere failover automático, testing extensivo. De 99.99% a 99.999% requiere arquitecturas multi-region y eliminación de single points of failure.',
    difficulty: 2,
  },

  // --- tail-latency ---
  {
    concept_id: 'tail-latency',
    type: 'definition',
    question_text: '¿Por qué la tail latency (p99) es más importante que la latencia promedio?',
    expected_answer: 'Porque en sistemas distribuidos con fan-out, la latencia total es determinada por el componente más lento. Si haces 100 llamadas paralelas, la probabilidad de que al menos una sea p99 es alta. Los usuarios con más datos suelen tener peor latencia y son los más valiosos.',
    difficulty: 1,
  },
  {
    concept_id: 'tail-latency',
    type: 'property',
    question_text: '¿Qué son hedged requests y cómo reducen la tail latency?',
    expected_answer: 'Enviar el mismo request a múltiples réplicas y usar la primera respuesta. Reduce tail latency porque es improbable que todas las réplicas sean lentas simultáneamente. Trade-off: genera carga extra, por eso se usa solo cuando la primera respuesta tarda más de un umbral.',
    difficulty: 2,
  },

  // ==========================================================================
  // PHASE 2: LLM Fundamentals + Reasoning/Agents
  // ==========================================================================

  // --- attention-mechanism ---
  {
    concept_id: 'attention-mechanism',
    type: 'definition',
    question_text: '¿Qué es el mecanismo de atención y qué problema resuelve?',
    expected_answer: 'Es un mecanismo que permite al modelo enfocarse en partes relevantes del input al producir cada parte del output. Resuelve el bottleneck de las RNNs donde toda la información del input se comprime en un solo vector de tamaño fijo.',
    difficulty: 1,
  },
  {
    concept_id: 'attention-mechanism',
    type: 'property',
    question_text: '¿Qué es self-attention y en qué se diferencia de cross-attention?',
    expected_answer: 'Self-attention: cada posición atiende a todas las posiciones en la misma secuencia (input atiende a input). Cross-attention: las queries vienen de una secuencia y los keys/values de otra (decoder atiende a encoder). Self-attention es el mecanismo central del Transformer.',
    difficulty: 2,
  },
  {
    concept_id: 'attention-mechanism',
    type: 'complexity',
    question_text: '¿Cuál es la complejidad computacional de self-attention y por qué es un problema?',
    expected_answer: 'O(n²) en la longitud de la secuencia, tanto en tiempo como en memoria. Para una secuencia de 100K tokens, son 10 billones de operaciones por capa. Esto limita el context window y es la razón de técnicas como sparse attention, flash attention, y sliding window attention.',
    difficulty: 3,
  },

  // --- transformer-architecture ---
  {
    concept_id: 'transformer-architecture',
    type: 'definition',
    question_text: '¿Cuáles son los componentes principales de un Transformer?',
    expected_answer: 'Multi-head attention (múltiples mecanismos de atención en paralelo), positional encoding (información de posición), feed-forward layers (transformación no-lineal por posición), layer normalization, y residual connections.',
    difficulty: 1,
  },
  {
    concept_id: 'transformer-architecture',
    type: 'fact',
    question_text: '¿Por qué el Transformer eliminó la recurrencia y qué ventaja da?',
    expected_answer: 'Las RNNs procesan tokens secuencialmente, impidiendo paralelización. El Transformer usa atención para capturar dependencias sin recurrencia, permitiendo procesar todos los tokens en paralelo durante el entrenamiento. Esto redujo dramáticamente el tiempo de entrenamiento.',
    difficulty: 2,
  },
  {
    concept_id: 'transformer-architecture',
    type: 'property',
    question_text: '¿Qué son las residual connections y por qué son necesarias en Transformers profundos?',
    expected_answer: 'Son conexiones que suman el input de una capa directamente a su output (x + f(x)). Permiten que los gradientes fluyan directamente a través de la red, evitando el vanishing gradient problem en redes profundas. Sin ellas, entrenar Transformers con muchas capas sería muy difícil.',
    difficulty: 2,
  },

  // --- query-key-value ---
  {
    concept_id: 'query-key-value',
    type: 'definition',
    question_text: '¿Qué representan Query, Key y Value en attention y cuál es la fórmula?',
    expected_answer: 'Query: qué estoy buscando. Key: qué contengo (para matching). Value: qué proveo si me seleccionan. Fórmula: Attention(Q,K,V) = softmax(QK^T / √d_k) V. La división por √d_k estabiliza los gradientes.',
    difficulty: 1,
  },
  {
    concept_id: 'query-key-value',
    type: 'fact',
    question_text: '¿Por qué se divide por √d_k en la fórmula de attention?',
    expected_answer: 'Sin la división, los productos punto QK^T crecen con la dimensión d_k, produciendo valores muy grandes. El softmax de valores grandes resulta en gradientes extremadamente pequeños (saturación). Dividir por √d_k mantiene la varianza del producto punto en ~1.',
    difficulty: 2,
  },
  {
    concept_id: 'query-key-value',
    type: 'property',
    question_text: '¿Qué es multi-head attention y por qué es mejor que single-head?',
    expected_answer: 'En vez de una sola función de atención, se proyectan Q, K, V con diferentes matrices aprendidas h veces, se computa atención en paralelo, y se concatenan los resultados. Permite al modelo atender a información de diferentes subspacios simultáneamente (ej: una head para sintaxis, otra para semántica).',
    difficulty: 2,
  },

  // --- positional-encoding ---
  {
    concept_id: 'positional-encoding',
    type: 'definition',
    question_text: '¿Por qué los Transformers necesitan positional encoding?',
    expected_answer: 'Porque self-attention es permutation-invariant: trata el input como un conjunto, no como una secuencia. Sin positional encoding, "el gato come pescado" y "pescado come el gato" producirían la misma representación.',
    difficulty: 1,
  },
  {
    concept_id: 'positional-encoding',
    type: 'comparison',
    question_text: '¿Cuáles son los tipos principales de positional encoding?',
    expected_answer: 'Sinusoidal (original Transformer): funciones seno/coseno de diferentes frecuencias, no aprendido. Learned: embeddings de posición entrenados. RoPE (Rotary Position Embeddings): codifica posición rotando los vectores Q y K, permite extrapolación a secuencias más largas que las vistas en training.',
    difficulty: 2,
    related_concept_id: 'transformer-architecture',
  },

  // --- scaling-laws ---
  {
    concept_id: 'scaling-laws',
    type: 'definition',
    question_text: '¿Qué son las scaling laws de LLMs?',
    expected_answer: 'Son relaciones empíricas que muestran que el loss de un LLM escala como ley de potencia con el tamaño del modelo (parámetros), la cantidad de datos de entrenamiento, y el compute total. Permiten predecir el rendimiento antes de entrenar.',
    difficulty: 1,
  },
  {
    concept_id: 'scaling-laws',
    type: 'fact',
    question_text: '¿Cuáles son las implicaciones prácticas de las scaling laws para el entrenamiento?',
    expected_answer: 'Ayudan a decidir cómo distribuir un presupuesto fijo de compute entre tamaño de modelo y datos. También muestran que no hay "techo" observable: más compute siempre mejora performance. Esto justifica las inversiones masivas en training runs.',
    difficulty: 2,
  },

  // --- compute-optimal-training ---
  {
    concept_id: 'compute-optimal-training',
    type: 'definition',
    question_text: '¿Cuál fue el hallazgo clave del paper Chinchilla sobre entrenamiento óptimo?',
    expected_answer: 'Que la mayoría de modelos grandes estaban significativamente sub-entrenados (pocos datos para su tamaño). Lo óptimo es escalar datos y parámetros igualmente. Chinchilla (70B params, 1.4T tokens) superó a Gopher (280B params, 300B tokens) con 4x menos parámetros.',
    difficulty: 1,
  },
  {
    concept_id: 'compute-optimal-training',
    type: 'fact',
    question_text: '¿Cuál es la ratio óptima de tokens a parámetros según Chinchilla?',
    expected_answer: 'Aproximadamente 20 tokens por parámetro. Un modelo de 10B necesita ~200B tokens de training data. Modelos como LLaMA siguieron esta receta: 7B con 1T tokens, 65B con 1.4T tokens.',
    difficulty: 2,
  },

  // --- foundation-models ---
  {
    concept_id: 'foundation-models',
    type: 'definition',
    question_text: '¿Qué son los foundation models y qué los hace diferentes?',
    expected_answer: 'Son modelos grandes entrenados con datos amplios que pueden adaptarse a muchas tareas downstream. A diferencia de modelos task-specific, una sola preentrenamiento sirve para múltiples aplicaciones. Exhiben habilidades emergentes que aparecen solo a cierta escala.',
    difficulty: 1,
  },
  {
    concept_id: 'foundation-models',
    type: 'property',
    question_text: '¿Qué son las emergent abilities en foundation models?',
    expected_answer: 'Son capacidades que no están presentes en modelos pequeños pero emergen a cierta escala: few-shot learning, chain-of-thought reasoning, arithmetic. Son difíciles de predecir con las scaling laws porque aparecen abruptamente, no gradualmente.',
    difficulty: 2,
  },

  // --- react-pattern ---
  {
    concept_id: 'react-pattern',
    type: 'definition',
    question_text: '¿Qué es el patrón ReAct y cómo combina reasoning y acting?',
    expected_answer: 'ReAct intercala razonamiento (chain-of-thought) con acciones (tool use). El modelo piensa paso a paso, toma una acción (buscar, calcular), observa el resultado, y repite. Produce reasoning grounded en información real y trazas auditables.',
    difficulty: 1,
  },
  {
    concept_id: 'react-pattern',
    type: 'property',
    question_text: '¿Cuáles son las tres fases del loop ReAct?',
    expected_answer: 'Thought (razonamiento sobre qué hacer), Action (ejecutar una herramienta o acción), Observation (observar el resultado). El ciclo se repite hasta que el modelo decide que tiene suficiente información para responder.',
    difficulty: 1,
  },
  {
    concept_id: 'react-pattern',
    type: 'comparison',
    question_text: '¿En qué se diferencia ReAct de chain-of-thought puro?',
    expected_answer: 'Chain-of-thought solo razona internamente sin acceso a información externa. ReAct puede actuar (usar tools) y observar resultados reales, lo que evita confabular hechos. ReAct es más preciso para tareas que requieren información factual actualizada.',
    difficulty: 2,
    related_concept_id: 'chain-of-thought',
  },

  // --- chain-of-thought ---
  {
    concept_id: 'chain-of-thought',
    type: 'definition',
    question_text: '¿Qué es chain-of-thought prompting y por qué mejora el rendimiento?',
    expected_answer: 'Es hacer que el modelo muestre pasos de razonamiento intermedios antes de dar la respuesta final. Mejora performance en tareas complejas porque descompone el problema en pasos más simples. Puede ser few-shot (con ejemplos) o zero-shot ("think step by step").',
    difficulty: 1,
  },
  {
    concept_id: 'chain-of-thought',
    type: 'fact',
    question_text: '¿Qué es zero-shot CoT y cuál fue el hallazgo sorprendente?',
    expected_answer: 'Agregar simplemente "Let\'s think step by step" al prompt activa razonamiento en cadena sin necesitar ejemplos. Esto mostró que la capacidad de razonamiento ya existía en los modelos y solo necesitaba ser elicitada con el prompt correcto.',
    difficulty: 1,
  },

  // --- tree-of-thoughts ---
  {
    concept_id: 'tree-of-thoughts',
    type: 'definition',
    question_text: '¿Cómo extiende Tree of Thoughts al chain-of-thought estándar?',
    expected_answer: 'En vez de una sola cadena de razonamiento, explora múltiples caminos en forma de árbol. Puede evaluar cada rama (auto-evaluación), hacer backtracking (descartar malas ramas), y explorar breadth-first o depth-first. Permite resolución deliberada de problemas.',
    difficulty: 2,
  },
  {
    concept_id: 'tree-of-thoughts',
    type: 'property',
    question_text: '¿Qué tipos de problemas se benefician de Tree of Thoughts vs CoT lineal?',
    expected_answer: 'Problemas que requieren exploración (juegos, puzzles, planning), donde un error temprano invalida toda la cadena. CoT lineal es suficiente para problemas donde el razonamiento es más directo (matemáticas, lógica simple). ToT es más costoso (múltiples generaciones).',
    difficulty: 2,
  },

  // --- reflexion ---
  {
    concept_id: 'reflexion',
    type: 'definition',
    question_text: '¿Qué es Reflexion y cómo permite a los agentes aprender de errores?',
    expected_answer: 'Reflexion es un patrón donde el agente, tras fallar, genera una reflexión verbal sobre qué salió mal y la almacena en memoria. En intentos futuros, consulta estas reflexiones para evitar los mismos errores. Aprendizaje sin actualizar pesos del modelo.',
    difficulty: 2,
  },
  {
    concept_id: 'reflexion',
    type: 'property',
    question_text: '¿Qué componentes necesita un sistema Reflexion?',
    expected_answer: 'Un actor (genera acciones), un evaluador (determina éxito/fracaso), un modelo de reflexión (genera feedback verbal), y una memoria episódica (almacena las reflexiones). El actor consulta la memoria antes de cada intento.',
    difficulty: 2,
  },

  // --- tool-use ---
  {
    concept_id: 'tool-use',
    type: 'definition',
    question_text: '¿Qué es tool use en LLMs y por qué es importante?',
    expected_answer: 'Es la capacidad de LLMs de llamar APIs externas, calculadoras, buscadores, etc. Extiende las capacidades más allá de los datos de entrenamiento: acceso a información actualizada, cálculos precisos, acciones en el mundo real. El desafío es saber cuándo usar qué herramienta.',
    difficulty: 1,
  },
  {
    concept_id: 'tool-use',
    type: 'property',
    question_text: '¿Cómo se implementa function calling en la práctica?',
    expected_answer: 'Se define un esquema de funciones disponibles (nombre, descripción, parámetros) en el prompt o via API especial. El modelo genera una llamada a función (nombre + argumentos) en vez de texto. El sistema ejecuta la función y retorna el resultado al modelo para que continúe.',
    difficulty: 2,
  },

  // --- plan-and-execute ---
  {
    concept_id: 'plan-and-execute',
    type: 'definition',
    question_text: '¿Cómo funciona el patrón Plan-and-Execute y en qué se diferencia de ReAct?',
    expected_answer: 'Plan-and-Execute primero genera un plan completo (lista de pasos), luego ejecuta cada paso. ReAct intercala razonamiento y acción en cada paso. Plan-and-Execute es mejor para tareas complejas que requieren coordinación, y permite re-planificar si un paso falla.',
    difficulty: 2,
  },
  {
    concept_id: 'plan-and-execute',
    type: 'property',
    question_text: '¿Cuándo conviene re-planificar vs continuar con el plan original?',
    expected_answer: 'Re-planificar cuando un paso falla, el resultado es inesperado, o surge nueva información que invalida el plan. Continuar cuando los resultados están dentro de lo esperado. El re-planning tiene costo (tokens, latencia), así que se hace solo cuando es necesario.',
    difficulty: 2,
  },

  // --- prompt-engineering ---
  {
    concept_id: 'prompt-engineering',
    type: 'definition',
    question_text: '¿Qué técnicas principales componen el prompt engineering?',
    expected_answer: 'Few-shot learning (ejemplos en el prompt), role prompting (asignar un rol al modelo), format specification (definir formato de output), constraint setting (reglas y límites), chain-of-thought, y system vs user prompts para separar instrucciones de datos.',
    difficulty: 1,
  },
  {
    concept_id: 'prompt-engineering',
    type: 'fact',
    question_text: '¿Por qué el orden y la estructura del prompt importan?',
    expected_answer: 'Los LLMs tienen primacy bias (prestan más atención al inicio) y recency bias (y al final). Instrucciones importantes deben ir al inicio o al final. Delimitadores claros separan secciones. La estructura consistente (JSON, markdown) mejora la adherencia al formato.',
    difficulty: 2,
  },

  // --- structured-output ---
  {
    concept_id: 'structured-output',
    type: 'definition',
    question_text: '¿Qué técnicas existen para obtener structured output de LLMs?',
    expected_answer: 'JSON mode (API-level), function calling schemas, grammar-constrained decoding (restringir tokens válidos), format instructions en prompt, y parseo + validación post-generación con retry. Las más confiables son las API-level porque restringen la generación directamente.',
    difficulty: 1,
  },
  {
    concept_id: 'structured-output',
    type: 'property',
    question_text: '¿Qué es grammar-constrained decoding?',
    expected_answer: 'Restringir los tokens que el modelo puede generar en cada paso para que el output sea siempre un JSON/formato válido. Se usa una gramática formal (como GBNF) que define la estructura. Garantiza output válido pero puede afectar la calidad del contenido.',
    difficulty: 2,
  },

  // --- fine-tuning-efficiency ---
  {
    concept_id: 'fine-tuning-efficiency',
    type: 'definition',
    question_text: '¿Qué es LoRA y por qué es eficiente?',
    expected_answer: 'LoRA (Low-Rank Adaptation) congela los pesos del modelo y agrega matrices de bajo rango entrenables a cada capa. Solo entrena ~0.1-1% de los parámetros totales. Reduce compute y memoria 10-1000x manteniendo calidad comparable al fine-tuning completo.',
    difficulty: 1,
  },
  {
    concept_id: 'fine-tuning-efficiency',
    type: 'fact',
    question_text: '¿Cuál es la intuición matemática detrás de LoRA?',
    expected_answer: 'Los cambios necesarios en los pesos durante fine-tuning tienen bajo rango intrínseco. En vez de actualizar la matriz completa W (d×d), se aprenden dos matrices A (d×r) y B (r×d) donde r << d. W_nuevo = W + BA. Esto captura las adaptaciones necesarias con muchos menos parámetros.',
    difficulty: 3,
  },

  // --- dpo ---
  {
    concept_id: 'dpo',
    type: 'definition',
    question_text: '¿Qué es DPO y cómo se compara con RLHF?',
    expected_answer: 'DPO (Direct Preference Optimization) alinea modelos directamente desde pares de preferencias (buena respuesta vs mala) sin necesitar un reward model ni RL. Misma función objetivo que RLHF pero con un loss supervisado. Más simple, estable y eficiente de entrenar.',
    difficulty: 1,
  },
  {
    concept_id: 'dpo',
    type: 'property',
    question_text: '¿Por qué DPO es más estable que RLHF?',
    expected_answer: 'RLHF requiere entrenar un reward model (que puede ser impreciso), luego hacer RL (que es inestable). DPO elimina ambos pasos: convierte el problema de RL en clasificación binaria (cuál respuesta es mejor). Menos hiperparámetros, convergencia más predecible.',
    difficulty: 2,
  },

  // --- test-time-compute ---
  {
    concept_id: 'test-time-compute',
    type: 'definition',
    question_text: '¿Qué es test-time compute scaling y qué dimensión nueva abre?',
    expected_answer: 'Es usar más compute en inferencia (no en training) para mejorar el razonamiento. Modelos como o1 y DeepSeek-R1 "piensan más" generando cadenas de razonamiento largas antes de responder. Abre una nueva dimensión de scaling: además de model size y training data, también inference compute.',
    difficulty: 1,
  },
  {
    concept_id: 'test-time-compute',
    type: 'property',
    question_text: '¿Cuál es el trade-off fundamental del test-time compute?',
    expected_answer: 'Latencia por calidad: respuestas más precisas requieren más tiempo y tokens de generación. Más costoso por query. No todos los problemas se benefician: preguntas simples no necesitan razonamiento extenso. Se necesita routing inteligente para decidir cuánto compute usar.',
    difficulty: 2,
  },

  // --- reasoning-models ---
  {
    concept_id: 'reasoning-models',
    type: 'definition',
    question_text: '¿Qué son los reasoning models (o1, R1) y cómo se entrenan?',
    expected_answer: 'Son LLMs entrenados con RL para producir cadenas de razonamiento extensas antes de responder. No se les da datos de razonamiento etiquetados: la capacidad emerge del RL que recompensa respuestas correctas. Excelen en math, código, y razonamiento complejo.',
    difficulty: 1,
  },
  {
    concept_id: 'reasoning-models',
    type: 'fact',
    question_text: '¿Cuál fue el insight clave de DeepSeek-R1 respecto al entrenamiento?',
    expected_answer: 'Que la capacidad de razonamiento puede emerger puramente de RL sin supervisión de razonamiento. Solo necesitas recompensar la respuesta correcta (outcome-based RL) y el modelo aprende a generar cadenas de razonamiento como estrategia. El razonamiento emergió, no se programó.',
    difficulty: 2,
  },

  // --- multimodal-embeddings ---
  {
    concept_id: 'multimodal-embeddings',
    type: 'definition',
    question_text: '¿Qué es CLIP y cómo crea embeddings multimodales?',
    expected_answer: 'CLIP (Contrastive Language-Image Pre-training) entrena un image encoder y un text encoder juntos para que imágenes y textos similares tengan representaciones cercanas en el mismo espacio vectorial. Usa contrastive learning: acerca pares correctos y aleja pares incorrectos.',
    difficulty: 1,
  },
  {
    concept_id: 'multimodal-embeddings',
    type: 'property',
    question_text: '¿Por qué CLIP permite zero-shot image classification?',
    expected_answer: 'Porque comparte espacio de embeddings entre texto e imágenes. Para clasificar, embeds los nombres de las clases como texto y la imagen, luego compara similitud coseno. No necesita entrenar un clasificador específico: cualquier texto sirve como "clase".',
    difficulty: 2,
  },

  // --- vision-language-models ---
  {
    concept_id: 'vision-language-models',
    type: 'definition',
    question_text: '¿Cuál es la arquitectura típica de un VLM?',
    expected_answer: 'Vision encoder (como CLIP o ViT) que procesa la imagen, una capa de proyección que mapea features visuales al espacio del LLM, y un LLM que procesa tanto los tokens visuales proyectados como texto. Se entrena con visual instruction tuning.',
    difficulty: 1,
  },
  {
    concept_id: 'vision-language-models',
    type: 'fact',
    question_text: '¿Qué es visual instruction tuning y por qué fue un breakthrough?',
    expected_answer: 'Es fine-tuning de un VLM con datos de instrucciones multimodales (pregunta sobre imagen → respuesta). LLaVA demostró que con datos de instruction tuning generados por GPT-4, un modelo relativamente pequeño podía competir con GPT-4V. Hizo VLMs accesibles y replicables.',
    difficulty: 2,
  },

  // ==========================================================================
  // PHASE 3: RAG, Memory, Context
  // ==========================================================================

  // --- rag-basics ---
  {
    concept_id: 'rag-basics',
    type: 'definition',
    question_text: '¿Qué es RAG y qué tres componentes tiene?',
    expected_answer: 'RAG (Retrieval-Augmented Generation) aumenta la generación del LLM con documentos recuperados. Tres componentes: Retriever (busca documentos relevantes), Reader/Ranker (selecciona los mejores), y Generator (LLM que genera la respuesta usando el contexto recuperado).',
    difficulty: 1,
  },
  {
    concept_id: 'rag-basics',
    type: 'property',
    question_text: '¿Qué ventajas tiene RAG sobre fine-tuning para incorporar conocimiento?',
    expected_answer: 'Actualizable sin reentrenamiento (solo actualiza la base de datos), fuentes auditables (puedes citar de dónde viene la info), más barato que fine-tuning, y reduce hallucination al grounding en documentos reales. Fine-tuning es mejor para cambiar el estilo o formato del modelo.',
    difficulty: 2,
  },
  {
    concept_id: 'rag-basics',
    type: 'fact',
    question_text: '¿Cuáles son las métricas principales para evaluar un sistema RAG?',
    expected_answer: 'Para retrieval: precision (% de docs relevantes entre los recuperados) y recall (% de docs relevantes encontrados). Para generation: faithfulness (respuesta soportada por docs), relevance (respuesta contesta la pregunta), y answer correctness.',
    difficulty: 2,
  },

  // --- embeddings ---
  {
    concept_id: 'embeddings',
    type: 'definition',
    question_text: '¿Qué son embeddings y por qué permiten búsqueda semántica?',
    expected_answer: 'Son representaciones vectoriales densas de texto donde significados similares producen vectores cercanos en el espacio. Permiten búsqueda semántica porque "perro" y "canino" tendrán vectores cercanos aunque las palabras sean diferentes.',
    difficulty: 1,
  },
  {
    concept_id: 'embeddings',
    type: 'property',
    question_text: '¿Qué dimensiones y métricas de distancia se usan comúnmente en embeddings?',
    expected_answer: 'Dimensiones típicas: 768 (sentence-transformers), 1536 (OpenAI ada-002), 3072 (OpenAI large). Métricas de distancia: cosine similarity (la más común, normalizada), dot product (incluye magnitud), y euclidean distance. Cosine es preferida porque es invariante a la magnitud del vector.',
    difficulty: 2,
  },

  // --- vector-search ---
  {
    concept_id: 'vector-search',
    type: 'definition',
    question_text: '¿Qué es vector search y cuál es el trade-off entre exacto y aproximado?',
    expected_answer: 'Es buscar los vectores más similares en una base de datos de embeddings. Búsqueda exacta: O(n) brute force, perfecta pero lenta. Búsqueda aproximada (ANN): usa índices como HNSW o IVF, mucho más rápida pero puede perder algunos resultados. Trade-off: velocidad vs recall.',
    difficulty: 1,
  },
  {
    concept_id: 'vector-search',
    type: 'fact',
    question_text: '¿Cómo funciona HNSW y por qué es el algoritmo más popular?',
    expected_answer: 'HNSW (Hierarchical Navigable Small World) construye un grafo multi-nivel donde cada nivel es más disperso. La búsqueda empieza en el nivel más alto (pocos nodos, saltos grandes) y baja a niveles más densos para refinamiento. Popular por ser rápido, preciso, y no requerir entrenamiento.',
    difficulty: 2,
  },

  // --- chunking-strategies ---
  {
    concept_id: 'chunking-strategies',
    type: 'definition',
    question_text: '¿Por qué es necesario chunking en RAG y cuáles son las estrategias principales?',
    expected_answer: 'Los documentos son demasiado largos para embeddings y context windows. Estrategias: fixed size (ej: 512 tokens), por oraciones/párrafos, recursive (divide jerárquicamente), y semantic (agrupa por tema). Cada una tiene trade-offs entre granularidad y contexto.',
    difficulty: 1,
  },
  {
    concept_id: 'chunking-strategies',
    type: 'property',
    question_text: '¿Cuál es el trade-off entre chunks grandes y pequeños?',
    expected_answer: 'Chunks pequeños: embeddings más precisos (un tema por chunk), mejor recall, pero pierden contexto circundante. Chunks grandes: mantienen contexto pero el embedding mezcla múltiples temas, peor precision. Solución: parent-child chunking (embed el hijo, recuperar el padre).',
    difficulty: 2,
  },

  // --- lost-in-middle ---
  {
    concept_id: 'lost-in-middle',
    type: 'definition',
    question_text: '¿Qué descubrió el paper "Lost in the Middle" sobre LLMs y contextos largos?',
    expected_answer: 'Que los LLMs prestan menos atención a la información en el medio del contexto. El rendimiento tiene forma de U: mejor cuando la info relevante está al inicio o al final. Esto afecta cómo se deben ordenar los documentos en RAG.',
    difficulty: 1,
  },
  {
    concept_id: 'lost-in-middle',
    type: 'property',
    question_text: '¿Cómo se mitiga el efecto "lost in the middle" en sistemas RAG?',
    expected_answer: 'Poner documentos más relevantes al inicio y al final del contexto. Reducir la cantidad de documentos (solo los más relevantes). Usar técnicas de compresión de contexto. O simplemente mantener contextos cortos y bien filtrados.',
    difficulty: 2,
  },

  // --- context-window-limits ---
  {
    concept_id: 'context-window-limits',
    type: 'definition',
    question_text: '¿Qué limita el context window y por qué más contexto no siempre es mejor?',
    expected_answer: 'El context window está limitado por la memoria (atención es O(n²)) y el costo (tokens). Más contexto no es mejor porque: el modelo puede distraerse con info irrelevante, lost-in-middle, y el costo escala linealmente con el tamaño.',
    difficulty: 1,
  },
  {
    concept_id: 'context-window-limits',
    type: 'fact',
    question_text: '¿Cuál es la diferencia entre "long context" del modelo y la calidad de uso de ese contexto?',
    expected_answer: 'Un modelo puede aceptar 128K tokens pero eso no significa que los use efectivamente. Needle-in-haystack tests muestran degradación en contextos largos. La capacidad real de procesamiento suele ser menor que el máximo técnico. Más importante que el tamaño es cuán bien se usa.',
    difficulty: 2,
  },

  // --- external-memory ---
  {
    concept_id: 'external-memory',
    type: 'definition',
    question_text: '¿Qué es external memory para LLMs y qué problema resuelve?',
    expected_answer: 'Es almacenar información fuera del contexto del modelo (en vector DBs, key-value stores) y recuperarla cuando se necesita. Resuelve la limitación del context window: permite acceso a conocimiento ilimitado sin los costos y limitaciones de contextos largos.',
    difficulty: 1,
  },
  {
    concept_id: 'external-memory',
    type: 'comparison',
    question_text: '¿Cuál es la diferencia entre external memory y RAG?',
    expected_answer: 'RAG recupera documentos para responder una pregunta específica. External memory es más amplio: incluye memorias episódicas (conversaciones pasadas), conocimiento persistente, y estado del usuario. MemGPT por ejemplo usa un sistema de paginación de memoria inspirado en OS.',
    difficulty: 2,
    related_concept_id: 'rag-basics',
  },

  // --- hybrid-search ---
  {
    concept_id: 'hybrid-search',
    type: 'definition',
    question_text: '¿Qué es hybrid search y por qué supera a búsqueda semántica sola?',
    expected_answer: 'Combina búsqueda densa (embeddings/semántica) con búsqueda sparse (keywords/BM25). La semántica encuentra sinónimos y paráfrasis. Keywords encuentra exactamente lo que pides (nombres propios, IDs, siglas). Juntas cubren más casos que cualquiera sola.',
    difficulty: 1,
  },
  {
    concept_id: 'hybrid-search',
    type: 'property',
    question_text: '¿Qué es Reciprocal Rank Fusion (RRF) y para qué se usa?',
    expected_answer: 'RRF es una técnica para combinar rankings de diferentes fuentes. Asigna un score basado en la posición en cada ranking: 1/(k + rank). Luego suma los scores. Es simple, no necesita normalización, y funciona bien en la práctica para fusionar resultados de búsqueda dense y sparse.',
    difficulty: 2,
  },

  // --- memory-management ---
  {
    concept_id: 'memory-management',
    type: 'definition',
    question_text: '¿Por qué "guardar todo para siempre" no funciona como estrategia de memoria para LLMs?',
    expected_answer: 'Porque genera ruido: memorias obsoletas o irrelevantes contaminan las recuperaciones. Además, los costos de storage y búsqueda crecen sin control. Se necesitan políticas activas de olvido, summarización, y priorización por importancia y recencia.',
    difficulty: 1,
  },
  {
    concept_id: 'memory-management',
    type: 'property',
    question_text: '¿Qué políticas de memoria se usan para LLM agents?',
    expected_answer: 'Recency bias (priorizar memorias recientes), importance scoring (memorias sobre eventos significativos pesan más), summarización (condensar memorias detalladas en resúmenes), conflict resolution (cuándo dos memorias se contradicen), y TTL/expiration para datos temporales.',
    difficulty: 2,
  },

  // ==========================================================================
  // PHASE 4: Safety, Guardrails, Evaluation
  // ==========================================================================

  // --- constitutional-ai ---
  {
    concept_id: 'constitutional-ai',
    type: 'definition',
    question_text: '¿Qué es Constitutional AI y cómo funciona?',
    expected_answer: 'Es un método de Anthropic para alinear modelos usando principios escritos (una "constitución"). El modelo genera respuestas, luego se auto-critica y revisa según los principios. Más escalable que RLHF con humanos porque la constitución se puede actualizar sin re-anotar datos.',
    difficulty: 1,
  },
  {
    concept_id: 'constitutional-ai',
    type: 'property',
    question_text: '¿Cuáles son las dos fases de Constitutional AI?',
    expected_answer: 'Fase 1 (Supervised): el modelo genera, se auto-critica según la constitución, y se revisa. Los pares revisados se usan para fine-tuning. Fase 2 (RL): se entrena un reward model con preferencias generadas por el propio modelo (RLAIF), luego RL contra ese reward model.',
    difficulty: 2,
  },

  // --- self-consistency ---
  {
    concept_id: 'self-consistency',
    type: 'definition',
    question_text: '¿Qué es self-consistency y cómo mejora la fiabilidad?',
    expected_answer: 'Es generar múltiples cadenas de razonamiento para la misma pregunta y tomar el voto mayoritario. Si 4 de 5 cadenas llegan a la misma respuesta, es más probable que sea correcta. Mejora accuracy en tareas de razonamiento a costa de generar múltiples respuestas.',
    difficulty: 1,
  },
  {
    concept_id: 'self-consistency',
    type: 'property',
    question_text: '¿Cuál es el trade-off principal de self-consistency?',
    expected_answer: 'Costo: necesitas generar N respuestas (típicamente 5-40) para una pregunta. Si cada generación cuesta $X, self-consistency cuesta $NX. Funciona mejor con temperature > 0 para diversidad. No ayuda si el modelo falla consistentemente por falta de conocimiento.',
    difficulty: 2,
  },

  // --- offline-evaluation ---
  {
    concept_id: 'offline-evaluation',
    type: 'definition',
    question_text: '¿Qué es offline evaluation de LLMs y cuáles son sus limitaciones?',
    expected_answer: 'Es evaluar modelos con benchmarks fijos (MMLU, HumanEval, etc.) antes de producción. Limitaciones: benchmark contamination (datos en training), no captura comportamiento real del usuario, métricas estáticas vs dinámicas, y el LLM-as-judge tiene sus propios biases.',
    difficulty: 1,
  },
  {
    concept_id: 'offline-evaluation',
    type: 'property',
    question_text: '¿Qué es LLM-as-judge y cuándo es apropiado?',
    expected_answer: 'Usar un LLM (típicamente más capaz) para evaluar outputs de otro LLM. Apropiado para evaluar calidad de texto, coherencia, y adherencia a instrucciones. Menos apropiado para factuality (el juez puede no saber la verdad) y requiere calibración contra evaluación humana.',
    difficulty: 2,
  },

  // --- online-evaluation ---
  {
    concept_id: 'online-evaluation',
    type: 'definition',
    question_text: '¿Qué es online evaluation y por qué complementa a la offline?',
    expected_answer: 'Es evaluar LLMs en producción con usuarios reales: A/B tests, shadow deployments, canary releases. Captura lo que benchmarks no pueden: satisfacción real, edge cases imprevistos, y cómo el modelo interactúa con el sistema completo.',
    difficulty: 1,
  },
  {
    concept_id: 'online-evaluation',
    type: 'fact',
    question_text: '¿Qué métricas se monitorean en online evaluation de LLMs?',
    expected_answer: 'Latencia y throughput (performance), user satisfaction (thumbs up/down, retention), task completion rate, hallucination rate (feedback negativo), cost per query, y safety incidents. Combinar métricas cuantitativas con feedback cualitativo.',
    difficulty: 2,
  },

  // --- red-teaming ---
  {
    concept_id: 'red-teaming',
    type: 'definition',
    question_text: '¿Qué es red teaming para LLMs?',
    expected_answer: 'Es pruebas adversarias para encontrar fallos: jailbreaks (bypass de safety), outputs dañinos, bias, leakage de información privada. Se hace manual (expertos) y automatizado (LLMs atacando LLMs). Esencial antes de deployment.',
    difficulty: 1,
  },
  {
    concept_id: 'red-teaming',
    type: 'property',
    question_text: '¿Cuáles son las categorías principales de ataques en red teaming de LLMs?',
    expected_answer: 'Jailbreaks (evadir safety training), prompt injection (override de instrucciones), data extraction (obtener datos de training), bias elicitation (provocar respuestas sesgadas), capability elicitation (hacer que el modelo haga cosas que no debería), y consistency testing.',
    difficulty: 2,
  },

  // --- output-validation ---
  {
    concept_id: 'output-validation',
    type: 'definition',
    question_text: '¿Qué es output validation y qué capas incluye?',
    expected_answer: 'Verificar outputs del LLM antes de usarlos: schema validation (JSON válido), content validation (no hay alucinaciones), safety filtering (no contenido dañino), y format validation (cumple el formato requerido). Defense in depth contra generaciones poco confiables.',
    difficulty: 1,
  },
  {
    concept_id: 'output-validation',
    type: 'property',
    question_text: '¿Cuál es el patrón de retry con output validation?',
    expected_answer: 'Generar → Validar → Si falla, incluir el error en el prompt y regenerar → Validar de nuevo → Si falla N veces, fallback (respuesta genérica, escalación humana). Importante: limitar retries (costo) y tener fallback claro. Zod/Pydantic son comunes para schema validation.',
    difficulty: 2,
  },

  // --- prompt-injection ---
  {
    concept_id: 'prompt-injection',
    type: 'definition',
    question_text: '¿Qué es prompt injection y cuáles son los dos tipos principales?',
    expected_answer: 'Es un ataque donde input malicioso sobrescribe las instrucciones del sistema. Direct injection: el usuario incluye instrucciones en su input. Indirect injection: datos externos (docs RAG, tool outputs) contienen instrucciones maliciosas. Análogo a SQL injection.',
    difficulty: 1,
  },
  {
    concept_id: 'prompt-injection',
    type: 'property',
    question_text: '¿Qué defensas existen contra prompt injection?',
    expected_answer: 'Input sanitization (filtrar patrones conocidos), output filtering (validar que el output cumple la tarea), privilege separation (el LLM no tiene acceso directo a datos sensibles), instruction hierarchy (system > user), y detección con clasificadores entrenados.',
    difficulty: 2,
  },

  // --- llm-security-owasp ---
  {
    concept_id: 'llm-security-owasp',
    type: 'definition',
    question_text: '¿Cuáles son las top 3 vulnerabilidades del OWASP Top 10 para LLMs?',
    expected_answer: 'LLM01: Prompt Injection (direct e indirect). LLM02: Insecure Output Handling (confiar en outputs sin validar). LLM03: Training Data Poisoning (datos maliciosos en el training). Estas tres cubren los vectores de ataque más comunes.',
    difficulty: 1,
  },
  {
    concept_id: 'llm-security-owasp',
    type: 'property',
    question_text: '¿Qué es "excessive agency" en el contexto de seguridad LLM?',
    expected_answer: 'Cuando un LLM tiene acceso a funciones, datos, o permisos innecesarios para su tarea. Si el modelo es manipulado (prompt injection), puede abusar de esos permisos. Principio de least privilege: dar al LLM solo los tools y accesos mínimos necesarios.',
    difficulty: 2,
  },

  // ==========================================================================
  // PHASE 5: Inference, Serving, Economics
  // ==========================================================================

  // --- kv-cache ---
  {
    concept_id: 'kv-cache',
    type: 'definition',
    question_text: '¿Qué es el KV cache y por qué es esencial para inference eficiente?',
    expected_answer: 'El KV cache almacena los key-value pairs computados para tokens anteriores durante la generación. Sin él, cada token nuevo requeriría recalcular attention sobre todos los tokens previos. Con KV cache, solo se computa attention del nuevo token contra los K/V cacheados.',
    difficulty: 1,
  },
  {
    concept_id: 'kv-cache',
    type: 'property',
    question_text: '¿Cómo escala el uso de memoria del KV cache?',
    expected_answer: 'Escala como: batch_size × sequence_length × num_layers × 2 × hidden_dim × precision. Para un modelo de 70B con secuencia de 4K y batch 32, puede usar ~40GB de memoria. Esta es la principal limitación del batch size y sequence length en inference.',
    difficulty: 2,
  },

  // --- batching-inference ---
  {
    concept_id: 'batching-inference',
    type: 'definition',
    question_text: '¿Qué es continuous batching y por qué superó al static batching?',
    expected_answer: 'Static batching procesa un batch entero y espera a que todos terminen. Continuous batching agrega nuevos requests al batch tan pronto como otros terminan, a nivel de iteración. Mejora throughput significativamente porque no hay GPUs idle esperando al request más lento.',
    difficulty: 2,
  },
  {
    concept_id: 'batching-inference',
    type: 'property',
    question_text: '¿Cuál es el desafío principal del batching para LLM inference?',
    expected_answer: 'Los requests tienen longitudes de entrada y salida diferentes. Requests cortos terminan rápido y los largos demoran. Sin continuous batching, los cortos esperan a los largos. Además, el KV cache de cada request ocupa memoria diferente, complicando el memory management.',
    difficulty: 2,
  },

  // --- paged-attention ---
  {
    concept_id: 'paged-attention',
    type: 'definition',
    question_text: '¿Qué es PagedAttention y qué problema resuelve?',
    expected_answer: 'PagedAttention (de vLLM) almacena el KV cache en bloques no contiguos, como la memoria virtual de un OS. Resuelve la fragmentación de memoria: el KV cache ya no necesita ser contiguo. Permite memory sharing (ej: beam search) y mayor batch size.',
    difficulty: 2,
  },
  {
    concept_id: 'paged-attention',
    type: 'fact',
    question_text: '¿Cuánto mejora vLLM el throughput comparado con inference naive?',
    expected_answer: 'vLLM reporta 2-4x mejor throughput que HuggingFace Transformers y 2.2x sobre TGI en varios benchmarks. La mejora viene de mejor utilización de GPU memory (menos desperdicio) y continuous batching.',
    difficulty: 2,
  },

  // --- speculative-decoding ---
  {
    concept_id: 'speculative-decoding',
    type: 'definition',
    question_text: '¿Cómo funciona speculative decoding?',
    expected_answer: 'Un modelo pequeño (draft) genera K tokens candidatos rápidamente. El modelo grande los verifica todos en paralelo en un solo forward pass. Los tokens aceptados se mantienen. Acelera inference 2-3x sin pérdida de calidad porque el output final es idéntico al del modelo grande.',
    difficulty: 2,
  },
  {
    concept_id: 'speculative-decoding',
    type: 'property',
    question_text: '¿Por qué speculative decoding no pierde calidad?',
    expected_answer: 'Porque el modelo grande verifica cada token y rechaza los incorrectos. La distribución de probabilidad del output es matemáticamente idéntica a la del modelo grande generando solo. Solo se aceptan tokens que el modelo grande hubiera generado.',
    difficulty: 2,
  },

  // --- quantization ---
  {
    concept_id: 'quantization',
    type: 'definition',
    question_text: '¿Qué es quantization y cuáles son los niveles comunes?',
    expected_answer: 'Reducir la precisión de los pesos del modelo: FP32 → FP16 → INT8 → INT4. Reduce memoria (INT8 es 2x menos que FP16, INT4 es 4x menos) y acelera inference. Trade-off: calidad puede degradarse especialmente en INT4.',
    difficulty: 1,
  },
  {
    concept_id: 'quantization',
    type: 'fact',
    question_text: '¿Cuál es la diferencia entre post-training quantization (PTQ) y quantization-aware training (QAT)?',
    expected_answer: 'PTQ: cuantiza después de entrenar, más simple pero puede perder más calidad. QAT: simula cuantización durante training para que el modelo se adapte, mejor calidad pero requiere reentrenar. Para LLMs grandes, PTQ con calibración (GPTQ, AWQ) es lo más común.',
    difficulty: 2,
  },

  // --- token-economics ---
  {
    concept_id: 'token-economics',
    type: 'definition',
    question_text: '¿Cómo se estructura el pricing de LLM APIs y por qué importa?',
    expected_answer: 'Se cobra por millón de tokens, separando input y output. Output es 3-5x más caro que input porque requiere generación secuencial (más compute por token). Entender esto es clave para optimizar costos: reducir output tokens tiene más impacto que reducir input.',
    difficulty: 1,
  },
  {
    concept_id: 'token-economics',
    type: 'property',
    question_text: '¿Qué estrategias reducen costos de LLM APIs?',
    expected_answer: 'Prompt caching (reutilizar prefijos), model routing (queries simples a modelos baratos), semantic caching (evitar queries repetidas), batching (descuentos por volumen), reducir output length, y usar context compression para inputs largos.',
    difficulty: 2,
  },

  // --- model-routing ---
  {
    concept_id: 'model-routing',
    type: 'definition',
    question_text: '¿Qué es model routing y por qué no usar siempre el mejor modelo?',
    expected_answer: 'Es dirigir queries a diferentes modelos según complejidad, latencia requerida, y costo. No se usa siempre el mejor porque es caro y lento. Queries simples (ej: clasificación) pueden resolverse con modelos pequeños 10x más baratos con la misma calidad.',
    difficulty: 1,
  },
  {
    concept_id: 'model-routing',
    type: 'property',
    question_text: '¿Cómo se clasifica la complejidad de un query para routing?',
    expected_answer: 'Clasificador ligero (modelo pequeño o embeddings) que estima complejidad. Features: longitud del query, presencia de instrucciones complejas, dominio técnico. También se puede usar cascading: intentar con modelo barato primero, escalar al caro solo si la confianza es baja.',
    difficulty: 2,
  },

  // --- semantic-caching ---
  {
    concept_id: 'semantic-caching',
    type: 'definition',
    question_text: '¿Qué es semantic caching y cómo se diferencia del caching exacto?',
    expected_answer: 'Semantic caching usa embeddings para encontrar queries similares (no idénticas) y retornar respuestas cacheadas. A diferencia del caching exacto, "¿Qué es Python?" y "Explicame Python" pueden ser un cache hit. Usa un threshold de similitud coseno.',
    difficulty: 1,
  },
  {
    concept_id: 'semantic-caching',
    type: 'property',
    question_text: '¿Cuál es el riesgo principal del semantic caching?',
    expected_answer: 'Retornar respuestas incorrectas si el threshold es muy bajo: queries similares pero con diferentes intenciones ("Python programming" vs "Python snake"). Requiere tuning cuidadoso del threshold y posiblemente validación del contexto. Una respuesta cacheada incorrecta es peor que una generación lenta.',
    difficulty: 2,
  },

  // --- prompt-caching ---
  {
    concept_id: 'prompt-caching',
    type: 'definition',
    question_text: '¿Qué es prompt caching (prefix caching) y cómo difiere del semantic caching?',
    expected_answer: 'Prompt caching es una optimización API-level que cachea los KV states computados para prefijos de prompts. Si dos requests comparten el mismo system prompt, los tokens del prefijo se procesan una sola vez. A diferencia de semantic caching, requiere coincidencia exacta del prefijo.',
    difficulty: 1,
  },
  {
    concept_id: 'prompt-caching',
    type: 'fact',
    question_text: '¿Cuánto ahorro genera prompt caching y cómo se estructura el prompt para aprovecharlo?',
    expected_answer: 'Los tokens cacheados son 75-90% más baratos. Para aprovecharlo, se estructura: contenido estático primero (system prompt, instrucciones, context largo) y contenido variable al final (pregunta del usuario). El prefijo estático se cachea entre requests.',
    difficulty: 2,
  },

  // --- llm-observability ---
  {
    concept_id: 'llm-observability',
    type: 'definition',
    question_text: '¿En qué se diferencia LLM observability de la observability tradicional?',
    expected_answer: 'Además de métricas clásicas (latencia, errores), LLM observability requiere: tracking de prompts y completions, uso de tokens y costos por query, latency breakdown por componente (retrieval, generation), detección de hallucinations, y quality metrics a lo largo del tiempo.',
    difficulty: 1,
  },
  {
    concept_id: 'llm-observability',
    type: 'property',
    question_text: '¿Qué herramientas y patrones se usan para LLM observability?',
    expected_answer: 'Langfuse, LangSmith, Phoenix para tracing de LLM. Patrones: trace por request (incluye retrieval, prompts, completions), cost attribution (costo por feature/usuario), quality scoring automático, y alertas por degradación de métricas de calidad.',
    difficulty: 2,
  },

  // --- rate-limiting ---
  {
    concept_id: 'rate-limiting',
    type: 'definition',
    question_text: '¿Cuál es la diferencia entre rate limiting y backpressure?',
    expected_answer: 'Rate limiting es proactivo: establece un techo de requests (ej: 100 RPM) y rechaza los excedentes. Backpressure es reactivo: cuando un componente está sobrecargado, señala upstream para que reduzca la velocidad. Ambos previenen sobrecarga pero actúan en diferentes puntos.',
    difficulty: 1,
  },
  {
    concept_id: 'rate-limiting',
    type: 'property',
    question_text: '¿Por qué rate limiting es especialmente importante para LLM APIs?',
    expected_answer: 'Los LLM APIs tienen rate limits estrictos (tokens/min, requests/min). Un retry excesivo puede agotar el budget. Un burst de requests puede causar 429 errors en cascada. Se necesita: token bucket, exponential backoff, queue management, y priorización de requests.',
    difficulty: 2,
  },

  // --- compound-ai-systems ---
  {
    concept_id: 'compound-ai-systems',
    type: 'definition',
    question_text: '¿Qué son compound AI systems y por qué son el estado del arte?',
    expected_answer: 'Son sistemas de IA compuestos por múltiples componentes: modelos, retrievers, tools, guardrails, routers. Los resultados SOTA vienen del diseño del sistema, no solo de mejores modelos. 60% de apps LLM usan RAG, 30% usan multi-step chains.',
    difficulty: 1,
  },
  {
    concept_id: 'compound-ai-systems',
    type: 'property',
    question_text: '¿Qué desafíos de diseño introduce un compound AI system vs un solo modelo?',
    expected_answer: 'Optimización end-to-end (cada componente afecta al siguiente), debugging distribuido (¿dónde falló?), latency budgeting (cómo distribuir el tiempo entre componentes), error propagation (un componente malo afecta todo), y cost management (múltiples llamadas a LLM).',
    difficulty: 2,
  },

  // ==========================================================================
  // PHASE 6: Frameworks
  // ==========================================================================

  // --- langchain-architecture ---
  {
    concept_id: 'langchain-architecture',
    type: 'definition',
    question_text: '¿Cuáles son los componentes principales de LangChain?',
    expected_answer: 'Chains (secuencias de operaciones), Agents (LLM decide qué herramienta usar), Memory (contexto entre interacciones), Tools (funciones que el agente puede llamar), Retrievers (búsqueda en documentos), y Callbacks (observability).',
    difficulty: 1,
  },
  {
    concept_id: 'langchain-architecture',
    type: 'property',
    question_text: '¿Cuáles son las críticas principales a LangChain?',
    expected_answer: 'Abstraction overhead (muchas capas ocultan lo que pasa), debugging difícil (errores crípticos en chains profundas), performance costs (overhead innecesario), API inestable (breaking changes frecuentes), y "framework lock-in" que dificulta personalización.',
    difficulty: 2,
  },

  // --- llamaindex-architecture ---
  {
    concept_id: 'llamaindex-architecture',
    type: 'definition',
    question_text: '¿Qué diferencia a LlamaIndex de LangChain en enfoque?',
    expected_answer: 'LlamaIndex está enfocado en data ingestion y retrieval: conectores de datos, estrategias de indexación, query engines. LangChain es más general (chains, agents, tools). LlamaIndex es mejor para RAG-specific use cases. Ambos pueden usarse juntos.',
    difficulty: 1,
  },
  {
    concept_id: 'llamaindex-architecture',
    type: 'property',
    question_text: '¿Qué tipos de índices ofrece LlamaIndex?',
    expected_answer: 'Vector index (búsqueda semántica), Tree index (resumen jerárquico), List index (procesamiento secuencial), y Keyword Table index (búsqueda por keywords). Cada uno tiene trade-offs de costo, latencia, y calidad de retrieval.',
    difficulty: 2,
  },

  // --- framework-tradeoffs ---
  {
    concept_id: 'framework-tradeoffs',
    type: 'definition',
    question_text: '¿Cuándo conviene usar un framework vs construir desde cero?',
    expected_answer: 'Framework: prototipos rápidos, exploración, equipo sin experiencia LLM. Desde cero: producción con requisitos específicos, cuando necesitas control total (debugging, performance, personalización). Los mejores ingenieros entienden ambos caminos.',
    difficulty: 1,
  },
  {
    concept_id: 'framework-tradeoffs',
    type: 'comparison',
    question_text: '¿Qué se pierde al usar un framework en producción?',
    expected_answer: 'Control sobre el flujo de ejecución, visibilidad en errores internos, performance (overhead de abstracciones), y flexibilidad para optimizar (ej: prompt caching, batching específico). También dependency risk: updates del framework pueden romper tu código.',
    difficulty: 2,
    related_concept_id: 'minimal-implementations',
  },

  // --- minimal-implementations ---
  {
    concept_id: 'minimal-implementations',
    type: 'definition',
    question_text: '¿Qué demuestra poder construir RAG o un agent en ~200 líneas sin frameworks?',
    expected_answer: 'Que entiendes los mecanismos fundamentales: embedding + retrieval + prompt construction para RAG, tool calling + loop + parsing para agents. Sin este entendimiento, usar frameworks es "magia" que no puedes debuggear ni optimizar en producción.',
    difficulty: 1,
  },
  {
    concept_id: 'minimal-implementations',
    type: 'property',
    question_text: '¿Cuáles son los componentes mínimos de un RAG system desde cero?',
    expected_answer: 'Chunking (split documentos), embedding (vectorizar chunks), storage (guardar en vector DB), retrieval (buscar por similitud), y prompt construction (armar prompt con contexto + query). Con estas 5 piezas, tienes RAG funcional sin ningún framework.',
    difficulty: 2,
  },

  // --- dspy-programming ---
  {
    concept_id: 'dspy-programming',
    type: 'definition',
    question_text: '¿Qué es DSPy y en qué se diferencia del prompt engineering manual?',
    expected_answer: 'DSPy permite programar (no prompting) LLMs con módulos declarativos que se auto-optimizan. Defines la firma (input → output) y DSPy compila prompts efectivos automáticamente via optimización. Supera prompts manuales por 25-65% porque optimiza sistemáticamente.',
    difficulty: 2,
  },
  {
    concept_id: 'dspy-programming',
    type: 'property',
    question_text: '¿Qué es un DSPy "signature" y cómo funciona la compilación?',
    expected_answer: 'Un signature define input/output de un módulo (ej: "question -> answer"). La compilación usa un optimizador (ej: BootstrapFewShot) que genera y evalúa ejemplos, seleccionando los que mejor funcionan como few-shot demos. El resultado es un prompt optimizado para la tarea.',
    difficulty: 3,
  },

  // ==========================================================================
  // ADDITIONAL QUESTIONS — Concepts with < 3 questions
  // ==========================================================================

  // --- batching-inference ---
  {
    concept_id: 'batching-inference',
    type: 'definition',
    question_text: '¿Qué es batching en inferencia LLM y por qué mejora el throughput en GPU?',
    expected_answer: 'Batching agrupa múltiples requests de inferencia en un solo forward pass por la GPU. Mejora el throughput porque las GPUs están diseñadas para operaciones paralelas sobre matrices: procesar un batch de 8 secuencias cuesta casi lo mismo que procesar 1, ya que la latencia está dominada por memory bandwidth, no por cómputo aritmético.',
    difficulty: 1,
  },
  {
    concept_id: 'batching-inference',
    type: 'comparison',
    question_text: '¿Cuál es la diferencia entre static batching y continuous batching (iteration-level batching), y por qué continuous batching es superior para LLMs?',
    expected_answer: 'Static batching espera a acumular N requests y las procesa juntas, pero todas deben esperar a que termine la secuencia más larga del batch. Continuous batching (usado por vLLM, TGI) inserta y retira requests del batch a nivel de cada iteración de decode, así un request corto libera su slot inmediatamente sin esperar al más largo. Esto reduce latencia de cola y aumenta utilización de GPU significativamente.',
    difficulty: 3,
  },

  // --- compound-ai-systems ---
  {
    concept_id: 'compound-ai-systems',
    type: 'definition',
    question_text: '¿Qué es un compound AI system y en qué se diferencia de usar un solo modelo LLM?',
    expected_answer: 'Un compound AI system combina múltiples componentes: LLMs, retrievers, herramientas externas, bases de datos, verificadores y orquestadores. A diferencia de un solo LLM, permite descomponer tareas complejas en pasos especializados, mejorar precisión con retrieval, verificar outputs con validadores, y usar modelos más baratos para subtareas simples. Ejemplos: RAG, agentes con tools, pipelines de verificación.',
    difficulty: 1,
  },
  {
    concept_id: 'compound-ai-systems',
    type: 'property',
    question_text: '¿Cuáles son los principales desafíos de diseño al construir compound AI systems comparado con un solo LLM endpoint?',
    expected_answer: 'Los desafíos principales son: 1) Observabilidad end-to-end (tracing distribuido entre componentes), 2) Gestión de errores en cascada (si el retriever falla, el LLM alucina), 3) Optimización conjunta (mejorar un componente puede degradar otro), 4) Latencia acumulada (cada componente añade latencia), 5) Testing y evaluación del sistema completo vs componentes individuales. La complejidad operacional crece significativamente.',
    difficulty: 2,
  },

  // --- compute-optimal-training ---
  {
    concept_id: 'compute-optimal-training',
    type: 'fact',
    question_text: '¿Qué establecen las Chinchilla scaling laws sobre la relación óptima entre parámetros del modelo y tokens de entrenamiento?',
    expected_answer: 'Las Chinchilla scaling laws (Hoffmann et al., 2022) establecen que para un presupuesto de cómputo fijo, el número de parámetros y el número de tokens de entrenamiento deben escalarse en proporción aproximadamente igual. Concretamente, un modelo óptimo debe entrenarse con ~20 tokens por parámetro. Esto demostró que GPT-3 (175B params, 300B tokens) estaba sub-entrenado: un modelo más pequeño con más datos habría sido mejor.',
    difficulty: 2,
  },
  {
    concept_id: 'compute-optimal-training',
    type: 'comparison',
    question_text: '¿Cómo cambiaron las Chinchilla scaling laws la estrategia de entrenamiento respecto a las scaling laws de Kaplan et al. (2020), y por qué modelos como LLaMA 2 violan Chinchilla deliberadamente?',
    expected_answer: 'Kaplan et al. sugerían que era más eficiente escalar parámetros que datos, llevando a modelos enormes sub-entrenados. Chinchilla corrigió esto mostrando que ambos deben escalarse proporcionalmente. Sin embargo, LLaMA 2 (70B params, 2T tokens, ~29 tokens/param) entrena con más datos de lo óptimo en cómputo porque optimiza para eficiencia en inferencia: un modelo más pequeño pero mejor entrenado es más barato de servir en producción, aunque el entrenamiento cueste más.',
    difficulty: 3,
  },

  // --- constitutional-ai ---
  {
    concept_id: 'constitutional-ai',
    type: 'definition',
    question_text: '¿Qué es Constitutional AI (CAI) y cuáles son sus dos fases principales de entrenamiento?',
    expected_answer: 'Constitutional AI es un método de Anthropic para alinear LLMs usando un conjunto de principios (constitución) en lugar de feedback humano para cada output. Fase 1 (SL-CAI): el modelo genera respuestas, se le pide que las critique y revise según la constitución, y se hace fine-tuning supervisado con las revisiones. Fase 2 (RL-CAI): se usa RLHF pero el feedback viene de un modelo entrenado en la constitución (AI feedback) en vez de humanos.',
    difficulty: 2,
  },
  {
    concept_id: 'constitutional-ai',
    type: 'property',
    question_text: '¿Qué ventajas tiene Constitutional AI sobre RLHF puro con anotadores humanos para la alineación de modelos?',
    expected_answer: 'Las ventajas son: 1) Escalabilidad: no necesita miles de anotadores humanos para cada iteración, 2) Transparencia: los principios son explícitos y auditables, a diferencia de los criterios implícitos de anotadores, 3) Consistencia: elimina la varianza entre anotadores humanos, 4) Iterabilidad: modificar la constitución es más rápido que reentrenar anotadores, 5) Reduce la exposición de humanos a contenido dañino durante el proceso de alineación.',
    difficulty: 2,
  },

  // --- context-window-limits ---
  {
    concept_id: 'context-window-limits',
    type: 'fact',
    question_text: '¿Por qué la atención estándar del Transformer tiene complejidad O(n²) respecto a la longitud de contexto y qué consecuencia práctica tiene esto?',
    expected_answer: 'En self-attention, cada token atiende a todos los demás, generando una matriz de atención de n×n. Esto significa que duplicar el contexto cuadruplica la memoria y el cómputo. Prácticamente, un contexto de 128K tokens requiere ~16 veces más recursos que uno de 32K. Por eso se usan técnicas como sliding window attention, sparse attention, o Flash Attention (que optimiza el acceso a memoria sin cambiar la complejidad teórica).',
    difficulty: 2,
  },
  {
    concept_id: 'context-window-limits',
    type: 'property',
    question_text: '¿Qué es el "lost in the middle" problem y qué estrategias existen para mitigarlo al trabajar con contextos largos?',
    expected_answer: 'Lost in the middle (Liu et al., 2023) muestra que los LLMs tienen un sesgo de posición: recuerdan mejor la información al inicio y al final del contexto, pero degradan significativamente para información en el medio. Estrategias de mitigación: 1) Reordenar chunks poniendo los más relevantes al inicio/final, 2) Usar summarization jerárquica en vez de concatenar todo, 3) Recursive retrieval con contextos más cortos, 4) Instruction repetition (repetir la instrucción al final).',
    difficulty: 3,
  },

  // --- dpo ---
  {
    concept_id: 'dpo',
    type: 'definition',
    question_text: '¿Qué es Direct Preference Optimization (DPO) y cómo simplifica el pipeline de RLHF?',
    expected_answer: 'DPO (Rafailov et al., 2023) es un método de alineación que elimina la necesidad de entrenar un reward model separado y usar PPO. En vez de RL, DPO reformula el objetivo como una clasificación directa sobre pares de preferencia (respuesta ganadora vs perdedora), derivando un loss function cerrado que optimiza la misma política óptima que RLHF pero con supervised learning estándar. Esto reduce complejidad, inestabilidad de entrenamiento y costo computacional.',
    difficulty: 2,
  },
  {
    concept_id: 'dpo',
    type: 'comparison',
    question_text: '¿Cuáles son las ventajas y limitaciones de DPO frente a RLHF con PPO para alinear LLMs?',
    expected_answer: 'Ventajas de DPO: 1) No necesita reward model separado, 2) Entrenamiento más estable (sin RL), 3) Menor costo computacional, 4) Implementación más simple. Limitaciones: 1) Asume un modelo de reward implícito Bradley-Terry que puede no capturar preferencias complejas, 2) Sensible a la calidad de los pares de preferencia (necesita que las diferencias sean claras), 3) No explora nuevas respuestas durante el entrenamiento (solo optimiza sobre datos existentes), 4) Puede sufrir overfitting a los pares de entrenamiento.',
    difficulty: 3,
  },

  // --- embracing-risk ---
  {
    concept_id: 'embracing-risk',
    type: 'definition',
    question_text: '¿Qué es un error budget en el contexto de SRE y cómo se calcula a partir de un SLO?',
    expected_answer: 'Un error budget es la cantidad máxima de indisponibilidad o errores permitidos en un período, derivado del SLO. Si el SLO es 99.9% de availability mensual, el error budget es 0.1% del tiempo = ~43 minutos/mes. Mientras quede error budget, los equipos pueden deployar features y tomar riesgos. Si se agota, se congela el desarrollo y se prioriza fiabilidad. Es la herramienta que alinea incentivos entre producto (velocidad) e ingeniería (estabilidad).',
    difficulty: 1,
  },
  {
    concept_id: 'embracing-risk',
    type: 'property',
    question_text: '¿Por qué perseguir 100% de reliability es contraproducente según el modelo de embracing risk de Google SRE?',
    expected_answer: 'Es contraproducente por tres razones: 1) Costo marginal exponencial: pasar de 99.99% a 99.999% puede costar 100x más que de 99% a 99.9%, 2) Los usuarios no distinguen la diferencia porque otros componentes del path (red, ISP, dispositivo) tienen menor reliability, 3) Congela la innovación: cada deploy es riesgo, así que 100% reliability implica cero cambios. El nivel correcto de reliability es el mínimo que mantiene a los usuarios satisfechos, ni más ni menos.',
    difficulty: 2,
  },

  // --- external-memory ---
  {
    concept_id: 'external-memory',
    type: 'definition',
    question_text: '¿Qué es external memory para LLMs y qué tipos de almacenamiento externo se usan comúnmente?',
    expected_answer: 'External memory son sistemas de almacenamiento fuera del modelo que le permiten acceder a información más allá de sus parámetros y ventana de contexto. Tipos principales: 1) Vector databases (Pinecone, Weaviate, Qdrant) para búsqueda semántica, 2) Knowledge graphs para relaciones estructuradas, 3) Key-value stores para memoria episódica de conversaciones, 4) Bases de datos SQL/NoSQL para datos factuales actualizados. Se accede mediante retrieval en tiempo de inferencia.',
    difficulty: 1,
  },
  {
    concept_id: 'external-memory',
    type: 'property',
    question_text: '¿Cuáles son los trade-offs entre almacenar conocimiento en los parámetros del modelo (parametric memory) vs en external memory (non-parametric)?',
    expected_answer: 'Parametric memory (pesos del modelo): ventajas son velocidad de acceso (un forward pass), razonamiento más fluido; desventajas son que requiere reentrenamiento para actualizar, sufre de alucinaciones, no es auditable. Non-parametric (external): ventajas son actualización en tiempo real, trazabilidad (puedes citar fuentes), escalabilidad de conocimiento sin reentrenar; desventajas son latencia adicional por retrieval, dependencia de la calidad del retriever, fragmentación del razonamiento entre chunks. Lo óptimo es combinar ambos (RAG).',
    difficulty: 3,
  },

  // --- fine-tuning-efficiency ---
  {
    concept_id: 'fine-tuning-efficiency',
    type: 'definition',
    question_text: '¿Qué es LoRA (Low-Rank Adaptation) y por qué reduce drásticamente el costo de fine-tuning?',
    expected_answer: 'LoRA congela los pesos originales del modelo e inyecta matrices de bajo rango descomponibles (A y B donde A es d×r y B es r×d, con r << d) en las capas de atención. En vez de actualizar los millones de parámetros originales, solo entrena estas matrices pequeñas. Para un LLM de 7B parámetros con rank r=16, LoRA puede reducir los parámetros entrenables a ~0.1% del total, reduciendo memoria GPU, tiempo de entrenamiento y almacenamiento del adaptador a unos pocos MB.',
    difficulty: 2,
  },
  {
    concept_id: 'fine-tuning-efficiency',
    type: 'comparison',
    question_text: '¿Qué diferencia a QLoRA de LoRA estándar y cuáles son los trade-offs de cuantización durante fine-tuning?',
    expected_answer: 'QLoRA (Dettmers et al., 2023) cuantiza el modelo base a 4-bit NormalFloat (NF4) y aplica LoRA sobre ese modelo cuantizado, usando double quantization y paged optimizers para manejar memory spikes. Esto permite fine-tunear un modelo de 65B en una sola GPU de 48GB. Trade-offs: ventajas son reducción de memoria ~4x vs LoRA estándar sin pérdida significativa de calidad; desventajas son velocidad de entrenamiento ~30% más lenta por la de-cuantización on-the-fly, y mayor complejidad de implementación. La calidad final es sorprendentemente comparable al fine-tuning completo en 16-bit.',
    difficulty: 3,
  },
  // --- foundation-models ---
  {
    concept_id: 'foundation-models',
    type: 'comparison',
    question_text: '¿Cuál es la diferencia entre fine-tuning y prompting como métodos de adaptación de foundation models a tareas específicas?',
    expected_answer: 'Fine-tuning ajusta los pesos del modelo con datos específicos de la tarea, logrando mejor performance pero requiriendo GPU, datos etiquetados y riesgo de catastrophic forgetting. Prompting (zero-shot, few-shot, instruction) no modifica pesos: es más barato y flexible pero depende de que la capacidad ya exista en el modelo base. Fine-tuning es preferible cuando hay datos abundantes y requisitos estrictos; prompting cuando se necesita generalidad y rapidez.',
    difficulty: 2,
  },
  {
    concept_id: 'foundation-models',
    type: 'fact',
    question_text: '¿Qué predicen las scaling laws de Kaplan/Chinchilla sobre el entrenamiento de foundation models?',
    expected_answer: 'Kaplan (OpenAI) mostró que la performance escala como power law con compute, datos y parámetros. Chinchilla (DeepMind) refinó esto: el tamaño óptimo del dataset debe escalar proporcionalmente al número de parámetros. Un modelo de 70B necesita ~1.4T tokens. Entrenar modelos más pequeños con más datos (como Llama) puede ser más eficiente en inference que modelos grandes sub-entrenados.',
    difficulty: 3,
  },

  // --- kv-cache ---
  {
    concept_id: 'kv-cache',
    type: 'comparison',
    question_text: '¿Qué diferencia hay entre la fase de prefill y la fase de decode en relación al KV cache?',
    expected_answer: 'En prefill, se procesan todos los tokens del prompt en paralelo y se llenan los KV caches de todas las capas de una vez. Es compute-bound (mucha computación matricial). En decode, se genera un token a la vez, se consulta el KV cache existente y se añade la nueva entrada K/V. Es memory-bound (el cuello de botella es leer el KV cache de memoria). Por eso prefill tiene alta utilización de GPU y decode es menos eficiente.',
    difficulty: 3,
  },
  {
    concept_id: 'kv-cache',
    type: 'fact',
    question_text: '¿Qué es PagedAttention (vLLM) y qué problema del KV cache resuelve?',
    expected_answer: 'PagedAttention gestiona el KV cache como páginas de memoria virtual en lugar de bloques contiguos. Resuelve la fragmentación de memoria: sin él, cada request necesita reservar memoria contigua para el máximo de tokens posible, desperdiciando hasta un 60-80%. Con PagedAttention, el KV cache se asigna en bloques pequeños on-demand, permitiendo mayor batch size y mejor utilización de memoria GPU.',
    difficulty: 3,
  },

  // --- llamaindex-architecture ---
  {
    concept_id: 'llamaindex-architecture',
    type: 'fact',
    question_text: '¿Qué es un NodeParser en LlamaIndex y por qué es crítico para la calidad del retrieval?',
    expected_answer: 'Un NodeParser toma Documents y los divide en Nodes (chunks). Es crítico porque la granularidad del chunking determina la calidad del retrieval: chunks muy grandes incluyen ruido, chunks muy pequeños pierden contexto. LlamaIndex ofrece SentenceSplitter, TokenTextSplitter, y SemanticSplitter. La elección afecta directamente precision y recall del sistema RAG.',
    difficulty: 2,
  },
  {
    concept_id: 'llamaindex-architecture',
    type: 'comparison',
    question_text: '¿Cuál es la diferencia entre un QueryEngine y un ChatEngine en LlamaIndex?',
    expected_answer: 'QueryEngine es stateless: recibe una query, busca en el índice, y genera una respuesta. Cada query es independiente. ChatEngine es stateful: mantiene historial de conversación, puede hacer follow-up queries usando contexto previo. Internamente, ChatEngine usa un QueryEngine pero añade memory management y condensación de queries (reformula la pregunta del usuario usando el historial).',
    difficulty: 2,
  },

  // --- lost-in-middle ---
  {
    concept_id: 'lost-in-middle',
    type: 'fact',
    question_text: '¿Qué relación tiene el efecto "lost in the middle" con el mecanismo de attention en transformers?',
    expected_answer: 'El attention tiende a asignar más peso a tokens recientes (recency bias por la posición) y a tokens iniciales (por el patrón de "attention sink" donde los primeros tokens acumulan attention desproporcionada). Los tokens en posiciones intermedias reciben menos attention, lo que explica la curva en U del rendimiento. Esto es un artefacto del entrenamiento con positional encodings y patrones de attention aprendidos.',
    difficulty: 3,
  },
  {
    concept_id: 'lost-in-middle',
    type: 'comparison',
    question_text: '¿Cómo afecta el "lost in the middle" de manera diferente a tareas de question answering vs summarización?',
    expected_answer: 'En question answering, el efecto es severo: si el fragmento con la respuesta está en el medio, la accuracy baja significativamente. En summarización, el impacto es menor pero diferente: los modelos tienden a sobre-representar información del inicio y final del documento. Para QA se mitiga reordenando documentos; para summarización se usan técnicas de map-reduce o chunking con resúmenes parciales.',
    difficulty: 3,
  },

  // --- maintainability ---
  {
    concept_id: 'maintainability',
    type: 'guarantee',
    question_text: '¿Qué garantiza la operability como pilar de la maintainability y qué prácticas concretas la soportan?',
    expected_answer: 'Operability garantiza que el equipo de operaciones pueda mantener el sistema corriendo de manera eficiente. Prácticas concretas: buen monitoring con visibilidad del estado interno, soporte para automatización (evitar procesos manuales), documentación operacional, comportamiento predecible ante fallos, buenas herramientas de deploy y rollback, y evitar dependencia en individuos específicos.',
    difficulty: 2,
  },
  {
    concept_id: 'maintainability',
    type: 'comparison',
    question_text: '¿Cuál es la diferencia entre complejidad esencial y complejidad accidental en el contexto de maintainability?',
    expected_answer: 'Complejidad esencial es inherente al problema que se resuelve y no se puede eliminar. Complejidad accidental es introducida por la implementación: malas abstracciones, code duplication, coupling innecesario, estado global. La meta de la maintainability es minimizar la complejidad accidental sin simplificar la esencial. Moseley y Marks (Out of the Tar Pit) argumentan que la mayoría de la complejidad en software real es accidental.',
    difficulty: 3,
  },

  // --- memory-management ---
  {
    concept_id: 'memory-management',
    type: 'comparison',
    question_text: '¿Cuál es la diferencia entre sliding window memory y summarization memory para conversaciones con LLMs?',
    expected_answer: 'Sliding window mantiene los últimos N mensajes literales y descarta los anteriores. Es simple pero pierde contexto importante de turnos antiguos. Summarization condensa mensajes antiguos en un resumen que se prepend al contexto. Preserva información clave pero introduce distorsión del resumen y costo de la llamada al LLM para resumir. Sliding window es O(1) en tokens; summarization es variable pero más eficiente en retención de información.',
    difficulty: 2,
  },
  {
    concept_id: 'memory-management',
    type: 'fact',
    question_text: '¿Qué es retrieval-augmented memory y cómo difiere de las estrategias de ventana?',
    expected_answer: 'Retrieval-augmented memory almacena todos los turnos de conversación en un vector store y, ante cada nueva query, recupera los K turnos más relevantes semánticamente. A diferencia de sliding window (recencia) o summarization (compresión), selecciona por relevancia. Ventaja: puede recuperar contexto de hace muchos turnos si es relevante. Desventaja: requiere embeddings, vector store, y la calidad depende del retrieval. Ideal para conversaciones largas donde la información relevante no es necesariamente la más reciente.',
    difficulty: 3,
  },

  // --- monitoring ---
  {
    concept_id: 'monitoring',
    type: 'property',
    question_text: '¿Qué es distributed tracing y por qué es esencial en arquitecturas de microservicios?',
    expected_answer: 'Distributed tracing sigue un request a través de múltiples servicios asignándole un trace ID único. Cada servicio genera spans (unidades de trabajo) que forman un árbol de llamadas. Es esencial porque en microservicios un request toca 5-20 servicios: sin tracing, diagnosticar latencia o errores requeriría correlacionar logs manualmente. Herramientas: Jaeger, Zipkin, OpenTelemetry. Permite identificar exactamente qué servicio introduce latencia o errores.',
    difficulty: 2,
  },
  {
    concept_id: 'monitoring',
    type: 'guarantee',
    question_text: '¿Qué garantiza un sistema de alerting bien diseñado y qué problemas tiene el alerting mal configurado?',
    expected_answer: 'Un buen alerting garantiza que los operadores se enteren de problemas reales a tiempo para actuar. Debe tener: alta signal-to-noise ratio (pocas falsas alarmas), actionable alerts (cada alerta tiene un runbook), severidad escalonada (page solo para críticos). Alerting mal configurado causa alert fatigue: demasiadas alertas falsas hacen que se ignoren las reales. Regla clave: si una alerta no requiere acción humana inmediata, no debería despertar a nadie.',
    difficulty: 2,
  },

  // --- multimodal-embeddings ---
  {
    concept_id: 'multimodal-embeddings',
    type: 'comparison',
    question_text: '¿Cuál es la diferencia entre CLIP e ImageBind en cuanto a modalidades y arquitectura?',
    expected_answer: 'CLIP alinea dos modalidades (texto e imagen) usando contrastive learning con pares imagen-texto. ImageBind (Meta) alinea seis modalidades (imagen, texto, audio, profundidad, térmico, IMU) usando imagen como ancla: cada modalidad se alinea con imagen, y por transitividad se alinean entre sí. ImageBind no necesita pares de todas las combinaciones, solo pares con imagen. Esto permite búsqueda cross-modal entre modalidades que nunca se entrenaron juntas directamente.',
    difficulty: 3,
  },
  {
    concept_id: 'multimodal-embeddings',
    type: 'property',
    question_text: '¿Qué es el modality gap en embeddings multimodales y cómo afecta la calidad del retrieval?',
    expected_answer: 'El modality gap es la separación sistemática entre embeddings de diferentes modalidades en el espacio compartido: los embeddings de texto e imágenes forman clusters separados incluso para conceptos equivalentes. Esto reduce la efectividad del retrieval cross-modal porque la distancia inter-modalidad es mayor que la intra-modalidad. Se mitiga con normalización, fine-tuning contrastivo más agresivo, o proyecciones adicionales que cierran el gap.',
    difficulty: 3,
  },

  // --- offline-evaluation ---
  {
    concept_id: 'offline-evaluation',
    type: 'comparison',
    question_text: '¿Cuál es la diferencia entre métricas de referencia (BLEU, ROUGE) y métricas sin referencia (LLM-as-judge) para evaluar LLMs?',
    expected_answer: 'Métricas de referencia comparan el output contra un gold standard: BLEU mide n-gram overlap (traducción), ROUGE mide recall de n-gramas (resumen). Son baratas y determinísticas pero penalizan paráfrasis válidas y no capturan corrección semántica. Métricas sin referencia como LLM-as-judge evalúan calidad directamente sin gold standard: capturan semántica pero son estocásticas, tienen bias (verbosity bias, position bias) y cuestan tokens. Lo ideal es combinar ambas.',
    difficulty: 2,
  },
  {
    concept_id: 'offline-evaluation',
    type: 'fact',
    question_text: '¿Qué es benchmark contamination y por qué amenaza la validez de la evaluación offline?',
    expected_answer: 'Benchmark contamination ocurre cuando los datos de un benchmark de evaluación aparecen en el training set del modelo. El modelo "memoriza" las respuestas en lugar de razonar. Amenaza la validez porque infla artificialmente los scores: un modelo puede obtener 90% en MMLU no por comprensión sino por memorización. Se mitiga con held-out test sets, benchmarks dinámicos (nuevos cada ciclo), y canary strings para detectar contaminación. Es un problema creciente con datasets de entrenamiento cada vez más grandes.',
    difficulty: 3,
  },
  // --- online-evaluation (batch 3) ---
  {
    concept_id: 'online-evaluation',
    type: 'comparison',
    question_text: '¿Cuál es la diferencia entre un A/B test y un canary deployment para evaluar un nuevo modelo LLM en producción?',
    expected_answer: 'A/B test divide el tráfico en dos grupos estables para medir diferencias estadísticas entre modelos (requiere tamaño de muestra y duración). Canary deployment expone el nuevo modelo a un porcentaje pequeño (1-5%) y se escala gradualmente, priorizando detectar fallos graves rápidamente antes de hacer rollout completo. A/B test optimiza para significancia estadística, canary para mitigación de riesgo.',
    difficulty: 2,
  },
  {
    concept_id: 'online-evaluation',
    type: 'property',
    question_text: '¿Qué es el guardrail hit rate como métrica de online evaluation y por qué su tendencia importa más que su valor absoluto?',
    expected_answer: 'Guardrail hit rate es el porcentaje de requests que activan un guardrail (safety filter, output validator, etc.). Un valor absoluto alto puede ser esperado (usuarios adversariales). Lo importante es la tendencia: si sube repentinamente tras un deploy, indica regresión del modelo. Si baja tras ajustes de prompt, confirma mejora. Se debe desglosar por tipo de guardrail para identificar la causa raíz.',
    difficulty: 3,
  },

  // --- paged-attention (batch 3) ---
  {
    concept_id: 'paged-attention',
    type: 'property',
    question_text: '¿Cómo funciona el memory sharing en PagedAttention y en qué escenarios es especialmente útil?',
    expected_answer: 'PagedAttention permite que múltiples secuencias compartan bloques de KV cache mediante copy-on-write: las páginas se comparten hasta que una secuencia diverge. Es especialmente útil en beam search (todas las beams comparten el prefijo), parallel sampling (múltiples completions del mismo prompt), y prefix sharing (requests con el mismo system prompt). Reduce uso de memoria hasta 55% en beam search.',
    difficulty: 3,
  },
  {
    concept_id: 'paged-attention',
    type: 'comparison',
    question_text: '¿Qué analogía existe entre PagedAttention y la memoria virtual de un sistema operativo?',
    expected_answer: 'PagedAttention mapea bloques lógicos (posiciones en la secuencia) a bloques físicos (memoria GPU) mediante una page table, igual que un OS mapea páginas virtuales a frames físicos. Los bloques no necesitan ser contiguos en memoria física. Permite asignación dinámica (no pre-allocar el máximo), eliminación de fragmentación externa, y técnicas como copy-on-write. La diferencia es que opera sobre KV cache en GPU en lugar de RAM.',
    difficulty: 2,
  },

  // --- plan-and-execute (batch 3) ---
  {
    concept_id: 'plan-and-execute',
    type: 'fact',
    question_text: '¿Cuáles son las estrategias de re-planning en Plan-and-Execute y cuáles son sus trade-offs?',
    expected_answer: 'Re-plan completo: regenera todo el plan desde cero con los resultados obtenidos (costoso pero robusto). Re-plan parcial: solo ajusta los pasos restantes manteniendo los completados (más eficiente, riesgo de plan inconsistente). Re-plan condicional: solo re-planifica si el resultado de un paso difiere significativamente de lo esperado (requiere criterios de divergencia). El trade-off principal es costo en tokens/latencia vs robustez ante cambios.',
    difficulty: 3,
  },
  {
    concept_id: 'plan-and-execute',
    type: 'comparison',
    question_text: '¿En qué se diferencia Plan-and-Execute de un sistema DAG-based (como un workflow engine)?',
    expected_answer: 'Plan-and-Execute genera el plan dinámicamente con un LLM y puede re-planificar ante resultados inesperados: el plan es flexible. Un DAG-based workflow tiene pasos y dependencias predefinidos: es más predecible y auditable pero rígido. Plan-and-Execute es mejor para tareas abiertas y ambiguas. DAG es mejor para pipelines conocidos con steps determinísticos. Se pueden combinar: Plan-and-Execute para decidir qué workflow ejecutar.',
    difficulty: 3,
    related_concept_id: 'react-pattern',
  },

  // --- positional-encoding (batch 3) ---
  {
    concept_id: 'positional-encoding',
    type: 'property',
    question_text: '¿Cómo funciona RoPE (Rotary Position Embeddings) y por qué facilita la extrapolación a secuencias largas?',
    expected_answer: 'RoPE codifica la posición rotando los vectores de query y key en pares de dimensiones, usando ángulos proporcionales a la posición. La atención entre dos tokens depende solo de su distancia relativa (no absoluta) porque el dot product de vectores rotados cancela las posiciones absolutas y deja solo la diferencia. Esto facilita la extrapolación porque el modelo aprende relaciones de distancia relativa que se generalizan a posiciones no vistas en training.',
    difficulty: 3,
  },
  {
    concept_id: 'positional-encoding',
    type: 'comparison',
    question_text: '¿Cuál es la diferencia entre ALiBi y RoPE como métodos de positional encoding?',
    expected_answer: 'ALiBi (Attention with Linear Biases) no modifica los embeddings: añade un bias lineal negativo a los attention scores proporcional a la distancia entre tokens (-m * |i-j|), penalizando posiciones lejanas. RoPE modifica los vectores Q y K mediante rotación. ALiBi es más simple, no tiene parámetros aprendibles, y extrapola bien sin fine-tuning. RoPE es más expresivo pero requiere técnicas como NTK-aware scaling para extrapolar. ALiBi tiende a favorecer localidad; RoPE es más flexible con dependencias largas.',
    difficulty: 3,
    related_concept_id: 'transformer-architecture',
  },

  // --- prompt-caching (batch 3) ---
  {
    concept_id: 'prompt-caching',
    type: 'property',
    question_text: '¿Qué condiciones deben cumplirse para que un cache hit ocurra en prompt caching y qué lo invalida?',
    expected_answer: 'El prefijo debe ser idéntico byte a byte (mismo system prompt, mismos ejemplos few-shot, mismo orden). Cualquier cambio en un carácter invalida el cache desde ese punto. También debe usarse el mismo modelo y, en algunos providers, los mismos parámetros (temperature no afecta, pero el modelo sí). El cache tiene TTL (típicamente 5-60 min en Anthropic/OpenAI) y se invalida si expira o si el proveedor lo evicta por presión de memoria.',
    difficulty: 2,
  },
  {
    concept_id: 'prompt-caching',
    type: 'comparison',
    question_text: '¿Cuál es la diferencia entre prompt caching (prefix caching) y KV cache reutilización dentro de una misma request?',
    expected_answer: 'KV cache intra-request es la reutilización estándar durante autoregressive generation: cada nuevo token reutiliza los KV states ya computados de tokens anteriores en la misma secuencia. Prompt caching (prefix caching) es inter-request: reutiliza KV states computados en requests anteriores que comparten el mismo prefijo. KV cache es automático y siempre ocurre. Prompt caching requiere diseño intencional del prompt y depende del proveedor.',
    difficulty: 3,
  },

  // --- prompt-engineering (batch 3) ---
  {
    concept_id: 'prompt-engineering',
    type: 'property',
    question_text: '¿Qué es chain-of-thought prompting y cuándo NO es beneficioso usarlo?',
    expected_answer: 'Chain-of-thought (CoT) pide al modelo que razone paso a paso antes de dar la respuesta final. No es beneficioso cuando: la tarea es simple y no requiere razonamiento (clasificación binaria trivial), cuando la latencia es crítica (CoT genera muchos más tokens), cuando el costo por token importa, o cuando el modelo puede confundirse con su propio razonamiento en tareas que domina bien sin CoT. También puede empeorar en tareas puramente de retrieval o lookup.',
    difficulty: 2,
  },
  {
    concept_id: 'prompt-engineering',
    type: 'guarantee',
    question_text: '¿Por qué los few-shot examples pueden causar un sesgo negativo y cómo se mitiga?',
    expected_answer: 'Los ejemplos few-shot pueden introducir: sesgo de distribución (si todos los ejemplos son de una categoría, el modelo favorece esa categoría), sesgo de formato (el modelo copia el estilo exacto incluso cuando no aplica), y anchoring (el modelo se ancla a los valores numéricos de los ejemplos). Se mitiga con: ejemplos diversos que cubran el espacio de posibilidades, variación en longitud y estilo de respuesta, e incluir contraejemplos. El orden de los ejemplos también importa por recency bias.',
    difficulty: 3,
  },

  // --- quantization (batch 3) ---
  {
    concept_id: 'quantization',
    type: 'property',
    question_text: '¿Qué son los outlier features y por qué dificultan la quantización de LLMs?',
    expected_answer: 'Los outlier features son activaciones con magnitudes 10-100x mayores que el promedio, presentes en pocas dimensiones pero en casi todas las capas de LLMs grandes (>6.7B parámetros). Dificultan la quantización porque comprimen el rango de representación: si un INT8 debe cubrir valores de -100 a 100 por un outlier, los valores normales (-1 a 1) pierden resolución. Soluciones: LLM.int8() separa outliers en FP16 y cuantiza el resto, SmoothQuant redistribuye la dificultad entre pesos y activaciones.',
    difficulty: 3,
  },
  {
    concept_id: 'quantization',
    type: 'comparison',
    question_text: '¿Cuál es la diferencia entre GPTQ y AWQ como métodos de post-training quantization?',
    expected_answer: 'GPTQ (Generative Pre-trained Transformer Quantization) cuantiza los pesos capa por capa minimizando el error de reconstrucción del output de cada capa usando un set de calibración. AWQ (Activation-aware Weight Quantization) observa que un pequeño porcentaje de pesos (1%) son críticos porque corresponden a canales con activaciones grandes, y los protege escalándolos antes de cuantizar. AWQ es más rápido de aplicar y suele preservar mejor la calidad en INT4 que GPTQ.',
    difficulty: 3,
  },

  // --- rate-limiting (batch 3) ---
  {
    concept_id: 'rate-limiting',
    type: 'fact',
    question_text: '¿Cómo funciona el algoritmo token bucket y por qué es preferido sobre fixed window para rate limiting?',
    expected_answer: 'Token bucket tiene un bucket con capacidad máxima B que se llena a tasa R tokens/segundo. Cada request consume un token; si el bucket está vacío, se rechaza. Permite bursts (hasta B requests instantáneos) mientras mantiene el rate promedio en R. Fixed window tiene el problema del boundary burst: un usuario puede hacer 2x requests en la frontera entre dos ventanas. Token bucket suaviza el tráfico naturalmente y es más justo.',
    difficulty: 2,
  },
  {
    concept_id: 'rate-limiting',
    type: 'comparison',
    question_text: '¿Cómo se implementa rate limiting dual (por requests y por tokens) para LLM APIs y por qué se necesitan ambos?',
    expected_answer: 'Se necesitan dos rate limiters independientes porque una request corta y una de 100K tokens tienen costos muy diferentes. Rate limit por requests previene sobrecarga de conexiones y overhead de procesamiento. Rate limit por tokens previene abuso de compute y controla costos. Se implementa con dos token buckets separados: uno para request count y otro para token count (estimado pre-request o medido post-request). Ambos deben pasar para que la request proceda.',
    difficulty: 3,
  },

  // --- reasoning-models (batch 3) ---
  {
    concept_id: 'reasoning-models',
    type: 'property',
    question_text: '¿Qué es el "thinking budget" en reasoning models y cómo afecta el trade-off entre calidad y costo?',
    expected_answer: 'El thinking budget limita cuántos tokens puede usar el modelo en su cadena de razonamiento interna antes de dar la respuesta. Más tokens de razonamiento generalmente mejoran la calidad en tareas complejas pero aumentan latencia y costo proporcionalmente. Algunas tareas simples no se benefician de razonamiento extenso. El control del thinking budget permite ajustar el trade-off por query: razonamiento corto para preguntas fáciles, largo para problemas complejos.',
    difficulty: 2,
  },
  {
    concept_id: 'reasoning-models',
    type: 'comparison',
    question_text: '¿Cuándo conviene usar un reasoning model (o1/R1) vs un modelo estándar con chain-of-thought prompting?',
    expected_answer: 'Reasoning models son superiores en: math compleja, code generation con lógica intrincada, problemas multi-paso con dependencias. Un modelo estándar con CoT es preferible cuando: la tarea es moderadamente simple (CoT basta), se necesita control sobre el formato de razonamiento, el costo es crítico (reasoning models cuestan 3-10x más por los tokens de thinking), o se requiere baja latencia. Reasoning models también pueden sobre-pensar tareas simples, empeorando el output.',
    difficulty: 3,
  },
  // --- red-teaming ---
  {
    concept_id: 'red-teaming',
    type: 'definition',
    question_text: '¿Qué es red teaming en el contexto de sistemas de IA y cuál es su objetivo principal?',
    expected_answer: 'Red teaming es un proceso de prueba adversarial donde evaluadores intentan provocar comportamientos no deseados en un modelo: jailbreaks, outputs tóxicos, filtraciones de datos del sistema prompt, o sesgos. El objetivo es encontrar modos de falla antes del deployment, no solo validar el happy path.',
    difficulty: 1,
  },
  {
    concept_id: 'red-teaming',
    type: 'comparison',
    question_text: '¿Cuál es la diferencia entre red teaming manual y red teaming automatizado (automated red teaming)? ¿Cuándo es preferible cada uno?',
    expected_answer: 'Manual usa humanos creativos que prueban ataques novedosos (jailbreaks lingüísticos, context manipulation), es mejor para encontrar vulnerabilidades inesperadas. Automatizado usa otro LLM para generar adversarial prompts a escala (e.g., Perez et al. 2022), es mejor para cobertura amplia y regresión. Lo ideal es combinar ambos: automatizado para superficie amplia, manual para profundidad y ataques multi-turno sofisticados.',
    difficulty: 2,
  },

  // --- reflexion ---
  {
    concept_id: 'reflexion',
    type: 'definition',
    question_text: '¿Qué es el patrón Reflexion en agentes y cómo difiere de un simple retry?',
    expected_answer: 'Reflexion (Shinn et al. 2023) es un patrón donde el agente, tras fallar una tarea, genera una reflexión verbal sobre qué salió mal y la almacena en una memoria episódica. En intentos futuros, consulta esas reflexiones para evitar los mismos errores. A diferencia de un retry simple, Reflexion mantiene estado entre intentos y aprende explícitamente de los fracasos sin actualizar pesos del modelo.',
    difficulty: 2,
  },
  {
    concept_id: 'reflexion',
    type: 'property',
    question_text: '¿Cuáles son los tres componentes principales del loop de Reflexion y qué rol cumple cada uno?',
    expected_answer: 'Actor: ejecuta la acción y produce un output. Evaluator: determina si el output es correcto o genera una señal de feedback (puede ser heurístico o LLM-based). Self-Reflection: el agente analiza el feedback y genera un resumen textual del error que se almacena en la memoria episódica para futuros intentos. Este ciclo se repite hasta éxito o límite de intentos.',
    difficulty: 2,
  },

  // --- scaling-laws ---
  {
    concept_id: 'scaling-laws',
    type: 'fact',
    question_text: '¿Qué descubrieron Kaplan et al. (2020) sobre la relación entre performance y tamaño del modelo?',
    expected_answer: 'Descubrieron power laws: la loss del modelo decrece como función potencial del número de parámetros (N), tamaño del dataset (D), y compute (C). Las relaciones son L ∝ N^(-αN), L ∝ D^(-αD), L ∝ C^(-αC). Además, el cuello de botella suele ser parámetros antes que datos, sugiriendo modelos grandes entrenados menos tiempo.',
    difficulty: 2,
  },
  {
    concept_id: 'scaling-laws',
    type: 'comparison',
    question_text: '¿En qué difiere la recomendación de Chinchilla (Hoffmann et al. 2022) respecto a las scaling laws originales de Kaplan?',
    expected_answer: 'Kaplan sugería que escalar parámetros era más eficiente que escalar datos, llevando a modelos grandes entrenados poco. Chinchilla demostró que parámetros y datos deben escalarse proporcionalmente: un modelo compute-optimal tiene ~20 tokens por parámetro. Chinchilla (70B params, 1.4T tokens) superó a Gopher (280B params, 300B tokens) con 4x menos parámetros, probando que muchos modelos grandes estaban sub-entrenados.',
    difficulty: 3,
  },

  // --- self-consistency ---
  {
    concept_id: 'self-consistency',
    type: 'definition',
    question_text: '¿Qué es self-consistency y cómo mejora sobre chain-of-thought estándar?',
    expected_answer: 'Self-consistency (Wang et al. 2022) samplea múltiples cadenas de razonamiento (con temperature > 0) para la misma pregunta y toma la respuesta final por voto mayoritario. Mejora sobre CoT estándar (greedy decoding) porque un solo path puede tener errores de razonamiento, pero es improbable que la mayoría de paths independientes cometan el mismo error. Típicamente mejora 5-15% en benchmarks de razonamiento.',
    difficulty: 1,
  },
  {
    concept_id: 'self-consistency',
    type: 'property',
    question_text: '¿Cuáles son las limitaciones principales de self-consistency y cuándo no funciona bien?',
    expected_answer: 'Costo: requiere N llamadas al modelo (típicamente 5-40 samples), multiplicando latencia y costo. No funciona bien en tareas abiertas sin respuesta discreta (escritura creativa, resumen) porque el voto mayoritario necesita respuestas comparables. Tampoco ayuda si el modelo tiene un sesgo sistemático: si todos los paths cometen el mismo error por falta de conocimiento, la mayoría sigue siendo incorrecta.',
    difficulty: 2,
  },

  // --- speculative-decoding ---
  {
    concept_id: 'speculative-decoding',
    type: 'definition',
    question_text: '¿Cómo funciona speculative decoding y por qué acelera la inferencia sin cambiar la distribución de salida?',
    expected_answer: 'Un modelo draft pequeño y rápido genera K tokens candidatos autorregressivamente. El modelo target grande verifica todos los K tokens en un solo forward pass (paralelo). Los tokens que el target acepta se conservan; al primer rechazo se re-samplea desde la distribución del target. Esto funciona porque la verificación paralela de K tokens cuesta lo mismo que generar 1 token en el modelo grande. La distribución final es idéntica a la del target porque se usa rejection sampling.',
    difficulty: 2,
  },
  {
    concept_id: 'speculative-decoding',
    type: 'property',
    question_text: '¿Qué factores determinan el speedup de speculative decoding y cuál es la tasa de aceptación típica?',
    expected_answer: 'El speedup depende de: (1) la tasa de aceptación α (qué tan bien el draft approxima al target), típicamente 70-90% en tokens fáciles; (2) el ratio de velocidad entre draft y target; (3) el número K de tokens especulados por paso. El speedup teórico es ~1/(1-α) × (costo_target/costo_draft). En la práctica se logra 2-3x speedup. Funciona peor en dominios donde el draft diverge mucho del target (código especializado, idiomas raros).',
    difficulty: 3,
  },

  // --- test-time-compute ---
  {
    concept_id: 'test-time-compute',
    type: 'definition',
    question_text: '¿Qué es test-time compute y cuáles son las principales estrategias para utilizarlo?',
    expected_answer: 'Test-time compute es invertir más cómputo durante la inferencia (no el entrenamiento) para mejorar la calidad del output. Estrategias principales: (1) cadenas de razonamiento más largas (CoT extendido), (2) sampling múltiple con selección (best-of-N, self-consistency), (3) verificación y refinamiento iterativo, (4) búsqueda en árbol (tree-of-thoughts). OpenAI o1 es el ejemplo canónico: usa chain-of-thought interno extenso con RL para aprender a pensar más.',
    difficulty: 1,
  },
  {
    concept_id: 'test-time-compute',
    type: 'comparison',
    question_text: '¿Cuándo es más eficiente invertir en test-time compute versus en train-time compute (modelos más grandes)?',
    expected_answer: 'Test-time compute es más eficiente cuando: (1) el problema requiere razonamiento multi-paso donde errores se propagan, (2) se necesita precisión alta en un dominio estrecho, (3) el costo por query es tolerable. Train-time compute es mejor cuando: (1) se necesita baja latencia, (2) el volumen de queries es masivo, (3) el conocimiento requerido es broad y factual. Snell et al. (2024) mostraron que en problemas difíciles, un modelo pequeño con test-time compute puede superar a un modelo 14x más grande.',
    difficulty: 3,
  },

  // --- tree-of-thoughts ---
  {
    concept_id: 'tree-of-thoughts',
    type: 'definition',
    question_text: '¿Qué es Tree of Thoughts (ToT) y cómo generaliza chain-of-thought?',
    expected_answer: 'Tree of Thoughts (Yao et al. 2023) extiende CoT explorando múltiples ramas de razonamiento en estructura de árbol. En cada nodo, el LLM genera varios "pensamientos" candidatos. Un evaluador (el mismo LLM o heurística) valora cada rama. Se usa BFS o DFS con backtracking para explorar el espacio. Generaliza CoT (una sola cadena) y self-consistency (múltiples cadenas independientes) permitiendo exploración deliberada con retroceso.',
    difficulty: 1,
  },
  {
    concept_id: 'tree-of-thoughts',
    type: 'property',
    question_text: '¿Cuáles son las limitaciones prácticas de Tree of Thoughts y en qué tipos de problemas se justifica usarlo?',
    expected_answer: 'Limitaciones: (1) costo computacional explosivo: cada nodo requiere múltiples llamadas al LLM para generar y evaluar, puede ser 10-100x más caro que CoT simple; (2) la calidad depende críticamente del evaluador de estados intermedios; (3) la descomposición en pasos discretos no es natural para todos los problemas. Se justifica en problemas con: espacio de búsqueda combinatorio (puzzles, planificación), donde el backtracking es esencial, y donde una respuesta correcta tiene alto valor (código, matemáticas).',
    difficulty: 3,
  },

  // --- vision-language-models ---
  {
    concept_id: 'vision-language-models',
    type: 'definition',
    question_text: '¿Cuáles son las dos arquitecturas principales para vision-language models y cómo procesan las imágenes?',
    expected_answer: 'Encoder-fusión: un vision encoder (típicamente ViT) procesa la imagen en patch embeddings, un adaptador/projector los mapea al espacio del LLM, y se concatenan con los text tokens (e.g., LLaVA usa linear projection, Flamingo usa perceiver resampler). Nativo multimodal: la imagen se tokeniza directamente en tokens discretos que el transformer procesa junto con texto (e.g., Gemini, Fuyu). La primera es más modular; la segunda permite generación de imágenes también.',
    difficulty: 2,
  },
  {
    concept_id: 'vision-language-models',
    type: 'fact',
    question_text: '¿Qué es el problema de hallucination en VLMs y cómo difiere de la hallucination en LLMs de solo texto?',
    expected_answer: 'En VLMs, hallucination se refiere a describir objetos, atributos o relaciones que no existen en la imagen. A diferencia de LLMs texto-only (donde hallucinan hechos), los VLMs tienen un ground truth visual verificable. Tipos comunes: objetos inventados, conteos incorrectos, relaciones espaciales erróneas, y atributos falsos. Es particularmente grave porque da falsa confianza al usuario. Se evalúa con benchmarks como POPE y CHAIR.',
    difficulty: 2,
  },

  // --- dspy-programming ---
  {
    concept_id: 'dspy-programming',
    type: 'definition',
    question_text: '¿Qué es DSPy y cuáles son sus tres abstracciones fundamentales?',
    expected_answer: 'DSPy (Khattab et al.) es un framework que reemplaza prompt engineering manual con optimización programática. Tres abstracciones: (1) Signatures: declaran input/output fields de un paso LLM (e.g., "question -> answer"); (2) Modules: componentes reutilizables que implementan un patrón (ChainOfThought, ReAct, ProgramOfThought); (3) Optimizers (compiladores): ajustan automáticamente los prompts/few-shot examples para maximizar una métrica sobre un dataset de desarrollo. El pipeline se escribe como código Python normal.',
    difficulty: 1,
  },
  {
    concept_id: 'dspy-programming',
    type: 'property',
    question_text: '¿Cómo funciona el optimizer BootstrapFewShot de DSPy y por qué es preferible a escribir few-shot examples manualmente?',
    expected_answer: 'BootstrapFewShot ejecuta el pipeline sobre ejemplos del dataset de desarrollo, recolecta las trazas intermedias (inputs/outputs de cada módulo) que pasan la métrica de validación, y las usa como few-shot examples en el prompt final. Es preferible al manual porque: (1) selecciona ejemplos que realmente mejoran la métrica, no los que "parecen buenos"; (2) genera examples para pasos intermedios que un humano no podría escribir fácilmente; (3) se re-optimiza automáticamente al cambiar el modelo o la tarea. El resultado es un prompt optimizado, no pesos entrenados.',
    difficulty: 3,
  },
];

// ============================================================================
// SEED SCRIPT
// ============================================================================

async function seedQuestionBank() {
  console.log(`\n=== Seeding Question Bank ===\n`);
  console.log(`Total questions to insert: ${questions.length}`);

  // Verify concepts exist
  const conceptIds = [...new Set(questions.map(q => q.concept_id))];
  const { data: existingConcepts, error: conceptError } = await supabase
    .from('concepts')
    .select('id')
    .in('id', conceptIds);

  if (conceptError) {
    console.error('Error checking concepts:', conceptError);
    process.exit(1);
  }

  const existingIds = new Set((existingConcepts || []).map(c => c.id));
  const missingConcepts = conceptIds.filter(id => !existingIds.has(id));

  if (missingConcepts.length > 0) {
    console.error(`Missing concepts in database: ${missingConcepts.join(', ')}`);
    console.error('Run the seed-database.ts script first to populate concepts.');
    process.exit(1);
  }

  console.log(`All ${conceptIds.length} concepts verified.`);

  // Check for related_concept_id validity
  const relatedIds = questions
    .filter(q => q.related_concept_id)
    .map(q => q.related_concept_id!);
  const missingRelated = relatedIds.filter(id => !existingIds.has(id));
  if (missingRelated.length > 0) {
    console.warn(`Warning: related_concept_ids not in DB: ${missingRelated.join(', ')}`);
  }

  // Clear existing questions
  const { error: deleteError } = await supabase
    .from('question_bank')
    .delete()
    .neq('id', '00000000-0000-0000-0000-000000000000'); // delete all

  if (deleteError) {
    console.error('Error clearing question_bank:', deleteError);
    process.exit(1);
  }
  console.log('Cleared existing questions.');

  // Insert in batches of 50
  const batchSize = 50;
  let inserted = 0;

  for (let i = 0; i < questions.length; i += batchSize) {
    const batch = questions.slice(i, i + batchSize);
    const rows = batch.map(q => ({
      concept_id: q.concept_id,
      type: q.type,
      question_text: q.question_text,
      expected_answer: q.expected_answer,
      difficulty: q.difficulty,
      related_concept_id: q.related_concept_id || null,
      is_active: true,
    }));

    const { error } = await supabase.from('question_bank').insert(rows);

    if (error) {
      console.error(`Error inserting batch starting at ${i}:`, error);
      process.exit(1);
    }

    inserted += batch.length;
    console.log(`  Inserted ${inserted}/${questions.length}`);
  }

  // Print summary by concept
  const byConcept: Record<string, number> = {};
  for (const q of questions) {
    byConcept[q.concept_id] = (byConcept[q.concept_id] || 0) + 1;
  }

  console.log(`\n=== Summary ===`);
  console.log(`Total questions: ${questions.length}`);
  console.log(`Concepts covered: ${Object.keys(byConcept).length}`);
  console.log(`\nQuestions per concept:`);
  for (const [concept, count] of Object.entries(byConcept).sort()) {
    console.log(`  ${concept}: ${count}`);
  }

  console.log('\n=== Done! ===\n');
}

seedQuestionBank().catch((err) => {
  console.error('Seed failed:', err);
  process.exit(1);
});
