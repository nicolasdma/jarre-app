/**
 * Jarre - Seed Data
 *
 * Complete study plan for AI/LLM Systems Architect path.
 * 9-phase curriculum: Distributed Systems → System Design & Integration
 * Run with: npx tsx scripts/seed-data.ts
 */

import type { ResourceType, StudyPhase } from '../src/types';

// ============================================================================
// TYPES FOR SEED DATA
// ============================================================================

interface SeedResource {
  id: string;
  title: string;
  type: ResourceType;
  url?: string;
  author?: string;
  phase: StudyPhase;
  description?: string;
  estimatedHours?: number;
  sortOrder: number; // Order within phase. Lower numbers appear first.
  concepts: string[]; // concept IDs this resource teaches
  prerequisites?: string[]; // concept IDs needed before
}

interface SeedConcept {
  id: string;
  name: string;
  canonicalDefinition: string;
  phase: StudyPhase;
  prerequisites?: string[]; // other concept IDs
}

// ============================================================================
// CONCEPTS
// ============================================================================

export const concepts: SeedConcept[] = [
  // ---------------------------------------------------------------------------
  // PHASE 1: Distributed Systems Fundamentals (17 concepts)
  // ---------------------------------------------------------------------------
  {
    id: 'reliability',
    name: 'Reliability',
    canonicalDefinition: 'The system continues to work correctly even when things go wrong (faults). A reliable system is fault-tolerant, not fault-free.',
    phase: 1,
  },
  {
    id: 'scalability',
    name: 'Scalability',
    canonicalDefinition: 'The ability of a system to handle increased load by adding resources. Measured in terms of load parameters (requests/sec, data volume, etc).',
    phase: 1,
  },
  {
    id: 'maintainability',
    name: 'Maintainability',
    canonicalDefinition: 'How easy it is for engineers to work on the system over time: operability, simplicity, and evolvability.',
    phase: 1,
  },
  {
    id: 'data-models',
    name: 'Data Models',
    canonicalDefinition: 'The way data is structured and related: relational, document, graph, etc. Each model makes certain operations easier and others harder.',
    phase: 1,
  },
  {
    id: 'encoding-evolution',
    name: 'Encoding & Schema Evolution',
    canonicalDefinition: 'How data is encoded for storage/transmission (JSON, Avro, Protobuf, Thrift) and how schemas evolve over time. Forward and backward compatibility enable independent service deployment. Key for microservices and ML pipelines.',
    phase: 1,
    prerequisites: ['data-models'],
  },
  {
    id: 'storage-engines',
    name: 'Storage Engines',
    canonicalDefinition: 'How databases store and retrieve data on disk. Key types: log-structured (LSM trees) vs page-oriented (B-trees). Trade-offs between write and read performance.',
    phase: 1,
    prerequisites: ['data-models'],
  },
  {
    id: 'replication',
    name: 'Replication',
    canonicalDefinition: 'Keeping copies of the same data on multiple machines. Purposes: high availability, fault tolerance, latency reduction. Key models: leader-based, multi-leader, leaderless.',
    phase: 1,
    prerequisites: ['reliability'],
  },
  {
    id: 'transactions',
    name: 'Transactions',
    canonicalDefinition: 'A mechanism for grouping multiple reads and writes into a logical unit. ACID guarantees: Atomicity, Consistency, Isolation, Durability. Isolation levels (read committed, snapshot isolation, serializable) trade off performance for correctness. Implementation: 2PL, SSI.',
    phase: 1,
    prerequisites: ['replication', 'storage-engines'],
  },
  {
    id: 'partitioning',
    name: 'Partitioning (Sharding)',
    canonicalDefinition: 'Splitting a large dataset across multiple machines. Each partition is a mini-database. Strategies: by key range, by hash. Challenge: rebalancing, hot spots.',
    phase: 1,
    prerequisites: ['scalability', 'replication'],
  },
  {
    id: 'distributed-failures',
    name: 'Distributed System Failures',
    canonicalDefinition: 'What can go wrong in distributed systems: network partitions, node failures, clock skew, Byzantine faults. Partial failures are the norm, not the exception.',
    phase: 1,
    prerequisites: ['replication', 'partitioning'],
  },
  {
    id: 'consistency-models',
    name: 'Consistency Models',
    canonicalDefinition: 'Guarantees about what values readers will see. Spectrum from strong (linearizability) to weak (eventual). CAP theorem: can\'t have consistency, availability, and partition tolerance simultaneously.',
    phase: 1,
    prerequisites: ['distributed-failures'],
  },
  {
    id: 'consensus',
    name: 'Consensus',
    canonicalDefinition: 'Getting multiple nodes to agree on a value. Fundamental algorithms: Paxos, Raft, Zab. Used for leader election, atomic commit, total order broadcast.',
    phase: 1,
    prerequisites: ['consistency-models'],
  },
  {
    id: 'stream-processing',
    name: 'Stream Processing',
    canonicalDefinition: 'Processing data continuously as it arrives, rather than in batches. Key concepts: event time vs processing time, windowing, exactly-once semantics.',
    phase: 1,
    prerequisites: ['partitioning'],
  },
  {
    id: 'slos-slis',
    name: 'SLOs and SLIs',
    canonicalDefinition: 'Service Level Objectives (SLOs) are target values for service reliability. Service Level Indicators (SLIs) are the metrics that measure it. Error budgets allow controlled risk-taking.',
    phase: 1,
  },
  {
    id: 'monitoring',
    name: 'Monitoring Distributed Systems',
    canonicalDefinition: 'Collecting and analyzing metrics, logs, and traces to understand system health. The four golden signals: latency, traffic, errors, saturation.',
    phase: 1,
    prerequisites: ['slos-slis'],
  },
  {
    id: 'embracing-risk',
    name: 'Embracing Risk',
    canonicalDefinition: '100% reliability is impossible and prohibitively expensive. Instead, define acceptable failure rates (error budgets) and invest accordingly.',
    phase: 1,
    prerequisites: ['slos-slis'],
  },
  {
    id: 'tail-latency',
    name: 'Tail Latency',
    canonicalDefinition: 'The slowest requests (p95-p99) dominate user experience and SLOs. In distributed and LLM systems, tail latency is affected by batching, stragglers, cache misses, and long generations. Techniques: hedged requests, tied requests, canary requests.',
    phase: 1,
    prerequisites: ['slos-slis', 'monitoring'],
  },

  // ---------------------------------------------------------------------------
  // PHASE 2: ML Infrastructure Bridge (8 concepts) — NEW
  // ---------------------------------------------------------------------------
  {
    id: 'gpu-compute-fundamentals',
    name: 'GPU Compute Fundamentals',
    canonicalDefinition: 'GPU architecture for ML: CUDA cores, memory hierarchy (HBM, SRAM), parallelism model. Why GPUs are essential for matrix operations in deep learning. Understanding FLOPS, memory bandwidth, and compute-bound vs memory-bound operations.',
    phase: 2,
    prerequisites: ['scalability'],
  },
  {
    id: 'data-parallelism',
    name: 'Data Parallelism',
    canonicalDefinition: 'Distributing training data across multiple GPUs, each with a copy of the model. Gradient synchronization via AllReduce. Trade-offs: communication overhead vs linear speedup. Foundation for distributed training.',
    phase: 2,
    prerequisites: ['gpu-compute-fundamentals', 'partitioning'],
  },
  {
    id: 'distributed-training',
    name: 'Distributed Training',
    canonicalDefinition: 'Training large models across multiple machines. Strategies: data parallelism, model parallelism (tensor, pipeline), expert parallelism. Tools: DeepSpeed, FSDP, Megatron-LM. Key challenge: communication overhead and fault tolerance.',
    phase: 2,
    prerequisites: ['data-parallelism'],
  },
  {
    id: 'ml-data-pipelines',
    name: 'ML Data Pipelines',
    canonicalDefinition: 'Infrastructure for collecting, cleaning, transforming, and serving data for ML training and inference. ETL/ELT patterns adapted for ML: data versioning, lineage tracking, quality validation. Tools: Apache Beam, Spark, dbt.',
    phase: 2,
    prerequisites: ['stream-processing', 'encoding-evolution'],
  },
  {
    id: 'feature-stores',
    name: 'Feature Stores',
    canonicalDefinition: 'Centralized system for managing, storing, and serving ML features. Ensures consistency between training and serving (train-serve skew prevention). Online store (low-latency serving) vs offline store (batch training). Tools: Feast, Tecton.',
    phase: 2,
    prerequisites: ['ml-data-pipelines', 'storage-engines'],
  },
  {
    id: 'model-serving-infra',
    name: 'Model Serving Infrastructure',
    canonicalDefinition: 'Systems for deploying and serving ML models in production. Considerations: latency requirements, throughput, model versioning, A/B testing, canary deployments, autoscaling. Tools: TorchServe, Triton, TFServing, BentoML.',
    phase: 2,
    prerequisites: ['scalability', 'reliability', 'slos-slis'],
  },
  {
    id: 'experiment-tracking',
    name: 'Experiment Tracking & MLOps',
    canonicalDefinition: 'Systematic tracking of ML experiments: hyperparameters, metrics, artifacts, model versions. Enables reproducibility and comparison. MLOps extends DevOps for ML: CI/CD for models, monitoring for drift. Tools: MLflow, W&B, DVC.',
    phase: 2,
    prerequisites: ['model-serving-infra', 'monitoring'],
  },
  {
    id: 'gpu-memory-management',
    name: 'GPU Memory Management',
    canonicalDefinition: 'Techniques for efficient GPU memory usage during training and inference. Gradient checkpointing (recompute vs store trade-off), mixed precision training (FP16/BF16), activation offloading, memory-efficient optimizers (8-bit Adam). Critical for training/serving large models.',
    phase: 2,
    prerequisites: ['gpu-compute-fundamentals'],
  },

  // ---------------------------------------------------------------------------
  // PHASE 3: Transformer Foundations (10 concepts)
  // ---------------------------------------------------------------------------
  {
    id: 'attention-mechanism',
    name: 'Attention Mechanism',
    canonicalDefinition: 'A way for models to focus on relevant parts of the input when producing output. Self-attention allows each position to attend to all positions in the previous layer.',
    phase: 3,
  },
  {
    id: 'transformer-architecture',
    name: 'Transformer Architecture',
    canonicalDefinition: 'Neural network architecture based entirely on attention mechanisms, no recurrence. Key components: multi-head attention, positional encoding, feed-forward layers.',
    phase: 3,
    prerequisites: ['attention-mechanism'],
  },
  {
    id: 'query-key-value',
    name: 'Query-Key-Value (QKV)',
    canonicalDefinition: 'The three projections in attention. Query: what am I looking for? Key: what do I contain? Value: what do I provide? Attention = softmax(QK^T/√d)V',
    phase: 3,
    prerequisites: ['attention-mechanism'],
  },
  {
    id: 'positional-encoding',
    name: 'Positional Encoding',
    canonicalDefinition: 'Since attention has no inherent notion of position, we add positional information to embeddings. Original: sinusoidal functions. Modern: learned or rotary (RoPE).',
    phase: 3,
    prerequisites: ['transformer-architecture'],
  },
  {
    id: 'scaling-laws',
    name: 'Scaling Laws',
    canonicalDefinition: 'Empirical relationships between model size, data, compute, and performance. Loss scales as power law with each factor. Guides efficient allocation of training budget.',
    phase: 3,
    prerequisites: ['transformer-architecture'],
  },
  {
    id: 'compute-optimal-training',
    name: 'Compute-Optimal Training',
    canonicalDefinition: 'Chinchilla finding: models are often undertrained. Optimal: scale data and parameters equally. 70B model trained on 1.4T tokens beats 280B on 300B tokens.',
    phase: 3,
    prerequisites: ['scaling-laws'],
  },
  {
    id: 'foundation-models',
    name: 'Foundation Models',
    canonicalDefinition: 'Large models trained on broad data that can be adapted to many downstream tasks. Emergent abilities appear at scale. Risks: homogenization, bias amplification.',
    phase: 3,
    prerequisites: ['scaling-laws'],
  },
  {
    id: 'fine-tuning-efficiency',
    name: 'Parameter-Efficient Fine-Tuning',
    canonicalDefinition: 'Adapting pre-trained models without updating all parameters. Key techniques: LoRA (low-rank adaptation), prefix tuning, adapters. Reduces compute/memory by 10-1000x while maintaining quality.',
    phase: 3,
    prerequisites: ['foundation-models', 'transformer-architecture'],
  },
  {
    id: 'dpo',
    name: 'Direct Preference Optimization (DPO)',
    canonicalDefinition: 'Simpler alternative to RLHF for alignment. Directly optimizes policy from preferences without training a reward model or using RL. Same objective as RLHF but with a supervised learning loss. Stable, efficient, widely adopted.',
    phase: 3,
    prerequisites: ['foundation-models'],
  },
  {
    id: 'context-extension',
    name: 'Context Extension Techniques',
    canonicalDefinition: 'Methods to extend the effective context window of transformers beyond their training length. RoPE scaling, YaRN (Yet Another RoPE extensioN), ALiBi (Attention with Linear Biases), sparse attention patterns. Trade-offs between context length, quality, and compute cost.',
    phase: 3,
    prerequisites: ['positional-encoding', 'attention-mechanism'],
  },

  // ---------------------------------------------------------------------------
  // PHASE 4: Agents & Reasoning (12 concepts)
  // ---------------------------------------------------------------------------
  {
    id: 'chain-of-thought',
    name: 'Chain-of-Thought',
    canonicalDefinition: 'Prompting technique where the model shows intermediate reasoning steps. Improves performance on complex tasks. Can be zero-shot ("think step by step") or few-shot.',
    phase: 4,
    prerequisites: ['foundation-models'],
  },
  {
    id: 'react-pattern',
    name: 'ReAct Pattern',
    canonicalDefinition: 'Interleaving reasoning (chain-of-thought) with acting (tool use). Model thinks step by step, takes action, observes result, repeats. Enables grounded, traceable reasoning.',
    phase: 4,
    prerequisites: ['foundation-models'],
  },
  {
    id: 'tree-of-thoughts',
    name: 'Tree of Thoughts',
    canonicalDefinition: 'Extension of chain-of-thought where model explores multiple reasoning paths, evaluates them, and can backtrack. Enables deliberate problem-solving.',
    phase: 4,
    prerequisites: ['chain-of-thought'],
  },
  {
    id: 'reflexion',
    name: 'Reflexion',
    canonicalDefinition: 'Agent pattern where model reflects on failures and stores verbal feedback in memory. Enables learning from mistakes without weight updates.',
    phase: 4,
    prerequisites: ['react-pattern'],
  },
  {
    id: 'tool-use',
    name: 'Tool Use',
    canonicalDefinition: 'Teaching LLMs to call external APIs, calculators, search engines. Extends capabilities beyond training data. Key challenge: knowing when to use which tool.',
    phase: 4,
    prerequisites: ['foundation-models'],
  },
  {
    id: 'plan-and-execute',
    name: 'Plan-and-Execute Agents',
    canonicalDefinition: 'Two-phase agent pattern: first create a plan (list of steps), then execute each step. Separates planning from execution, enables re-planning on failure.',
    phase: 4,
    prerequisites: ['react-pattern', 'tool-use'],
  },
  {
    id: 'prompt-engineering',
    name: 'Prompt Engineering',
    canonicalDefinition: 'The discipline of designing effective prompts for LLMs. Includes techniques like few-shot learning, role prompting, format specification, and constraint setting. Foundation for all LLM applications.',
    phase: 4,
    prerequisites: ['foundation-models'],
  },
  {
    id: 'structured-output',
    name: 'Structured Output',
    canonicalDefinition: 'Constraining LLM outputs to valid formats (JSON, XML, function calls). Techniques: JSON mode, function calling schemas, grammar-constrained decoding. Essential for reliable tool use and data extraction.',
    phase: 4,
    prerequisites: ['foundation-models', 'prompt-engineering'],
  },
  {
    id: 'test-time-compute',
    name: 'Test-Time Compute Scaling',
    canonicalDefinition: 'Using more compute at inference for better reasoning. Models like o1 and DeepSeek-R1 trade latency for accuracy by "thinking longer". Enables solving problems that stumped previous models. New scaling dimension beyond model size and training data.',
    phase: 4,
    prerequisites: ['chain-of-thought', 'foundation-models'],
  },
  {
    id: 'reasoning-models',
    name: 'Reasoning Models (o1, R1)',
    canonicalDefinition: 'LLMs trained to produce extended reasoning chains before answering. Use reinforcement learning to learn when to think more vs less. Excel at math, code, and complex reasoning. Key insight: reasoning capability emerges from RL without labeled reasoning data.',
    phase: 4,
    prerequisites: ['test-time-compute', 'chain-of-thought'],
  },
  {
    id: 'multi-agent-systems',
    name: 'Multi-Agent Systems',
    canonicalDefinition: 'Architectures where multiple LLM agents collaborate on tasks. Patterns: supervisor-worker (one agent delegates), debate (agents argue to consensus), ensemble (parallel execution + aggregation). Challenges: coordination overhead, error propagation, cost multiplication.',
    phase: 4,
    prerequisites: ['plan-and-execute', 'tool-use'],
  },
  {
    id: 'agent-reliability',
    name: 'Agent Reliability',
    canonicalDefinition: 'Engineering reliable agent systems: retry/fallback strategies, output validation loops, human-in-the-loop (HITL) checkpoints, sandboxing for safety. Without explicit reliability engineering, agents fail silently and unpredictably.',
    phase: 4,
    prerequisites: ['react-pattern', 'structured-output'],
  },

  // ---------------------------------------------------------------------------
  // PHASE 5: RAG, Memory & Context (15 concepts)
  // ---------------------------------------------------------------------------
  {
    id: 'embeddings',
    name: 'Embeddings',
    canonicalDefinition: 'Dense vector representations of text. Similar meanings → similar vectors. Enable semantic search. Models: sentence-transformers, OpenAI embeddings, etc.',
    phase: 5,
    prerequisites: ['transformer-architecture'],
  },
  {
    id: 'vector-search',
    name: 'Vector Search',
    canonicalDefinition: 'Finding similar items by comparing embedding vectors. Algorithms: exact (brute force), approximate (HNSW, IVF). Trade-off: speed vs recall.',
    phase: 5,
    prerequisites: ['embeddings'],
  },
  {
    id: 'rag-basics',
    name: 'Retrieval-Augmented Generation',
    canonicalDefinition: 'Augmenting LLM generation with retrieved documents. Reduces hallucination, enables knowledge updates without retraining. Components: retriever, reader, generator.',
    phase: 5,
    prerequisites: ['foundation-models'],
  },
  {
    id: 'chunking-strategies',
    name: 'Chunking Strategies',
    canonicalDefinition: 'How to split documents for embedding. Options: fixed size, sentence-based, semantic, recursive. Trade-off: too small loses context, too large dilutes relevance.',
    phase: 5,
    prerequisites: ['rag-basics', 'embeddings'],
  },
  {
    id: 'hybrid-search',
    name: 'Hybrid Search',
    canonicalDefinition: 'Combining dense (embedding) and sparse (keyword/BM25) retrieval. Often outperforms either alone. Requires score normalization and fusion strategies.',
    phase: 5,
    prerequisites: ['vector-search'],
  },
  {
    id: 'lost-in-middle',
    name: 'Lost in the Middle',
    canonicalDefinition: 'LLMs pay less attention to information in the middle of long contexts. Performance is U-shaped: best for info at start or end. Implications for RAG ordering.',
    phase: 5,
    prerequisites: ['rag-basics'],
  },
  {
    id: 'context-window-limits',
    name: 'Context Window Limits',
    canonicalDefinition: 'Maximum tokens a model can process. Attention is O(n²) in context length. Longer context ≠ better understanding. Cost scales with context size.',
    phase: 5,
    prerequisites: ['attention-mechanism'],
  },
  {
    id: 'external-memory',
    name: 'External Memory for LLMs',
    canonicalDefinition: 'Storing information outside the model (vector DBs, key-value stores) and retrieving as needed. Enables unbounded knowledge without context limits.',
    phase: 5,
    prerequisites: ['rag-basics', 'vector-search'],
  },
  {
    id: 'memory-management',
    name: 'LLM Memory Management',
    canonicalDefinition: 'Policies for storing, retrieving, updating, and forgetting memories. Includes recency bias, importance scoring, summarization, conflict resolution, and drift control. Without explicit management, "store everything forever" breaks systems quietly.',
    phase: 5,
    prerequisites: ['external-memory'],
  },
  {
    id: 'reranking',
    name: 'Reranking',
    canonicalDefinition: 'Second-stage retrieval that reorders initial results for higher relevance. Cross-encoder rerankers are more accurate but slower than bi-encoders. ColBERT enables efficient token-level interaction. Cohere Rerank, BGE-Reranker as production options.',
    phase: 5,
    prerequisites: ['rag-basics', 'vector-search'],
  },
  {
    id: 'query-decomposition',
    name: 'Query Decomposition',
    canonicalDefinition: 'Breaking complex queries into simpler sub-queries for better retrieval. Techniques: HyDE (hypothetical document embeddings), step-back prompting (abstract before retrieve), multi-hop (chain of retrievals). Improves recall on complex questions.',
    phase: 5,
    prerequisites: ['rag-basics', 'chain-of-thought'],
  },
  {
    id: 'agentic-rag',
    name: 'Agentic RAG',
    canonicalDefinition: 'RAG systems where an agent dynamically decides retrieval strategy: when to retrieve, which sources to query, whether to refine queries, when to stop. Goes beyond static retrieve-then-generate pipelines. Combines RAG with agent reasoning loops.',
    phase: 5,
    prerequisites: ['rag-basics', 'react-pattern', 'tool-use'],
  },
  {
    id: 'graph-rag',
    name: 'Graph RAG',
    canonicalDefinition: 'Augmenting RAG with knowledge graphs. Entities and relationships provide structured context that vector search misses. Microsoft GraphRAG: community detection + summarization for global queries. Complements vector search for multi-hop reasoning.',
    phase: 5,
    prerequisites: ['rag-basics', 'data-models'],
  },
  {
    id: 'rag-evaluation',
    name: 'RAG Evaluation',
    canonicalDefinition: 'Metrics and frameworks for evaluating RAG systems. RAGAS framework: faithfulness (grounded in context), answer relevancy, context precision/recall. Component-level evaluation (retriever vs generator) vs end-to-end. Essential for systematic RAG improvement.',
    phase: 5,
    prerequisites: ['rag-basics'],
  },
  {
    id: 'rag-vs-long-context',
    name: 'RAG vs Long Context',
    canonicalDefinition: 'When to use RAG vs stuffing everything into a long context window. Needle-in-haystack tests show long context degrades with more content. RAG: better for large corpora, cost-effective, updatable. Long context: simpler architecture, better for small documents. Hybrid approaches emerging.',
    phase: 5,
    prerequisites: ['rag-basics', 'context-extension', 'context-window-limits'],
  },

  // ---------------------------------------------------------------------------
  // PHASE 6: Multimodal & Emerging (7 concepts)
  // ---------------------------------------------------------------------------
  {
    id: 'multimodal-embeddings',
    name: 'Multimodal Embeddings (CLIP)',
    canonicalDefinition: 'Joint embedding space for images and text via contrastive learning. CLIP learns to align image and text representations. Enables zero-shot image classification, image-text retrieval, and serves as vision backbone for VLMs.',
    phase: 6,
    prerequisites: ['embeddings', 'transformer-architecture'],
  },
  {
    id: 'vision-language-models',
    name: 'Vision-Language Models (VLMs)',
    canonicalDefinition: 'LLMs extended with visual understanding. Architecture: vision encoder + projection layer + LLM. Can describe images, answer visual questions, reason about diagrams. Examples: GPT-4V, Claude Vision, LLaVA. Training: visual instruction tuning.',
    phase: 6,
    prerequisites: ['multimodal-embeddings', 'foundation-models'],
  },
  {
    id: 'multimodal-rag',
    name: 'Multimodal RAG',
    canonicalDefinition: 'RAG systems that handle multiple modalities: text, images, tables, diagrams. Cross-modal retrieval finds relevant images for text queries and vice versa. ColPali: vision-language model for document retrieval without OCR. Challenges: embedding alignment across modalities.',
    phase: 6,
    prerequisites: ['rag-basics', 'multimodal-embeddings'],
  },
  {
    id: 'image-generation-arch',
    name: 'Image Generation Architectures',
    canonicalDefinition: 'Architectures for generating images from text. Diffusion models: iterative denoising from noise to image. Latent diffusion (Stable Diffusion): operates in compressed latent space for efficiency. Key concepts: noise schedules, classifier-free guidance, ControlNet for fine control.',
    phase: 6,
    prerequisites: ['transformer-architecture'],
  },
  {
    id: 'speech-models',
    name: 'Speech Models',
    canonicalDefinition: 'Models for speech recognition (ASR) and synthesis (TTS). Whisper: robust ASR trained on 680K hours of multilingual data. Modern TTS: neural vocoders, voice cloning. Enables voice interfaces for LLM applications.',
    phase: 6,
    prerequisites: ['transformer-architecture'],
  },
  {
    id: 'state-space-models',
    name: 'State Space Models',
    canonicalDefinition: 'Alternative to Transformers with linear scaling in sequence length. Mamba: selective state spaces with input-dependent selection. RWKV: RNN-Transformer hybrid. Advantages: O(n) inference, constant memory. Trade-off: may underperform Transformers on tasks requiring global attention.',
    phase: 6,
    prerequisites: ['transformer-architecture', 'context-window-limits'],
  },
  {
    id: 'knowledge-distillation',
    name: 'Knowledge Distillation',
    canonicalDefinition: 'Training a smaller "student" model to mimic a larger "teacher" model. Types: output distillation (match logits), feature distillation (match intermediate representations), data distillation (teacher generates training data). Enables deployment of capable models on constrained hardware.',
    phase: 6,
    prerequisites: ['foundation-models', 'fine-tuning-efficiency'],
  },

  // ---------------------------------------------------------------------------
  // PHASE 7: Safety, Guardrails & Evaluation (11 concepts)
  // ---------------------------------------------------------------------------
  {
    id: 'constitutional-ai',
    name: 'Constitutional AI',
    canonicalDefinition: 'Training AI to follow principles (a "constitution") through self-critique and revision. RLHF alternative that\'s more scalable and transparent.',
    phase: 7,
    prerequisites: ['foundation-models'],
  },
  {
    id: 'self-consistency',
    name: 'Self-Consistency',
    canonicalDefinition: 'Sampling multiple reasoning paths and taking majority vote. Improves accuracy on reasoning tasks. Trade-off: cost (multiple generations) vs reliability.',
    phase: 7,
    prerequisites: ['chain-of-thought'],
  },
  {
    id: 'offline-evaluation',
    name: 'Offline LLM Evaluation',
    canonicalDefinition: 'Benchmark-based and regression evaluation using fixed datasets (MMLU, HumanEval, custom test suites). Useful for iteration and comparison, but limited for predicting real-world behavior. Includes LLM-as-judge patterns.',
    phase: 7,
    prerequisites: ['foundation-models'],
  },
  {
    id: 'online-evaluation',
    name: 'Online LLM Evaluation',
    canonicalDefinition: 'Evaluating LLMs in production using A/B tests, shadow traffic, canary deployments, and human feedback. Captures real user behavior that offline metrics miss. Requires careful metric selection and statistical rigor.',
    phase: 7,
    prerequisites: ['slos-slis', 'foundation-models'],
  },
  {
    id: 'red-teaming',
    name: 'Red Teaming LLMs',
    canonicalDefinition: 'Adversarial testing to find failure modes: jailbreaks, harmful outputs, bias. Manual and automated approaches. Essential before deployment.',
    phase: 7,
    prerequisites: ['offline-evaluation'],
  },
  {
    id: 'output-validation',
    name: 'Output Validation',
    canonicalDefinition: 'Checking LLM outputs before using them: schema validation (JSON), fact-checking, safety filtering. Defense in depth against unreliable generations.',
    phase: 7,
    prerequisites: ['foundation-models'],
  },
  {
    id: 'prompt-injection',
    name: 'Prompt Injection',
    canonicalDefinition: 'Attack where malicious input overrides system instructions. Direct injection: user prompt contains attack. Indirect injection: external data (retrieved docs, tool outputs) contains attack. Defenses: input sanitization, output filtering, privilege separation, instruction hierarchy.',
    phase: 7,
    prerequisites: ['foundation-models', 'rag-basics'],
  },
  {
    id: 'llm-security-owasp',
    name: 'LLM Security (OWASP Top 10)',
    canonicalDefinition: 'Systematic framework for LLM vulnerabilities: prompt injection, insecure output handling, training data poisoning, model DoS, supply chain risks, sensitive information disclosure, excessive agency. Essential checklist before deploying any LLM application.',
    phase: 7,
    prerequisites: ['prompt-injection', 'output-validation'],
  },
  {
    id: 'guardrails-libraries',
    name: 'Guardrails Libraries',
    canonicalDefinition: 'Frameworks for adding input/output guardrails to LLM applications. NeMo Guardrails: dialog rails with Colang. Instructor: structured output with Pydantic validation. Guardrails AI: RAIL specs for output validation. Comparison of approaches and when to use each.',
    phase: 7,
    prerequisites: ['output-validation', 'structured-output'],
  },
  {
    id: 'content-moderation',
    name: 'Content Moderation',
    canonicalDefinition: 'Multi-layer pipelines for detecting and filtering harmful, toxic, or policy-violating content in LLM inputs and outputs. Layers: keyword filters, classifier models, LLM-as-judge. Challenges: false positives, adversarial attacks, context-dependent policies.',
    phase: 7,
    prerequisites: ['output-validation', 'constitutional-ai'],
  },
  {
    id: 'eval-harnesses',
    name: 'Evaluation Harnesses',
    canonicalDefinition: 'Tools and frameworks for systematic LLM evaluation. promptfoo: prompt testing and regression. lm-eval-harness: standardized benchmarks. LLM-as-judge (MT-Bench): using strong LLMs to evaluate weaker ones. Enables CI/CD for prompt engineering.',
    phase: 7,
    prerequisites: ['offline-evaluation', 'online-evaluation'],
  },

  // ---------------------------------------------------------------------------
  // PHASE 8: Inference & Economics (13 concepts)
  // ---------------------------------------------------------------------------
  {
    id: 'kv-cache',
    name: 'KV Cache',
    canonicalDefinition: 'Caching key-value pairs from previous tokens during generation. Avoids recomputation. Memory scales with batch size × sequence length × layers.',
    phase: 8,
    prerequisites: ['query-key-value', 'transformer-architecture'],
  },
  {
    id: 'batching-inference',
    name: 'Batching for Inference',
    canonicalDefinition: 'Processing multiple requests together to utilize GPU parallelism. Challenge: different sequence lengths. Solutions: continuous batching, iteration-level scheduling.',
    phase: 8,
    prerequisites: ['kv-cache'],
  },
  {
    id: 'paged-attention',
    name: 'PagedAttention',
    canonicalDefinition: 'Memory management technique from vLLM. Stores KV cache in non-contiguous blocks like virtual memory. Enables efficient memory sharing and larger batches.',
    phase: 8,
    prerequisites: ['kv-cache', 'batching-inference'],
  },
  {
    id: 'speculative-decoding',
    name: 'Speculative Decoding',
    canonicalDefinition: 'Using a small draft model to generate candidate tokens, then verifying with the large model in parallel. Speeds up inference without quality loss.',
    phase: 8,
    prerequisites: ['kv-cache'],
  },
  {
    id: 'quantization',
    name: 'Quantization',
    canonicalDefinition: 'Reducing precision of model weights (FP16 → INT8 → INT4). Reduces memory and speeds up inference. Trade-off: potential quality degradation.',
    phase: 8,
    prerequisites: ['transformer-architecture'],
  },
  {
    id: 'token-economics',
    name: 'Token Economics',
    canonicalDefinition: 'Understanding LLM costs: input tokens vs output tokens, price per million, context length impact. Output tokens are 3-5x more expensive than input.',
    phase: 8,
    prerequisites: ['kv-cache'],
  },
  {
    id: 'model-routing',
    name: 'Model Routing',
    canonicalDefinition: 'Directing requests to different models based on complexity, cost, latency requirements. Simple queries → small/local models. Complex → large models.',
    phase: 8,
    prerequisites: ['token-economics'],
  },
  {
    id: 'semantic-caching',
    name: 'Semantic Caching',
    canonicalDefinition: 'Caching LLM responses based on semantic similarity of queries, not exact match. Uses embeddings to find similar past queries. Can reduce API costs 60-70% but requires tuning similarity thresholds to avoid returning wrong answers.',
    phase: 8,
    prerequisites: ['embeddings', 'token-economics'],
  },
  {
    id: 'prompt-caching',
    name: 'Prompt Caching (Prefix Caching)',
    canonicalDefinition: 'API-level optimization that caches KV states for prompt prefixes. Reusing cached prefixes is 75-90% cheaper. Requires structuring prompts with static content first. Different from semantic caching: exact prefix match, not semantic similarity.',
    phase: 8,
    prerequisites: ['kv-cache', 'token-economics'],
  },
  {
    id: 'llm-observability',
    name: 'LLM Observability',
    canonicalDefinition: 'Monitoring, tracing, and debugging LLM systems in production. Goes beyond traditional observability: tracking prompts, completions, token usage, latency per component, cost attribution, and detecting silent failures like hallucinations.',
    phase: 8,
    prerequisites: ['monitoring', 'token-economics'],
  },
  {
    id: 'rate-limiting',
    name: 'Rate Limiting & Backpressure',
    canonicalDefinition: 'Controlling request flow to prevent system overload. Rate limiting: proactive ceiling on requests. Backpressure: reactive signal when downstream is overwhelmed. Essential for LLM APIs with strict rate limits and expensive retries.',
    phase: 8,
    prerequisites: ['slos-slis'],
  },
  {
    id: 'compound-ai-systems',
    name: 'Compound AI Systems',
    canonicalDefinition: 'AI systems composed of multiple interacting components: models, retrievers, tools, guardrails, routers. State-of-the-art results come from system design, not just better models. 60% of LLM apps use RAG, 30% use multi-step chains. Requires thinking beyond model deployment to system-level orchestration.',
    phase: 8,
    prerequisites: ['model-routing', 'rag-basics', 'output-validation'],
  },
  {
    id: 'mixture-of-experts',
    name: 'Mixture of Experts (MoE)',
    canonicalDefinition: 'Architecture where only a subset of model parameters are active per token. A router network selects which "expert" sub-networks to activate. Examples: GPT-4 (rumored), Mixtral 8x7B, DeepSeek-V3. Enables larger total capacity with constant inference cost. Trade-off: memory for all experts must be loaded.',
    phase: 8,
    prerequisites: ['foundation-models', 'model-routing'],
  },

  // ---------------------------------------------------------------------------
  // PHASE 9: System Design & Integration (8 concepts)
  // ---------------------------------------------------------------------------
  {
    id: 'langchain-architecture',
    name: 'LangChain Architecture',
    canonicalDefinition: 'Framework for building LLM applications. Components: chains, agents, memory, tools. Study to understand what problems it solves, then critique: abstraction overhead, debugging difficulty, performance costs.',
    phase: 9,
    prerequisites: ['react-pattern', 'rag-basics'],
  },
  {
    id: 'llamaindex-architecture',
    name: 'LlamaIndex Architecture',
    canonicalDefinition: 'Framework focused on data ingestion and retrieval for LLMs. Indexes, query engines, data connectors. Better for RAG-specific use cases. Study the design decisions, then build your own.',
    phase: 9,
    prerequisites: ['rag-basics', 'chunking-strategies'],
  },
  {
    id: 'framework-tradeoffs',
    name: 'Framework Trade-offs',
    canonicalDefinition: 'When to use frameworks vs build from scratch. Frameworks: fast prototyping, community support. Custom: control, performance, debuggability. Production systems often need custom implementations. The best engineers understand both.',
    phase: 9,
    prerequisites: ['langchain-architecture', 'llamaindex-architecture'],
  },
  {
    id: 'minimal-implementations',
    name: 'Minimal Implementations',
    canonicalDefinition: 'Building RAG, agents, and memory from scratch in ~200 lines. Proves you understand the core mechanics. Reveals what frameworks hide. Essential before using any framework in production.',
    phase: 9,
    prerequisites: ['rag-basics', 'react-pattern', 'external-memory'],
  },
  {
    id: 'dspy-programming',
    name: 'DSPy: Programming LLMs',
    canonicalDefinition: 'Framework for programming (not prompting) LLMs. Declarative modules that self-improve through optimization. Compiles high-level signatures into effective prompts. Alternative to manual prompt engineering. Outperforms hand-crafted prompts by 25-65%.',
    phase: 9,
    prerequisites: ['prompt-engineering', 'compound-ai-systems'],
  },
  {
    id: 'system-design-patterns',
    name: 'System Design Patterns',
    canonicalDefinition: 'Architectural patterns for production LLM systems. Gateway pattern: centralized API management. Orchestrator pattern: workflow coordination. RAG-agent hybrid: combining retrieval with reasoning. Each pattern has specific trade-offs in complexity, latency, and reliability.',
    phase: 9,
    prerequisites: ['compound-ai-systems', 'agent-reliability'],
  },
  {
    id: 'cost-quality-latency',
    name: 'Cost-Quality-Latency Triangle',
    canonicalDefinition: 'The fundamental trade-off triangle in LLM systems. You can optimize for any two but not all three. Higher quality requires more compute (cost + latency). Lower latency limits reasoning depth (quality). Lower cost means smaller models or caching (quality risk). Architects must explicitly choose their priority.',
    phase: 9,
    prerequisites: ['token-economics', 'model-routing', 'compound-ai-systems'],
  },
  {
    id: 'production-architectures',
    name: 'Production Architectures',
    canonicalDefinition: 'Case studies of real-world LLM system architectures. Cursor: code completion with speculative edits. Perplexity: search with RAG + citations. ChatGPT: conversation with tools + memory. Analysis of their architectural decisions and trade-offs.',
    phase: 9,
    prerequisites: ['system-design-patterns', 'llm-observability'],
  },
  {
    id: 'agent-protocol-design',
    name: 'Agent Protocol Design',
    canonicalDefinition: 'Designing communication protocols for AI agents: session management, command translation, streaming, and lifecycle control. Patterns: request-response, NDJSON streams, gateway abstraction. Trade-offs between simplicity and expressiveness.',
    phase: 9,
    prerequisites: ['system-design-patterns', 'react-pattern'],
  },
  {
    id: 'plugin-channel-architecture',
    name: 'Plugin & Channel Architecture',
    canonicalDefinition: 'Extensible plugin architectures for multi-channel agent systems. Standardized interfaces (docks) abstract platform differences. Concerns: message normalization, security policies, onboarding flows, runtime monitoring.',
    phase: 9,
    prerequisites: ['system-design-patterns', 'framework-tradeoffs'],
  },
  {
    id: 'agent-skill-orchestration',
    name: 'Agent Skill Orchestration',
    canonicalDefinition: 'How agents discover, authenticate, and invoke external tools at scale. Skill metadata, CLI tool integration, permission models, and execution sandboxing. The boundary between agent autonomy and safety controls.',
    phase: 9,
    prerequisites: ['tool-use', 'agent-reliability'],
  },
  {
    id: 'agent-memory-persistence',
    name: 'Agent Memory Persistence',
    canonicalDefinition: 'Practical implementations of persistent memory for stateful agents: file-backed short-term memory, vector search with LanceDB for long-term recall, autoCapture/autoRecall hooks. Trade-offs between retrieval accuracy, latency, and storage cost.',
    phase: 9,
    prerequisites: ['external-memory', 'vector-search'],
  },
  {
    id: 'agent-ui-generation',
    name: 'Agent UI Generation (A2UI)',
    canonicalDefinition: 'Declarative frameworks for agents to generate rich, updateable UIs. JSON-based component catalogs, security-first rendering (no executable code from agents), framework-agnostic portability. The separation of UI intent from implementation.',
    phase: 9,
    prerequisites: ['structured-output', 'production-architectures'],
  },
];

// ============================================================================
// RESOURCES
// ============================================================================

export const resources: SeedResource[] = [
  // ---------------------------------------------------------------------------
  // PHASE 1: BOOKS - Distributed Systems
  // ---------------------------------------------------------------------------
  {
    id: 'ddia-ch1',
    title: 'DDIA Chapter 1: Reliable, Scalable, and Maintainable Applications',
    type: 'book',
    author: 'Martin Kleppmann',
    phase: 1,
    description: 'Foundation chapter. Defines the three key properties of data systems and why they matter.',
    estimatedHours: 3,
    sortOrder: 10,
    concepts: ['reliability', 'scalability', 'maintainability'],
  },
  {
    id: 'ddia-ch2',
    title: 'DDIA Chapter 2: Data Models and Query Languages',
    type: 'book',
    author: 'Martin Kleppmann',
    phase: 1,
    description: 'Relational vs document vs graph models. How data model choice affects application code.',
    estimatedHours: 4,
    sortOrder: 20,
    concepts: ['data-models'],
  },
  {
    id: 'ddia-ch3',
    title: 'DDIA Chapter 3: Storage and Retrieval',
    type: 'book',
    author: 'Martin Kleppmann',
    phase: 1,
    description: 'How databases store data on disk. LSM trees vs B-trees. Column-oriented storage.',
    estimatedHours: 5,
    sortOrder: 30,
    concepts: ['storage-engines'],
    prerequisites: ['data-models'],
  },
  {
    id: 'ddia-ch4',
    title: 'DDIA Chapter 4: Encoding and Evolution',
    type: 'book',
    author: 'Martin Kleppmann',
    phase: 1,
    description: 'Data encoding formats (JSON, Avro, Protobuf, Thrift). Schema evolution and compatibility. Dataflow through databases, services, and message passing.',
    estimatedHours: 4,
    sortOrder: 35,
    concepts: ['encoding-evolution'],
    prerequisites: ['data-models'],
  },
  {
    id: 'ddia-ch5',
    title: 'DDIA Chapter 5: Replication',
    type: 'book',
    author: 'Martin Kleppmann',
    phase: 1,
    description: 'Leader-based, multi-leader, and leaderless replication. Handling failures and conflicts.',
    estimatedHours: 5,
    sortOrder: 40,
    concepts: ['replication'],
    prerequisites: ['reliability'],
  },
  {
    id: 'ddia-ch6',
    title: 'DDIA Chapter 6: Partitioning',
    type: 'book',
    author: 'Martin Kleppmann',
    phase: 1,
    description: 'Strategies for splitting data across nodes. Rebalancing. Secondary indexes.',
    estimatedHours: 4,
    sortOrder: 50,
    concepts: ['partitioning'],
    prerequisites: ['scalability', 'replication'],
  },
  {
    id: 'ddia-ch7',
    title: 'DDIA Chapter 7: Transactions',
    type: 'book',
    author: 'Martin Kleppmann',
    phase: 1,
    description: 'ACID guarantees, isolation levels (read committed, snapshot isolation, serializability). Implementations: 2PL, SSI. When transactions are needed and when they hurt performance.',
    estimatedHours: 5,
    sortOrder: 55,
    concepts: ['transactions'],
    prerequisites: ['replication', 'storage-engines'],
  },
  {
    id: 'ddia-ch8',
    title: 'DDIA Chapter 8: The Trouble with Distributed Systems',
    type: 'book',
    author: 'Martin Kleppmann',
    phase: 1,
    description: 'What can go wrong: network problems, clocks, process pauses. Why distributed systems are hard.',
    estimatedHours: 4,
    sortOrder: 60,
    concepts: ['distributed-failures'],
    prerequisites: ['replication', 'partitioning'],
  },
  {
    id: 'ddia-ch9',
    title: 'DDIA Chapter 9: Consistency and Consensus',
    type: 'book',
    author: 'Martin Kleppmann',
    phase: 1,
    description: 'Linearizability, ordering guarantees, distributed transactions, consensus algorithms.',
    estimatedHours: 6,
    sortOrder: 70,
    concepts: ['consistency-models', 'consensus'],
    prerequisites: ['distributed-failures'],
  },
  {
    id: 'ddia-ch11',
    title: 'DDIA Chapter 11: Stream Processing',
    type: 'book',
    author: 'Martin Kleppmann',
    phase: 1,
    description: 'Processing unbounded data. Event time vs processing time. Stream joins and fault tolerance.',
    estimatedHours: 5,
    sortOrder: 80,
    concepts: ['stream-processing'],
    prerequisites: ['partitioning'],
  },
  {
    id: 'tanenbaum-ch1',
    title: 'Distributed Systems Ch 1: Introduction',
    type: 'book',
    author: 'Tanenbaum & Van Steen',
    phase: 1,
    description: 'Goals and types of distributed systems. Design challenges.',
    estimatedHours: 2,
    sortOrder: 90,
    concepts: ['reliability', 'scalability'],
  },
  {
    id: 'tanenbaum-ch5',
    title: 'Distributed Systems Ch 5: Replication',
    type: 'book',
    author: 'Tanenbaum & Van Steen',
    phase: 1,
    description: 'Data-centric and client-centric consistency. Replica management.',
    estimatedHours: 4,
    sortOrder: 100,
    concepts: ['replication', 'consistency-models'],
    prerequisites: ['reliability'],
  },
  {
    id: 'sre-ch3',
    title: 'SRE Book Ch 3: Embracing Risk',
    type: 'book',
    author: 'Google',
    phase: 1,
    description: 'Why 100% reliability is wrong target. Error budgets. Risk tolerance.',
    estimatedHours: 2,
    sortOrder: 110,
    concepts: ['embracing-risk', 'slos-slis'],
  },
  {
    id: 'sre-ch4',
    title: 'SRE Book Ch 4: Service Level Objectives',
    type: 'book',
    author: 'Google',
    phase: 1,
    description: 'Defining and measuring SLOs. Choosing SLIs. Error budgets in practice.',
    estimatedHours: 2,
    sortOrder: 120,
    concepts: ['slos-slis'],
  },
  {
    id: 'sre-ch6',
    title: 'SRE Book Ch 6: Monitoring Distributed Systems',
    type: 'book',
    author: 'Google',
    phase: 1,
    description: 'The four golden signals. White-box vs black-box monitoring. Alert philosophy.',
    estimatedHours: 2,
    sortOrder: 130,
    concepts: ['monitoring'],
    prerequisites: ['slos-slis'],
  },
  {
    id: 'tail-at-scale-paper',
    title: 'The Tail at Scale',
    type: 'paper',
    url: 'https://research.google/pubs/the-tail-at-scale/',
    author: 'Dean & Barroso (Google)',
    phase: 1,
    description: 'SIGOPS Hall of Fame paper. Why tail latency dominates at scale. Techniques: hedged requests, tied requests, canary requests.',
    estimatedHours: 2,
    sortOrder: 140,
    concepts: ['tail-latency'],
    prerequisites: ['slos-slis', 'monitoring'],
  },

  // ---------------------------------------------------------------------------
  // PHASE 1: COURSES - Distributed Systems
  // ---------------------------------------------------------------------------
  {
    id: 'mit-6824',
    title: 'MIT 6.824: Distributed Systems',
    type: 'course',
    url: 'https://pdos.csail.mit.edu/6.824/',
    phase: 1,
    description: 'Graduate-level course covering fault tolerance, replication, consistency. Includes Raft implementation.',
    estimatedHours: 40,
    sortOrder: 150,
    concepts: ['replication', 'consensus', 'distributed-failures', 'consistency-models'],
    prerequisites: ['reliability'],
  },
  {
    id: 'stanford-cs244b',
    title: 'Stanford CS244b: Distributed Systems',
    type: 'course',
    url: 'https://www.scs.stanford.edu/20sp-cs244b/',
    phase: 1,
    description: 'Focus on practical distributed systems. Case studies of real systems.',
    estimatedHours: 30,
    sortOrder: 160,
    concepts: ['replication', 'partitioning', 'consensus'],
    prerequisites: ['reliability', 'scalability'],
  },

  // ---------------------------------------------------------------------------
  // PHASE 1: VIDEOS - Distributed Systems (Hussein Nasser)
  // ---------------------------------------------------------------------------
  {
    id: 'hussein-backend-beginner',
    title: 'Hussein Nasser - Backend Engineering (Beginner)',
    type: 'course',
    url: 'https://www.youtube.com/playlist?list=PLQnljOFTspQUNnO4p00ua_C5mKTfldiYT',
    author: 'Hussein Nasser',
    phase: 1,
    description: 'System design fundamentals every backend engineer needs. APIs, databases, caching, scaling.',
    estimatedHours: 13,
    sortOrder: 170,
    concepts: ['scalability', 'reliability', 'data-models'],
  },
  {
    id: 'hussein-distributed-systems',
    title: 'Hussein Nasser - Distributed Systems',
    type: 'course',
    url: 'https://www.youtube.com/c/HusseinNasser-software-engineering',
    author: 'Hussein Nasser',
    phase: 1,
    description: 'Distributed transactions, two-phase commit, database sharding, Kafka, caching techniques.',
    estimatedHours: 5,
    sortOrder: 180,
    concepts: ['partitioning', 'replication', 'distributed-failures', 'stream-processing'],
    prerequisites: ['reliability', 'scalability'],
  },
  {
    id: 'hussein-backend-advanced',
    title: 'Hussein Nasser - Backend Engineering (Advanced)',
    type: 'course',
    url: 'https://www.youtube.com/playlist?list=PLQnljOFTspQUybacGRk1b_p13dgI-SmcZ',
    author: 'Hussein Nasser',
    phase: 1,
    description: 'Database replication, distributed transactions, network protocols, connection management, load balancing.',
    estimatedHours: 10,
    sortOrder: 190,
    concepts: ['replication', 'partitioning', 'consistency-models'],
    prerequisites: ['reliability', 'scalability'],
  },
  {
    id: 'hussein-backpressure',
    title: 'Hussein Nasser - Backpressure Explained',
    type: 'video',
    url: 'https://www.youtube.com/watch?v=8KPIPSItIHs',
    author: 'Hussein Nasser',
    phase: 1,
    description: 'Understanding backpressure in distributed systems.',
    estimatedHours: 1,
    sortOrder: 200,
    concepts: ['reliability', 'scalability'],
  },
  {
    id: 'hussein-latency-throughput',
    title: 'Hussein Nasser - Latency vs Throughput Explained',
    type: 'video',
    url: 'https://www.youtube.com/watch?v=m64WBXQM4PE',
    author: 'Hussein Nasser',
    phase: 1,
    description: 'Deep dive into latency vs throughput trade-off.',
    estimatedHours: 2,
    sortOrder: 210,
    concepts: ['scalability', 'tail-latency'],
  },
  {
    id: 'hussein-system-design',
    title: 'Hussein Nasser - System Design Fundamentals',
    type: 'video',
    url: 'https://www.youtube.com/watch?v=0163cssUxLA',
    author: 'Hussein Nasser',
    phase: 1,
    description: 'System design fundamentals for engineers.',
    estimatedHours: 10,
    sortOrder: 220,
    concepts: ['scalability', 'reliability'],
  },

  // ---------------------------------------------------------------------------
  // PHASE 2: ML Infrastructure Bridge
  // ---------------------------------------------------------------------------
  {
    id: 'megatron-lm-paper',
    title: 'Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism',
    type: 'paper',
    url: 'https://arxiv.org/abs/1909.08053',
    author: 'Shoeybi et al. (NVIDIA)',
    phase: 2,
    description: 'How NVIDIA trains massive models across GPUs. Introduces tensor parallelism for Transformers. Demonstrates efficient GPU utilization, inter-node communication, and scaling to billions of parameters.',
    estimatedHours: 5,
    sortOrder: 10,
    concepts: ['gpu-compute-fundamentals', 'distributed-training'],
    prerequisites: ['scalability'],
  },
  {
    id: 'zero-paper',
    title: 'ZeRO: Memory Optimizations Toward Training Trillion Parameter Models',
    type: 'paper',
    url: 'https://arxiv.org/abs/1910.02054',
    author: 'Rajbhandari et al. (Microsoft)',
    phase: 2,
    description: 'Foundation of DeepSpeed. Three stages of memory optimization: partition optimizer states (ZeRO-1), gradients (ZeRO-2), parameters (ZeRO-3). Enables training models 8x larger on same hardware.',
    estimatedHours: 4,
    sortOrder: 20,
    concepts: ['data-parallelism', 'gpu-memory-management', 'distributed-training'],
    prerequisites: ['gpu-compute-fundamentals'],
  },
  {
    id: 'hidden-tech-debt-paper',
    title: 'Hidden Technical Debt in Machine Learning Systems',
    type: 'paper',
    url: 'https://papers.nips.cc/paper/2015/hash/86df7dcfd896fcaf2674f757a2463eba-Abstract.html',
    author: 'Sculley et al. (Google)',
    phase: 2,
    description: 'NIPS 2015 classic. ML code is a small fraction of real ML systems. Covers pipeline jungles, data dependencies, configuration debt, monitoring, and experiment management. The paper that launched MLOps as a discipline.',
    estimatedHours: 3,
    sortOrder: 30,
    concepts: ['ml-data-pipelines', 'experiment-tracking'],
    prerequisites: ['monitoring'],
  },
  {
    id: 'tfx-paper',
    title: 'TFX: A TensorFlow-Based Production-Scale Machine Learning Platform',
    type: 'paper',
    url: 'https://dl.acm.org/doi/10.1145/3097983.3098021',
    author: 'Baylor et al. (Google)',
    phase: 2,
    description: 'KDD 2017. How Google deploys ML at scale. Data validation, feature engineering, training, serving, and monitoring as a unified platform. Blueprint for production ML infrastructure.',
    estimatedHours: 4,
    sortOrder: 40,
    concepts: ['model-serving-infra', 'ml-data-pipelines'],
    prerequisites: ['scalability', 'reliability', 'slos-slis'],
  },
  {
    id: 'feast-paper',
    title: 'Feast: Feature Store for Machine Learning',
    type: 'paper',
    url: 'https://arxiv.org/abs/2105.12738',
    author: 'Tecton/Google',
    phase: 2,
    description: 'Design and implementation of an open-source feature store. Online/offline consistency, point-in-time correctness, feature sharing across teams. Prevents the #1 cause of ML bugs: train-serve skew.',
    estimatedHours: 3,
    sortOrder: 50,
    concepts: ['feature-stores'],
    prerequisites: ['ml-data-pipelines', 'storage-engines'],
  },
  {
    id: 'mixed-precision-paper',
    title: 'Mixed Precision Training',
    type: 'paper',
    url: 'https://arxiv.org/abs/1710.03740',
    author: 'Micikevicius et al. (NVIDIA/Baidu)',
    phase: 2,
    description: 'ICLR 2018. Training with FP16 while maintaining FP32 accuracy. Loss scaling technique to prevent underflow. 2-3x speedup on modern GPUs. Foundation for all efficient training.',
    estimatedHours: 3,
    sortOrder: 60,
    concepts: ['gpu-memory-management', 'gpu-compute-fundamentals'],
    prerequisites: ['gpu-compute-fundamentals'],
  },

  // ---------------------------------------------------------------------------
  // PHASE 3: PAPERS - Transformer Foundations
  // ---------------------------------------------------------------------------
  {
    id: 'attention-paper',
    title: 'Attention Is All You Need',
    type: 'paper',
    url: 'https://arxiv.org/abs/1706.03762',
    author: 'Vaswani et al.',
    phase: 3,
    description: 'The original Transformer paper. Introduces self-attention, multi-head attention, positional encoding.',
    estimatedHours: 6,
    sortOrder: 10,
    concepts: ['attention-mechanism', 'transformer-architecture', 'query-key-value', 'positional-encoding'],
  },
  {
    id: 'scaling-laws-paper',
    title: 'Scaling Laws for Neural Language Models',
    type: 'paper',
    url: 'https://arxiv.org/abs/2001.08361',
    author: 'Kaplan et al. (OpenAI)',
    phase: 3,
    description: 'Empirical study of how loss scales with model size, data, and compute.',
    estimatedHours: 4,
    sortOrder: 20,
    concepts: ['scaling-laws'],
    prerequisites: ['transformer-architecture'],
  },
  {
    id: 'chinchilla-paper',
    title: 'Training Compute-Optimal Large Language Models',
    type: 'paper',
    url: 'https://arxiv.org/abs/2203.15556',
    author: 'Hoffmann et al. (DeepMind)',
    phase: 3,
    description: 'Chinchilla paper. Shows models are undertrained, optimal scaling requires more data.',
    estimatedHours: 3,
    sortOrder: 30,
    concepts: ['compute-optimal-training'],
    prerequisites: ['scaling-laws'],
  },
  {
    id: 'foundation-models-paper',
    title: 'On the Opportunities and Risks of Foundation Models',
    type: 'paper',
    url: 'https://arxiv.org/abs/2108.07258',
    author: 'Bommasani et al. (Stanford)',
    phase: 3,
    description: 'Comprehensive overview of foundation models: capabilities, risks, societal impact.',
    estimatedHours: 8,
    sortOrder: 40,
    concepts: ['foundation-models'],
    prerequisites: ['scaling-laws'],
  },
  {
    id: 'lora-paper',
    title: 'LoRA: Low-Rank Adaptation of Large Language Models',
    type: 'paper',
    url: 'https://arxiv.org/abs/2106.09685',
    author: 'Hu et al. (Microsoft)',
    phase: 3,
    description: 'Foundational paper on parameter-efficient fine-tuning. Reduces trainable parameters by 10,000x.',
    estimatedHours: 3,
    sortOrder: 50,
    concepts: ['fine-tuning-efficiency'],
    prerequisites: ['transformer-architecture'],
  },
  {
    id: 'dpo-paper',
    title: 'Direct Preference Optimization: Your Language Model is Secretly a Reward Model',
    type: 'paper',
    url: 'https://arxiv.org/abs/2305.18290',
    author: 'Rafailov et al. (Stanford)',
    phase: 3,
    description: 'Simpler alternative to RLHF. No reward model, no RL needed. Matches or exceeds PPO-based RLHF.',
    estimatedHours: 3,
    sortOrder: 55,
    concepts: ['dpo'],
    prerequisites: ['foundation-models'],
  },
  {
    id: 'yarn-paper',
    title: 'YaRN: Efficient Context Window Extension of Large Language Models',
    type: 'paper',
    url: 'https://arxiv.org/abs/2309.00071',
    author: 'Peng et al.',
    phase: 3,
    description: 'Extends RoPE-based models to longer contexts with minimal fine-tuning. Compute-efficient context extension method.',
    estimatedHours: 3,
    sortOrder: 58,
    concepts: ['context-extension'],
    prerequisites: ['positional-encoding', 'attention-mechanism'],
  },

  // ---------------------------------------------------------------------------
  // PHASE 3: VIDEOS - Transformer Understanding (Karpathy)
  // ---------------------------------------------------------------------------
  {
    id: 'karpathy-nn-zero-to-hero',
    title: 'Andrej Karpathy - Neural Networks: Zero to Hero',
    type: 'course',
    url: 'https://www.youtube.com/playlist?list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ',
    author: 'Andrej Karpathy',
    phase: 3,
    description: 'Complete course from backprop basics to GPT. Builds intuition for attention, context costs, and why reasoning is expensive.',
    estimatedHours: 15,
    sortOrder: 60,
    concepts: ['attention-mechanism', 'transformer-architecture'],
  },
  {
    id: 'karpathy-build-gpt',
    title: 'Andrej Karpathy - Let\'s Build GPT from Scratch',
    type: 'video',
    url: 'https://www.youtube.com/watch?v=kCc8FmEb1nY',
    author: 'Andrej Karpathy',
    phase: 3,
    description: 'Building GPT from scratch in code. Understanding attention, positional encoding, and inference in practice.',
    estimatedHours: 4,
    sortOrder: 70,
    concepts: ['transformer-architecture', 'attention-mechanism', 'query-key-value'],
    prerequisites: ['attention-mechanism'],
  },
  {
    id: 'karpathy-intro-llms',
    title: 'Andrej Karpathy - Intro to Large Language Models',
    type: 'video',
    url: 'https://www.youtube.com/watch?v=zjkBMFhNj_g',
    author: 'Andrej Karpathy',
    phase: 3,
    description: 'One-hour overview of how LLMs work, from tokens to training to inference.',
    estimatedHours: 2,
    sortOrder: 80,
    concepts: ['foundation-models', 'scaling-laws'],
  },
  {
    id: 'gpt3-paper',
    title: 'Language Models are Few-Shot Learners (GPT-3)',
    type: 'paper',
    url: 'https://arxiv.org/abs/2005.14165',
    author: 'Brown et al. (OpenAI)',
    phase: 3,
    description: 'The paper that demonstrated emergence at scale. 175B parameters, few-shot learning without fine-tuning. Established the foundation model paradigm and in-context learning.',
    estimatedHours: 6,
    sortOrder: 90,
    concepts: ['foundation-models', 'scaling-laws'],
    prerequisites: ['transformer-architecture'],
  },

  // ---------------------------------------------------------------------------
  // PHASE 4: PAPERS - Reasoning & Agents
  // ---------------------------------------------------------------------------
  {
    id: 'react-paper',
    title: 'ReAct: Synergizing Reasoning and Acting in Language Models',
    type: 'paper',
    url: 'https://arxiv.org/abs/2210.03629',
    author: 'Yao et al.',
    phase: 4,
    description: 'Interleaving reasoning traces with actions. Foundation for modern LLM agents.',
    estimatedHours: 4,
    sortOrder: 10,
    concepts: ['react-pattern', 'chain-of-thought', 'tool-use'],
    prerequisites: ['foundation-models'],
  },
  {
    id: 'tree-of-thoughts-paper',
    title: 'Tree of Thoughts: Deliberate Problem Solving with Large Language Models',
    type: 'paper',
    url: 'https://arxiv.org/abs/2305.10601',
    author: 'Yao et al.',
    phase: 4,
    description: 'Exploring multiple reasoning paths with evaluation and backtracking.',
    estimatedHours: 3,
    sortOrder: 20,
    concepts: ['tree-of-thoughts'],
    prerequisites: ['chain-of-thought'],
  },
  {
    id: 'reflexion-paper',
    title: 'Reflexion: Language Agents with Verbal Reinforcement Learning',
    type: 'paper',
    url: 'https://arxiv.org/abs/2303.11366',
    author: 'Shinn et al.',
    phase: 4,
    description: 'Agents that learn from verbal feedback stored in memory.',
    estimatedHours: 3,
    sortOrder: 30,
    concepts: ['reflexion'],
    prerequisites: ['react-pattern'],
  },
  {
    id: 'toolformer-paper',
    title: 'Toolformer: Language Models Can Teach Themselves to Use Tools',
    type: 'paper',
    url: 'https://arxiv.org/abs/2302.04761',
    author: 'Schick et al. (Meta)',
    phase: 4,
    description: 'Self-supervised approach to teaching LLMs when and how to use tools.',
    estimatedHours: 3,
    sortOrder: 40,
    concepts: ['tool-use'],
    prerequisites: ['foundation-models'],
  },
  {
    id: 'prompt-report-paper',
    title: 'The Prompt Report: A Systematic Survey of Prompt Engineering Techniques',
    type: 'paper',
    url: 'https://arxiv.org/abs/2406.06608',
    author: 'Schulhoff et al.',
    phase: 4,
    description: 'Most comprehensive survey on prompt engineering. Taxonomy of 58 techniques across 33 vocabulary terms.',
    estimatedHours: 6,
    sortOrder: 50,
    concepts: ['prompt-engineering', 'chain-of-thought'],
    prerequisites: ['foundation-models'],
  },
  {
    id: 'gorilla-paper',
    title: 'Gorilla: Large Language Model Connected with Massive APIs',
    type: 'paper',
    url: 'https://arxiv.org/abs/2305.15334',
    author: 'Patil et al. (Berkeley)',
    phase: 4,
    description: 'Training LLMs for accurate API/function calling. Foundation for reliable tool use.',
    estimatedHours: 3,
    sortOrder: 60,
    concepts: ['structured-output', 'tool-use'],
    prerequisites: ['foundation-models'],
  },
  {
    id: 'openai-o1-blog',
    title: 'Learning to Reason with LLMs',
    type: 'article',
    url: 'https://openai.com/index/learning-to-reason-with-llms/',
    author: 'OpenAI',
    phase: 4,
    description: 'OpenAI\'s introduction to o1 reasoning models. Test-time compute scaling.',
    estimatedHours: 2,
    sortOrder: 70,
    concepts: ['test-time-compute', 'reasoning-models'],
    prerequisites: ['chain-of-thought'],
  },
  {
    id: 'deepseek-r1-paper',
    title: 'DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning',
    type: 'paper',
    url: 'https://arxiv.org/abs/2501.12948',
    author: 'DeepSeek',
    phase: 4,
    description: 'Reasoning emerges from pure RL without labeled data. Open weights available.',
    estimatedHours: 4,
    sortOrder: 75,
    concepts: ['reasoning-models', 'test-time-compute'],
    prerequisites: ['chain-of-thought', 'foundation-models'],
  },
  {
    id: 's1-test-time-scaling',
    title: 's1: Simple Test-Time Scaling',
    type: 'paper',
    url: 'https://arxiv.org/abs/2501.19393',
    author: 'Muennighoff et al.',
    phase: 4,
    description: 'Practical guide to test-time compute scaling. Budget forcing for reasoning length control.',
    estimatedHours: 2,
    sortOrder: 80,
    concepts: ['test-time-compute'],
    prerequisites: ['reasoning-models'],
  },
  {
    id: 'agent-sandbox-patterns',
    title: 'The Two Patterns by Which Agents Connect Sandboxes',
    type: 'article',
    url: 'https://blog.langchain.com/the-two-patterns-by-which-agents-connect-sandboxes/',
    author: 'Harrison Chase (LangChain)',
    phase: 4,
    description: 'Agent IN Sandbox vs Sandbox as Tool. Security, latency, and operational trade-offs.',
    estimatedHours: 1,
    sortOrder: 85,
    concepts: ['tool-use', 'react-pattern', 'plan-and-execute'],
  },
  {
    id: 'autogen-paper',
    title: 'AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation',
    type: 'paper',
    url: 'https://arxiv.org/abs/2308.08155',
    author: 'Wu et al. (Microsoft)',
    phase: 4,
    description: 'Framework for multi-agent conversations. Supervisor-worker patterns, customizable agents, human-in-the-loop integration.',
    estimatedHours: 4,
    sortOrder: 88,
    concepts: ['multi-agent-systems'],
    prerequisites: ['plan-and-execute', 'tool-use'],
  },
  {
    id: 'anthropic-effective-agents',
    title: 'Building Effective Agents',
    type: 'article',
    url: 'https://www.anthropic.com/engineering/building-effective-agents',
    author: 'Anthropic',
    phase: 4,
    description: 'Anthropic\'s guide to building reliable agent systems. Patterns for retries, validation, HITL, and sandboxing.',
    estimatedHours: 2,
    sortOrder: 90,
    concepts: ['agent-reliability', 'react-pattern'],
    prerequisites: ['react-pattern', 'structured-output'],
  },

  // ---------------------------------------------------------------------------
  // PHASE 4: VIDEOS - Agents & Reasoning
  // ---------------------------------------------------------------------------
  {
    id: 'karpathy-llm-os',
    title: 'Andrej Karpathy - LLMs as Operating Systems',
    type: 'video',
    author: 'Andrej Karpathy',
    phase: 4,
    description: 'Conceptualizing LLMs as a new kind of operating system. Memory, tools, scheduling.',
    estimatedHours: 2,
    sortOrder: 95,
    concepts: ['foundation-models', 'tool-use'],
    prerequisites: ['foundation-models'],
  },
  {
    id: 'react-prompting-videos',
    title: 'ReAct Prompting Explained (AssemblyAI/HuggingFace)',
    type: 'video',
    url: 'https://www.youtube.com/@AssemblyAI',
    author: 'AssemblyAI',
    phase: 4,
    description: 'Visual explanations of ReAct prompting pattern.',
    estimatedHours: 2,
    sortOrder: 100,
    concepts: ['react-pattern', 'chain-of-thought'],
    prerequisites: ['foundation-models'],
  },
  {
    id: 'cot-prompting-videos',
    title: 'Chain of Thought Prompting Explained',
    type: 'video',
    url: 'https://www.youtube.com/@DeepLearningAI',
    author: 'DeepLearning.AI',
    phase: 4,
    description: 'Understanding chain of thought prompting. Zero-shot vs few-shot CoT.',
    estimatedHours: 2,
    sortOrder: 110,
    concepts: ['chain-of-thought'],
    prerequisites: ['foundation-models'],
  },
  {
    id: 'lilian-weng-agents',
    title: 'Lilian Weng - LLM Agent Architectures',
    type: 'article',
    url: 'https://lilianweng.github.io/',
    author: 'Lilian Weng (OpenAI)',
    phase: 4,
    description: 'Comprehensive blog posts on agent architectures: tool-using agents, planning vs reacting, memory.',
    estimatedHours: 6,
    sortOrder: 120,
    concepts: ['react-pattern', 'plan-and-execute', 'tool-use'],
    prerequisites: ['foundation-models'],
  },

  // ---------------------------------------------------------------------------
  // PHASE 5: PAPERS - RAG & Memory
  // ---------------------------------------------------------------------------
  {
    id: 'rag-paper',
    title: 'Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks',
    type: 'paper',
    url: 'https://arxiv.org/abs/2005.11401',
    author: 'Lewis et al. (Meta)',
    phase: 5,
    description: 'The original RAG paper. Combining retrieval with generation.',
    estimatedHours: 4,
    sortOrder: 10,
    concepts: ['rag-basics', 'embeddings'],
    prerequisites: ['foundation-models'],
  },
  {
    id: 'lost-in-middle-paper',
    title: 'Lost in the Middle: How Language Models Use Long Contexts',
    type: 'paper',
    url: 'https://arxiv.org/abs/2307.03172',
    author: 'Liu et al.',
    phase: 5,
    description: 'LLMs struggle with information in the middle of long contexts. U-shaped attention.',
    estimatedHours: 2,
    sortOrder: 20,
    concepts: ['lost-in-middle', 'context-window-limits'],
    prerequisites: ['rag-basics'],
  },
  {
    id: 'long-context-paper',
    title: 'Do Long-Context Models Really Understand?',
    type: 'paper',
    phase: 5,
    description: 'Critical examination of whether long context = better understanding.',
    estimatedHours: 2,
    sortOrder: 30,
    concepts: ['context-window-limits'],
    prerequisites: ['attention-mechanism'],
  },
  {
    id: 'memgpt-paper',
    title: 'MemGPT: Towards LLMs as Operating Systems',
    type: 'paper',
    url: 'https://arxiv.org/abs/2310.08560',
    author: 'Packer et al. (Berkeley)',
    phase: 5,
    description: 'Virtual context management inspired by OS memory hierarchy.',
    estimatedHours: 4,
    sortOrder: 40,
    concepts: ['memory-management', 'external-memory'],
    prerequisites: ['external-memory', 'tool-use'],
  },
  {
    id: 'generative-agents-paper',
    title: 'Generative Agents: Interactive Simulacra of Human Behavior',
    type: 'paper',
    url: 'https://arxiv.org/abs/2304.03442',
    author: 'Park et al. (Stanford)',
    phase: 5,
    description: 'Agents with memory, reflection, and planning. Memory stream architecture.',
    estimatedHours: 5,
    sortOrder: 50,
    concepts: ['memory-management', 'external-memory', 'reflexion'],
    prerequisites: ['external-memory', 'react-pattern'],
  },
  {
    id: 'graphrag-paper',
    title: 'From Local to Global: A Graph RAG Approach to Query-Focused Summarization',
    type: 'paper',
    url: 'https://arxiv.org/abs/2404.16130',
    author: 'Edge et al. (Microsoft)',
    phase: 5,
    description: 'Microsoft GraphRAG: community detection + summarization for global queries. Outperforms naive RAG on complex questions requiring multi-document synthesis.',
    estimatedHours: 4,
    sortOrder: 55,
    concepts: ['graph-rag'],
    prerequisites: ['rag-basics', 'data-models'],
  },
  {
    id: 'ragas-docs',
    title: 'RAGAS: Evaluation Framework for RAG',
    type: 'article',
    url: 'https://docs.ragas.io/',
    author: 'RAGAS',
    phase: 5,
    description: 'Framework for evaluating RAG pipelines. Metrics: faithfulness, answer relevancy, context precision/recall. Automated evaluation without ground truth.',
    estimatedHours: 3,
    sortOrder: 58,
    concepts: ['rag-evaluation'],
    prerequisites: ['rag-basics'],
  },
  {
    id: 'hyde-paper',
    title: 'Precise Zero-Shot Dense Retrieval without Relevance Labels (HyDE)',
    type: 'paper',
    url: 'https://arxiv.org/abs/2212.10496',
    author: 'Gao et al.',
    phase: 5,
    description: 'Hypothetical Document Embeddings: generate a hypothetical answer, then use it for retrieval. Improves zero-shot retrieval significantly.',
    estimatedHours: 2,
    sortOrder: 60,
    concepts: ['query-decomposition'],
    prerequisites: ['rag-basics', 'chain-of-thought'],
  },
  {
    id: 'colbertv2-paper',
    title: 'ColBERTv2: Effective and Efficient Retrieval via Lightweight Late Interaction',
    type: 'paper',
    url: 'https://arxiv.org/abs/2112.01488',
    author: 'Santhanam et al.',
    phase: 5,
    description: 'Token-level late interaction for efficient reranking. Better quality than bi-encoders, much faster than cross-encoders.',
    estimatedHours: 3,
    sortOrder: 62,
    concepts: ['reranking'],
    prerequisites: ['vector-search'],
  },
  {
    id: 'self-rag-paper',
    title: 'Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection',
    type: 'paper',
    url: 'https://arxiv.org/abs/2310.11511',
    author: 'Asai et al.',
    phase: 5,
    description: 'LLM learns when to retrieve and self-evaluates quality. Foundation for agentic RAG approaches.',
    estimatedHours: 3,
    sortOrder: 65,
    concepts: ['agentic-rag'],
    prerequisites: ['rag-basics', 'react-pattern'],
  },
  {
    id: 'needle-in-haystack',
    title: 'Needle in a Haystack: Pressure Testing Long Context LLMs',
    type: 'article',
    url: 'https://github.com/gkamradt/LLMTest_NeedleInAHaystack',
    author: 'Greg Kamradt',
    phase: 5,
    description: 'Empirical test of long-context performance. Reveals when RAG beats long context and vice versa.',
    estimatedHours: 2,
    sortOrder: 68,
    concepts: ['rag-vs-long-context'],
    prerequisites: ['rag-basics', 'context-window-limits'],
  },

  // ---------------------------------------------------------------------------
  // PHASE 5: ARTICLES - RAG Practical
  // ---------------------------------------------------------------------------
  {
    id: 'llamaindex-rag-pitfalls',
    title: 'LlamaIndex Blog: RAG Pitfalls',
    type: 'article',
    url: 'https://www.llamaindex.ai/blog',
    phase: 5,
    description: 'Practical lessons on what goes wrong with RAG in production.',
    estimatedHours: 2,
    sortOrder: 70,
    concepts: ['chunking-strategies', 'hybrid-search'],
    prerequisites: ['rag-basics'],
  },
  {
    id: 'pinecone-rag-production',
    title: 'Pinecone: RAG in Production',
    type: 'video',
    url: 'https://www.youtube.com/c/PineconeIO',
    phase: 5,
    description: 'Engineering talks on running RAG at scale.',
    estimatedHours: 3,
    sortOrder: 80,
    concepts: ['vector-search', 'hybrid-search', 'chunking-strategies'],
    prerequisites: ['rag-basics', 'embeddings'],
  },
  {
    id: 'clawvault-agent-memory',
    title: 'Solving Memory for Openclaw & General Agents (ClawVault)',
    type: 'article',
    url: 'https://clawvault.dev',
    author: 'Pedro (@sillydarket), Versatly',
    phase: 5,
    description: 'File-based agent memory architecture using markdown + YAML + wiki-links.',
    estimatedHours: 1,
    sortOrder: 90,
    concepts: ['external-memory', 'memory-management'],
  },

  // ---------------------------------------------------------------------------
  // PHASE 6: Multimodal & Emerging — NEW
  // ---------------------------------------------------------------------------
  {
    id: 'clip-paper',
    title: 'CLIP: Learning Transferable Visual Models From Natural Language Supervision',
    type: 'paper',
    url: 'https://arxiv.org/abs/2103.00020',
    author: 'Radford et al. (OpenAI)',
    phase: 6,
    description: 'Foundation of multimodal AI. Contrastive learning aligns image and text embeddings.',
    estimatedHours: 4,
    sortOrder: 10,
    concepts: ['multimodal-embeddings'],
    prerequisites: ['transformer-architecture'],
  },
  {
    id: 'llava-paper',
    title: 'LLaVA: Visual Instruction Tuning',
    type: 'paper',
    url: 'https://arxiv.org/abs/2304.08485',
    author: 'Liu et al.',
    phase: 6,
    description: 'NeurIPS 2023 Oral. Architecture: CLIP encoder + projection + LLaMA. Open-source VLM.',
    estimatedHours: 4,
    sortOrder: 20,
    concepts: ['vision-language-models'],
    prerequisites: ['multimodal-embeddings', 'foundation-models'],
  },
  {
    id: 'mamba-paper',
    title: 'Mamba: Linear-Time Sequence Modeling with Selective State Spaces',
    type: 'paper',
    url: 'https://arxiv.org/abs/2312.00752',
    author: 'Gu & Dao',
    phase: 6,
    description: 'State space model with input-dependent selection. Linear scaling alternative to Transformers. 5x faster inference on long sequences.',
    estimatedHours: 4,
    sortOrder: 30,
    concepts: ['state-space-models'],
    prerequisites: ['transformer-architecture'],
  },
  {
    id: 'whisper-paper',
    title: 'Robust Speech Recognition via Large-Scale Weak Supervision',
    type: 'paper',
    url: 'https://arxiv.org/abs/2212.04356',
    author: 'Radford et al. (OpenAI)',
    phase: 6,
    description: 'Whisper ASR model. Trained on 680K hours of multilingual data. Robust to noise, accents, domains.',
    estimatedHours: 3,
    sortOrder: 40,
    concepts: ['speech-models'],
    prerequisites: ['transformer-architecture'],
  },
  {
    id: 'distillation-survey',
    title: 'A Survey on Knowledge Distillation of Large Language Models',
    type: 'paper',
    url: 'https://arxiv.org/abs/2402.13116',
    author: 'Xu et al.',
    phase: 6,
    description: 'Comprehensive survey of LLM distillation methods. Output, feature, and data distillation approaches.',
    estimatedHours: 4,
    sortOrder: 50,
    concepts: ['knowledge-distillation'],
    prerequisites: ['foundation-models', 'fine-tuning-efficiency'],
  },

  // ---------------------------------------------------------------------------
  // PHASE 7: PAPERS - Safety & Guardrails
  // ---------------------------------------------------------------------------
  {
    id: 'constitutional-ai-paper',
    title: 'Constitutional AI: Harmlessness from AI Feedback',
    type: 'paper',
    url: 'https://arxiv.org/abs/2212.08073',
    author: 'Bai et al. (Anthropic)',
    phase: 7,
    description: 'Training AI to follow principles through self-critique. Alternative to RLHF.',
    estimatedHours: 4,
    sortOrder: 10,
    concepts: ['constitutional-ai'],
    prerequisites: ['foundation-models'],
  },
  {
    id: 'self-consistency-paper',
    title: 'Self-Consistency Improves Chain of Thought Reasoning',
    type: 'paper',
    url: 'https://arxiv.org/abs/2203.11171',
    author: 'Wang et al. (Google)',
    phase: 7,
    description: 'Sample multiple reasoning paths, take majority vote. Simple but effective.',
    estimatedHours: 2,
    sortOrder: 20,
    concepts: ['self-consistency'],
    prerequisites: ['chain-of-thought'],
  },
  {
    id: 'red-teaming-paper',
    title: 'Red Teaming Language Models',
    type: 'paper',
    phase: 7,
    description: 'Methods for adversarial testing of LLMs before deployment.',
    estimatedHours: 3,
    sortOrder: 30,
    concepts: ['red-teaming', 'offline-evaluation'],
    prerequisites: ['foundation-models'],
  },
  {
    id: 'helm-paper',
    title: 'Holistic Evaluation of Language Models (HELM)',
    type: 'paper',
    url: 'https://arxiv.org/abs/2211.09110',
    author: 'Liang et al. (Stanford)',
    phase: 7,
    description: 'Comprehensive framework for LLM evaluation: 7 metrics across 42 scenarios.',
    estimatedHours: 5,
    sortOrder: 40,
    concepts: ['offline-evaluation'],
    prerequisites: ['foundation-models'],
  },
  {
    id: 'ml-testing-survey',
    title: 'Test & Evaluation Best Practices for ML-Enabled Systems',
    type: 'paper',
    url: 'https://arxiv.org/abs/2310.06800',
    author: 'Carnegie Mellon SEI',
    phase: 7,
    description: 'T&E across ML lifecycle: component, integration, post-deployment.',
    estimatedHours: 4,
    sortOrder: 50,
    concepts: ['online-evaluation', 'offline-evaluation'],
    prerequisites: ['foundation-models', 'slos-slis'],
  },
  {
    id: 'ml-testing-practices',
    title: 'Machine Learning Testing: Survey, Landscapes and Horizons',
    type: 'paper',
    url: 'https://arxiv.org/abs/1906.10742',
    phase: 7,
    description: 'Comprehensive survey of ML testing. Online testing, runtime monitoring.',
    estimatedHours: 6,
    sortOrder: 60,
    concepts: ['online-evaluation', 'offline-evaluation'],
    prerequisites: ['foundation-models'],
  },
  {
    id: 'owasp-llm-top10',
    title: 'OWASP Top 10 for LLM Applications',
    type: 'article',
    url: 'https://owasp.org/www-project-top-10-for-large-language-model-applications/',
    author: 'OWASP',
    phase: 7,
    description: 'Industry standard for LLM security. Essential checklist before any LLM deployment.',
    estimatedHours: 3,
    sortOrder: 65,
    concepts: ['llm-security-owasp', 'prompt-injection'],
    prerequisites: ['foundation-models'],
  },
  {
    id: 'prompt-injection-guide',
    title: 'Prompt Injection: What\'s the worst that can happen?',
    type: 'article',
    url: 'https://simonwillison.net/2023/Apr/14/worst-that-can-happen/',
    author: 'Simon Willison',
    phase: 7,
    description: 'Practical deep-dive into prompt injection attacks. Defense strategies.',
    estimatedHours: 2,
    sortOrder: 68,
    concepts: ['prompt-injection'],
    prerequisites: ['foundation-models', 'rag-basics'],
  },
  {
    id: 'nemo-guardrails-docs',
    title: 'NeMo Guardrails Documentation',
    type: 'article',
    url: 'https://docs.nvidia.com/nemo/guardrails/',
    author: 'NVIDIA',
    phase: 7,
    description: 'Dialog rails with Colang language. Input/output guardrails, topical rails, fact-checking rails. Production-ready guardrails framework.',
    estimatedHours: 4,
    sortOrder: 72,
    concepts: ['guardrails-libraries'],
    prerequisites: ['output-validation', 'structured-output'],
  },
  {
    id: 'instructor-docs',
    title: 'Instructor: Structured Outputs with Pydantic',
    type: 'article',
    url: 'https://python.useinstructor.com/',
    author: 'Jason Liu',
    phase: 7,
    description: 'Structured output extraction with automatic retries and validation. Built on Pydantic. Simple but powerful guardrails approach.',
    estimatedHours: 3,
    sortOrder: 74,
    concepts: ['guardrails-libraries'],
    prerequisites: ['structured-output'],
  },
  {
    id: 'promptfoo-docs',
    title: 'promptfoo: LLM Evaluation & Testing',
    type: 'article',
    url: 'https://www.promptfoo.dev/docs/intro/',
    author: 'promptfoo',
    phase: 7,
    description: 'Open-source tool for testing and evaluating LLM outputs. CI/CD for prompts. LLM-as-judge support.',
    estimatedHours: 3,
    sortOrder: 76,
    concepts: ['eval-harnesses'],
    prerequisites: ['offline-evaluation'],
  },
  {
    id: 'llm-as-judge-paper',
    title: 'Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena',
    type: 'paper',
    url: 'https://arxiv.org/abs/2306.05685',
    author: 'Zheng et al. (LMSYS)',
    phase: 7,
    description: 'Using strong LLMs to evaluate weaker ones. MT-Bench multi-turn benchmark. 80%+ agreement with human judges.',
    estimatedHours: 3,
    sortOrder: 78,
    concepts: ['eval-harnesses', 'offline-evaluation'],
    prerequisites: ['offline-evaluation'],
  },

  // ---------------------------------------------------------------------------
  // PHASE 7: VIDEOS - Safety & Evaluation
  // ---------------------------------------------------------------------------
  {
    id: 'anthropic-safety-talks',
    title: 'Anthropic Safety Research Talks',
    type: 'video',
    url: 'https://www.youtube.com/@anthropic-ai',
    phase: 7,
    description: 'Research presentations on LLM reliability, hallucinations, alignment.',
    estimatedHours: 4,
    sortOrder: 80,
    concepts: ['constitutional-ai', 'offline-evaluation', 'output-validation'],
    prerequisites: ['foundation-models'],
  },
  {
    id: 'deeplearning-ai-llm-evals',
    title: 'DeepLearning.AI - LLM Evaluation',
    type: 'video',
    url: 'https://www.youtube.com/@DeepLearningAI',
    author: 'DeepLearning.AI',
    phase: 7,
    description: 'Why LLM evaluation is hard. Metrics, benchmarks, real-world evaluation strategies.',
    estimatedHours: 3,
    sortOrder: 90,
    concepts: ['offline-evaluation', 'online-evaluation'],
    prerequisites: ['foundation-models'],
  },
  {
    id: 'anthropic-alignment-paper',
    title: 'Sleeper Agents: Training Deceptive LLMs That Persist Through Safety Training',
    type: 'paper',
    url: 'https://arxiv.org/abs/2401.05566',
    author: 'Hubinger et al. (Anthropic)',
    phase: 7,
    description: 'Demonstrates that standard safety training (RLHF, adversarial training) fails to remove backdoor behaviors in LLMs. Raises fundamental questions about alignment verification. Essential for understanding limits of current safety approaches.',
    estimatedHours: 4,
    sortOrder: 100,
    concepts: ['constitutional-ai', 'offline-evaluation'],
    prerequisites: ['foundation-models'],
  },
  {
    id: 'stanford-hai-talks',
    title: 'Stanford HAI Technical Talks',
    type: 'video',
    url: 'https://www.youtube.com/@StanfordHAI',
    author: 'Stanford HAI',
    phase: 7,
    description: 'Technical talks on evaluation, safety, and governance.',
    estimatedHours: 4,
    sortOrder: 110,
    concepts: ['offline-evaluation', 'constitutional-ai'],
    prerequisites: ['foundation-models'],
  },

  // ---------------------------------------------------------------------------
  // PHASE 8: PAPERS - Inference & Economics
  // ---------------------------------------------------------------------------
  {
    id: 'vllm-paper',
    title: 'vLLM: Easy, Fast, and Cheap LLM Serving with PagedAttention',
    type: 'paper',
    url: 'https://arxiv.org/abs/2309.06180',
    author: 'Kwon et al. (Berkeley)',
    phase: 8,
    description: 'PagedAttention for efficient KV cache management. Enables high-throughput serving.',
    estimatedHours: 3,
    sortOrder: 10,
    concepts: ['paged-attention', 'kv-cache', 'batching-inference'],
    prerequisites: ['kv-cache'],
  },
  {
    id: 'speculative-decoding-paper',
    title: 'Speculative Decoding',
    type: 'paper',
    phase: 8,
    description: 'Using small models to draft, large models to verify.',
    estimatedHours: 2,
    sortOrder: 20,
    concepts: ['speculative-decoding'],
    prerequisites: ['kv-cache'],
  },
  {
    id: 'openai-pricing-docs',
    title: 'OpenAI API Pricing Documentation',
    type: 'article',
    url: 'https://openai.com/pricing',
    phase: 8,
    description: 'Understanding token costs, rate limits, batching discounts.',
    estimatedHours: 1,
    sortOrder: 30,
    concepts: ['token-economics'],
  },
  {
    id: 'anthropic-pricing-docs',
    title: 'Anthropic API Pricing Documentation',
    type: 'article',
    url: 'https://www.anthropic.com/pricing',
    phase: 8,
    description: 'Claude pricing tiers, context window costs.',
    estimatedHours: 1,
    sortOrder: 40,
    concepts: ['token-economics'],
  },
  {
    id: 'gptcache-paper',
    title: 'GPTCache: Semantic Cache for LLM Applications',
    type: 'paper',
    url: 'https://arxiv.org/abs/2411.05276',
    author: 'Zilliz',
    phase: 8,
    description: 'Semantic caching using query embeddings. Reduces API calls 60-70%.',
    estimatedHours: 2,
    sortOrder: 50,
    concepts: ['semantic-caching'],
    prerequisites: ['embeddings', 'token-economics'],
  },
  {
    id: 'prompt-caching-docs',
    title: 'Prompt Caching - Claude & OpenAI Documentation',
    type: 'article',
    url: 'https://docs.anthropic.com/en/docs/build-with-claude/prompt-caching',
    phase: 8,
    description: 'API-level prompt caching. 90% cheaper for cached prefixes.',
    estimatedHours: 1,
    sortOrder: 60,
    concepts: ['prompt-caching'],
    prerequisites: ['kv-cache', 'token-economics'],
  },
  {
    id: 'llm-observability-guide',
    title: 'LLM Observability with OpenTelemetry',
    type: 'article',
    url: 'https://opentelemetry.io/blog/2024/llm-observability/',
    phase: 8,
    description: 'Extending observability for LLM systems. Tracing prompts, completions, token usage.',
    estimatedHours: 2,
    sortOrder: 70,
    concepts: ['llm-observability'],
    prerequisites: ['monitoring'],
  },
  {
    id: 'langfuse-observability',
    title: 'Langfuse: Open Source LLM Observability',
    type: 'article',
    url: 'https://langfuse.com/docs',
    phase: 8,
    description: 'Production LLM observability platform. Open source alternative to LangSmith.',
    estimatedHours: 3,
    sortOrder: 80,
    concepts: ['llm-observability', 'online-evaluation'],
    prerequisites: ['monitoring', 'token-economics'],
  },
  {
    id: 'compound-ai-systems-blog',
    title: 'The Shift from Models to Compound AI Systems',
    type: 'article',
    url: 'https://bair.berkeley.edu/blog/2024/02/18/compound-ai-systems/',
    author: 'Zaharia, Khattab et al. (Berkeley)',
    phase: 8,
    description: 'Seminal blog post on Compound AI Systems. Must-read for AI architects.',
    estimatedHours: 2,
    sortOrder: 85,
    concepts: ['compound-ai-systems'],
    prerequisites: ['rag-basics', 'model-routing'],
  },
  {
    id: 'compound-ai-survey',
    title: 'From Standalone LLMs to Integrated Intelligence: A Survey of Compound AI Systems',
    type: 'paper',
    url: 'https://arxiv.org/abs/2406.00584',
    phase: 8,
    description: 'Academic survey on compound AI systems. Blueprint architecture for enterprise.',
    estimatedHours: 4,
    sortOrder: 88,
    concepts: ['compound-ai-systems'],
    prerequisites: ['compound-ai-systems'],
  },
  {
    id: 'switch-transformers-paper',
    title: 'Switch Transformers: Scaling to Trillion Parameter Models',
    type: 'paper',
    url: 'https://arxiv.org/abs/2101.03961',
    author: 'Fedus et al. (Google)',
    phase: 8,
    description: 'Mixture of Experts at scale. Simplified routing, expert parallelism. Foundation for understanding GPT-4 and Mixtral architectures.',
    estimatedHours: 4,
    sortOrder: 90,
    concepts: ['mixture-of-experts'],
    prerequisites: ['foundation-models'],
  },

  // ---------------------------------------------------------------------------
  // PHASE 8: VIDEOS - Inference Infrastructure
  // ---------------------------------------------------------------------------
  {
    id: 'modal-vercel-infrastructure',
    title: 'Modal/Vercel/Replicate - LLM Infrastructure Talks',
    type: 'video',
    url: 'https://www.youtube.com/@modal_labs',
    author: 'Modal / Vercel / Replicate',
    phase: 8,
    description: 'Real LLM infrastructure at scale. Serving, optimization, production deployment.',
    estimatedHours: 4,
    sortOrder: 95,
    concepts: ['batching-inference', 'paged-attention', 'token-economics'],
    prerequisites: ['kv-cache'],
  },
  {
    id: 'openai-research-talks',
    title: 'OpenAI Research Talks',
    type: 'video',
    url: 'https://www.youtube.com/@OpenAI',
    author: 'OpenAI',
    phase: 8,
    description: 'Insights into how OpenAI systems work.',
    estimatedHours: 4,
    sortOrder: 100,
    concepts: ['scaling-laws', 'batching-inference'],
    prerequisites: ['transformer-architecture'],
  },
  {
    id: 'deepmind-systems-talks',
    title: 'DeepMind Systems Talks',
    type: 'video',
    url: 'https://www.youtube.com/@DeepMind',
    author: 'DeepMind',
    phase: 8,
    description: 'Systems-level research from DeepMind. Architecture and scaling.',
    estimatedHours: 4,
    sortOrder: 110,
    concepts: ['scaling-laws', 'compute-optimal-training'],
    prerequisites: ['transformer-architecture'],
  },

  // ---------------------------------------------------------------------------
  // PHASE 9: DOCUMENTATION & VIDEOS - System Design & Integration
  // ---------------------------------------------------------------------------
  {
    id: 'langchain-docs',
    title: 'LangChain Documentation (Architecture)',
    type: 'article',
    url: 'https://python.langchain.com/docs/',
    phase: 9,
    description: 'Official docs. Focus on architecture sections, not tutorials.',
    estimatedHours: 4,
    sortOrder: 10,
    concepts: ['langchain-architecture'],
    prerequisites: ['react-pattern', 'rag-basics'],
  },
  {
    id: 'langchain-videos',
    title: 'LangChain Official Videos',
    type: 'video',
    url: 'https://www.youtube.com/@LangChain',
    author: 'LangChain',
    phase: 9,
    description: 'LangChain architecture overview, LangGraph concepts.',
    estimatedHours: 4,
    sortOrder: 20,
    concepts: ['langchain-architecture', 'framework-tradeoffs'],
    prerequisites: ['react-pattern', 'rag-basics'],
  },
  {
    id: 'autogen-videos',
    title: 'Microsoft AutoGen - Multi-Agent Systems',
    type: 'video',
    url: 'https://www.youtube.com/@MicrosoftResearch',
    author: 'Microsoft Research',
    phase: 9,
    description: 'AutoGen multi-agent systems, agent collaboration patterns.',
    estimatedHours: 4,
    sortOrder: 30,
    concepts: ['langchain-architecture', 'framework-tradeoffs'],
    prerequisites: ['react-pattern', 'plan-and-execute'],
  },
  {
    id: 'llamaindex-docs',
    title: 'LlamaIndex Documentation (Internals)',
    type: 'article',
    url: 'https://docs.llamaindex.ai/',
    phase: 9,
    description: 'Focus on how indexes and query engines work internally.',
    estimatedHours: 4,
    sortOrder: 40,
    concepts: ['llamaindex-architecture'],
    prerequisites: ['rag-basics', 'chunking-strategies'],
  },
  {
    id: 'dspy-paper',
    title: 'DSPy: Compiling Declarative Language Model Calls into Self-Improving Pipelines',
    type: 'paper',
    url: 'https://arxiv.org/abs/2310.03714',
    author: 'Khattab et al. (Stanford)',
    phase: 9,
    description: 'ICLR 2024. Framework for programming (not prompting) LLMs.',
    estimatedHours: 4,
    sortOrder: 45,
    concepts: ['dspy-programming'],
    prerequisites: ['prompt-engineering', 'compound-ai-systems'],
  },
  {
    id: 'dspy-docs',
    title: 'DSPy Documentation & Examples',
    type: 'article',
    url: 'https://dspy.ai/',
    author: 'Stanford NLP',
    phase: 9,
    description: 'Official DSPy docs. Signatures, modules, and teleprompters.',
    estimatedHours: 4,
    sortOrder: 48,
    concepts: ['dspy-programming'],
    prerequisites: ['dspy-programming'],
  },
  {
    id: 'rag-from-scratch',
    title: 'Build RAG From Scratch (No Frameworks)',
    type: 'article',
    url: 'https://huggingface.co/blog/ngxson/make-your-own-rag',
    phase: 9,
    description: 'Implement RAG in ~200 lines without frameworks. Proves you understand the mechanics.',
    estimatedHours: 4,
    sortOrder: 50,
    concepts: ['minimal-implementations', 'framework-tradeoffs'],
    prerequisites: ['rag-basics', 'embeddings', 'vector-search'],
  },
  {
    id: 'rag-without-frameworks',
    title: 'RAG Applications Without LangChain or LlamaIndex',
    type: 'article',
    url: 'https://blog.futuresmart.ai/building-rag-applications-without-langchain-or-llamaindex',
    phase: 9,
    description: 'Production-ready RAG with just ChromaDB and OpenAI API. No black boxes.',
    estimatedHours: 3,
    sortOrder: 60,
    concepts: ['minimal-implementations', 'framework-tradeoffs'],
    prerequisites: ['rag-basics', 'vector-search'],
  },
  {
    id: 'minimal-agent-loop',
    title: 'Minimal Agent Loop Implementation',
    type: 'article',
    url: 'https://github.com/pguso/rag-from-scratch',
    phase: 9,
    description: 'Build agents from the ground up. Demystifies what frameworks actually do.',
    estimatedHours: 4,
    sortOrder: 70,
    concepts: ['minimal-implementations', 'framework-tradeoffs'],
    prerequisites: ['react-pattern', 'tool-use'],
  },
  {
    id: 'applied-llms-blog',
    title: 'Applied LLMs: What We\'ve Learned From a Year of Building',
    type: 'article',
    url: 'https://applied-llms.org/',
    author: 'Eugene Yan et al.',
    phase: 9,
    description: 'Practitioner guide from senior engineers. Covers system design patterns, cost-quality-latency trade-offs, production architectures. Must-read for AI architects.',
    estimatedHours: 3,
    sortOrder: 75,
    concepts: ['system-design-patterns', 'cost-quality-latency', 'production-architectures'],
    prerequisites: ['compound-ai-systems'],
  },
  {
    id: 'openclaw-casestudy',
    title: 'OpenClaw: Arquitectura de un Sistema de Agentes de IA en Producción',
    type: 'article',
    url: 'https://github.com/openclaw/openclaw',
    author: 'Peter Steinberger',
    phase: 9,
    description: 'Case study arquitectónico del sistema OpenClaw (145k+ stars). Analiza: Agent Communication Protocol (ACP), plugin architecture para multi-channel, sistema de skills, memoria persistente con vector search, y A2UI para generación declarativa de UIs por agentes.',
    estimatedHours: 6,
    sortOrder: 80,
    concepts: ['agent-protocol-design', 'plugin-channel-architecture', 'agent-skill-orchestration', 'agent-memory-persistence', 'agent-ui-generation'],
    prerequisites: ['system-design-patterns', 'tool-use', 'external-memory', 'production-architectures'],
  },
];

// ============================================================================
// PROJECTS (Practical Application)
// ============================================================================

export const projects = [
  {
    id: 'project-kv-store',
    title: 'Distributed Key-Value Store',
    phase: 1 as StudyPhase,
    description: 'Build a simple distributed KV store with replication. Simulate network partitions and observe behavior.',
    deliverables: [
      'Leader-based replication working',
      'Handles leader failure gracefully',
      'Can demonstrate split-brain scenario',
      'Written explanation of consistency trade-offs observed',
    ],
  },
  {
    id: 'project-ml-pipeline',
    title: 'ML Training Pipeline',
    phase: 2 as StudyPhase,
    description: 'Build a data pipeline that prepares training data, tracks experiments, and serves a model. Integrate distributed training concepts.',
    deliverables: [
      'Data pipeline with versioning',
      'Experiment tracking with metrics comparison',
      'Model serving endpoint with monitoring',
      'Written analysis of GPU utilization and bottlenecks',
    ],
  },
  {
    id: 'project-transformer',
    title: 'Transformer from Scratch',
    phase: 3 as StudyPhase,
    description: 'Implement a small Transformer model from scratch. Train it on a text dataset and analyze attention patterns.',
    deliverables: [
      'Working multi-head attention implementation',
      'Positional encoding (sinusoidal and RoPE)',
      'Training loop with loss tracking',
      'Attention visualization and analysis',
    ],
  },
  {
    id: 'project-react-agent',
    title: 'Self-Improving Coding Agent',
    phase: 4 as StudyPhase,
    description: 'Build an autonomous coding agent that follows the plan-execute-test-reflect loop. The agent receives a coding task, plans an approach, writes code in a sandboxed environment, runs tests, and reflects on failures to improve. Must implement memory hierarchy and structured output for all tool interactions. No LangChain or similar frameworks allowed.',
    deliverables: [
      'Agentic loop: plan → code → test → reflect cycle with observable iteration history',
      'Sandboxed code execution (Docker or subprocess) with timeout and resource limits',
      'Two-level memory: session scratchpad + persistent reflection store of past failures',
      'Structured JSON output validated with schema for 4+ tools (code_execute, file_read, file_write, test_run)',
      'Traceability dashboard: visible log of each loop step with reasoning, action, and observed result',
      'Reflection mechanism: on test failure, agent generates written error analysis and uses it in next attempt',
      'Written doc: architecture diagram + analysis of 3+ observed failures and how the agent handled them',
    ],
  },
  {
    id: 'project-rag-system',
    title: 'Personal Knowledge Agent with Memory',
    phase: 5 as StudyPhase,
    description: 'Build a knowledge management system that ingests documents (PDF, markdown, web pages), builds a knowledge graph alongside vector embeddings, and provides Q&A with memory consolidation. Must demonstrate multiple retrieval strategies, memory lifecycle management, and RAG evaluation metrics.',
    deliverables: [
      'Ingestion pipeline for 3+ formats with 3 chunking strategies implemented and compared',
      'Dual retrieval: vector search (embeddings + cosine) + knowledge graph (entities and relations)',
      'Hybrid search: dense (vector) + sparse (BM25) with score normalization and fusion',
      'Memory lifecycle: storage → retrieval → consolidation → decay, with metrics per stage',
      'Quantitative evaluation with RAGAS: faithfulness > 0.7, context precision > 0.6, on 20+ test queries',
      'Query decomposition: complex queries automatically split into sub-queries with logged reasoning',
      'Reranking: cross-encoder reranker stage after initial retrieval with measured improvement',
      'Written analysis: RAG vs long-context comparison for 5 representative queries with cost and quality metrics',
    ],
  },
  {
    id: 'project-multimodal-rag',
    title: 'Multimodal RAG System',
    phase: 6 as StudyPhase,
    description: 'Build a RAG system that handles text, images, and tables. Cross-modal retrieval and generation.',
    deliverables: [
      'Image + text retrieval working',
      'Table extraction and querying',
      'Cross-modal search (text query → image results)',
      'Performance comparison: multimodal vs text-only RAG',
    ],
  },
  {
    id: 'project-validators',
    title: 'LLM Output Validators',
    phase: 7 as StudyPhase,
    description: 'Build a validation layer for LLM outputs. Include guardrails pipeline, evaluation harness, and content moderation.',
    deliverables: [
      'JSON schema validator with error recovery',
      'Multi-layer guardrails pipeline (NeMo or Instructor)',
      'Evaluation harness with LLM-as-judge',
      'Content moderation filters',
      'Metrics: false positive/negative rates',
    ],
  },
  {
    id: 'project-router',
    title: 'Intelligent Model Router with Edge Optimization',
    phase: 8 as StudyPhase,
    description: 'Build a model routing system that classifies requests by complexity and routes to the optimal model considering cost, quality, and latency. Include edge optimization: quantize a small model for local inference and compare with cloud APIs. Implement observability, semantic caching, and cost tracking.',
    deliverables: [
      'Query complexity classifier (feature-based, not LLM-based) with 3+ tiers: trivial, medium, complex',
      'Routing to 3 tiers: quantized local model (INT4/INT8), cloud economic model, cloud premium model',
      'Quantized SLM (< 3B params) with measured quality degradation vs original on 50+ query benchmark',
      'Semantic caching: cache responses by embedding similarity with configurable threshold and hit rate metrics',
      'Observability dashboard: latency per tier (p50, p95, p99), cumulative cost, tokens consumed, cache hit rate',
      'Cost-quality-latency analysis: comparative table of 3 tiers across 100+ test queries',
      'Rate limiting and backpressure: graceful degradation when premium tier is saturated',
      'Decision log: documented analysis of 10+ routing decisions explaining why each query went to each tier',
    ],
  },
  {
    id: 'project-system-design',
    title: 'Enterprise Workflow Orchestrator',
    phase: 9 as StudyPhase,
    description: 'Design and build a multi-agent workflow orchestration system for a concrete scenario (e.g., automated incident response or document processing pipeline). Must demonstrate production-grade patterns: supervisor-worker agents, audit trail, observability, guardrails, cost tracking, and graceful degradation. This is the capstone that integrates ALL prior phases.',
    deliverables: [
      'Multi-agent architecture: supervisor delegates to 3+ specialized worker agents with defined protocol',
      'Complete audit trail: every decision, tool call, and result logged with timestamp, agent, I/O, tokens',
      'Guardrails pipeline: input validation (prompt injection defense) + output validation (schema + safety)',
      'End-to-end observability: distributed tracing across agents with per-component latency and cost',
      'Graceful degradation: fallback to smaller model or HITL checkpoint on agent failure',
      'Cost-quality-latency analysis: 3 configurations (quality-optimized, balanced, cost-optimized) measured on 20+ workflows',
      'Framework comparison: implement without LangChain/LlamaIndex, then analyze what those frameworks solve and what they don\'t',
      'Architecture document: C4 diagram, decision log of every trade-off, comparison with real systems (Cursor, Perplexity)',
    ],
  },
];

// ============================================================================
// PROJECT-CONCEPT MAPPINGS (Which concepts advance to Level 2 per project)
// ============================================================================

export const projectConcepts: Array<{ projectId: string; conceptId: string }> = [
  // Phase 1: Distributed KV Store (5 conceptos)
  { projectId: 'project-kv-store', conceptId: 'replication' },
  { projectId: 'project-kv-store', conceptId: 'partitioning' },
  { projectId: 'project-kv-store', conceptId: 'distributed-failures' },
  { projectId: 'project-kv-store', conceptId: 'consistency-models' },
  { projectId: 'project-kv-store', conceptId: 'consensus' },

  // Phase 2: ML Training Pipeline (5 conceptos)
  { projectId: 'project-ml-pipeline', conceptId: 'gpu-compute-fundamentals' },
  { projectId: 'project-ml-pipeline', conceptId: 'data-parallelism' },
  { projectId: 'project-ml-pipeline', conceptId: 'distributed-training' },
  { projectId: 'project-ml-pipeline', conceptId: 'ml-data-pipelines' },
  { projectId: 'project-ml-pipeline', conceptId: 'experiment-tracking' },

  // Phase 3: Transformer from Scratch (5 conceptos)
  { projectId: 'project-transformer', conceptId: 'attention-mechanism' },
  { projectId: 'project-transformer', conceptId: 'transformer-architecture' },
  { projectId: 'project-transformer', conceptId: 'query-key-value' },
  { projectId: 'project-transformer', conceptId: 'positional-encoding' },
  { projectId: 'project-transformer', conceptId: 'scaling-laws' },

  // Phase 4: Self-Improving Coding Agent (6 conceptos)
  { projectId: 'project-react-agent', conceptId: 'react-pattern' },
  { projectId: 'project-react-agent', conceptId: 'reflexion' },
  { projectId: 'project-react-agent', conceptId: 'tool-use' },
  { projectId: 'project-react-agent', conceptId: 'plan-and-execute' },
  { projectId: 'project-react-agent', conceptId: 'structured-output' },
  { projectId: 'project-react-agent', conceptId: 'agent-reliability' },

  // Phase 5: Personal Knowledge Agent (11 conceptos)
  { projectId: 'project-rag-system', conceptId: 'embeddings' },
  { projectId: 'project-rag-system', conceptId: 'vector-search' },
  { projectId: 'project-rag-system', conceptId: 'rag-basics' },
  { projectId: 'project-rag-system', conceptId: 'chunking-strategies' },
  { projectId: 'project-rag-system', conceptId: 'hybrid-search' },
  { projectId: 'project-rag-system', conceptId: 'external-memory' },
  { projectId: 'project-rag-system', conceptId: 'memory-management' },
  { projectId: 'project-rag-system', conceptId: 'graph-rag' },
  { projectId: 'project-rag-system', conceptId: 'reranking' },
  { projectId: 'project-rag-system', conceptId: 'query-decomposition' },
  { projectId: 'project-rag-system', conceptId: 'rag-evaluation' },

  // Phase 6: Multimodal RAG (4 conceptos)
  { projectId: 'project-multimodal-rag', conceptId: 'multimodal-embeddings' },
  { projectId: 'project-multimodal-rag', conceptId: 'vision-language-models' },
  { projectId: 'project-multimodal-rag', conceptId: 'multimodal-rag' },
  { projectId: 'project-multimodal-rag', conceptId: 'knowledge-distillation' },

  // Phase 7: LLM Output Validators (6 conceptos)
  { projectId: 'project-validators', conceptId: 'output-validation' },
  { projectId: 'project-validators', conceptId: 'prompt-injection' },
  { projectId: 'project-validators', conceptId: 'llm-security-owasp' },
  { projectId: 'project-validators', conceptId: 'guardrails-libraries' },
  { projectId: 'project-validators', conceptId: 'content-moderation' },
  { projectId: 'project-validators', conceptId: 'eval-harnesses' },

  // Phase 8: Edge AI Model Router (9 conceptos)
  { projectId: 'project-router', conceptId: 'quantization' },
  { projectId: 'project-router', conceptId: 'kv-cache' },
  { projectId: 'project-router', conceptId: 'batching-inference' },
  { projectId: 'project-router', conceptId: 'token-economics' },
  { projectId: 'project-router', conceptId: 'model-routing' },
  { projectId: 'project-router', conceptId: 'semantic-caching' },
  { projectId: 'project-router', conceptId: 'llm-observability' },
  { projectId: 'project-router', conceptId: 'rate-limiting' },
  { projectId: 'project-router', conceptId: 'compound-ai-systems' },

  // Phase 9: Enterprise Workflow Orchestrator (8 conceptos = 100% fase)
  { projectId: 'project-system-design', conceptId: 'system-design-patterns' },
  { projectId: 'project-system-design', conceptId: 'production-architectures' },
  { projectId: 'project-system-design', conceptId: 'cost-quality-latency' },
  { projectId: 'project-system-design', conceptId: 'compound-ai-systems' },
  { projectId: 'project-system-design', conceptId: 'multi-agent-systems' },
  { projectId: 'project-system-design', conceptId: 'minimal-implementations' },
  { projectId: 'project-system-design', conceptId: 'framework-tradeoffs' },
  { projectId: 'project-system-design', conceptId: 'llm-observability' },
];

// ============================================================================
// HELPER: Get all data
// ============================================================================

export function getSeedData() {
  return {
    concepts,
    resources,
    projects,
    projectConcepts,
  };
}

// ============================================================================
// CLI: Print summary
// ============================================================================

if (require.main === module) {
  console.log('=== Jarre Seed Data Summary ===\n');

  console.log(`Concepts: ${concepts.length}`);
  for (let phase = 1; phase <= 9; phase++) {
    const count = concepts.filter(c => c.phase === phase).length;
    console.log(`  Phase ${phase}: ${count} concepts`);
  }

  console.log(`\nResources: ${resources.length}`);
  for (let phase = 1; phase <= 9; phase++) {
    const count = resources.filter(r => r.phase === phase).length;
    console.log(`  Phase ${phase}: ${count} resources`);
  }

  console.log(`\nProjects: ${projects.length}`);
  for (const p of projects) {
    const conceptCount = projectConcepts.filter(pc => pc.projectId === p.id).length;
    console.log(`  Phase ${p.phase}: ${p.title} (${conceptCount} concepts)`);
  }

  console.log(`\nProject-Concept Mappings: ${projectConcepts.length}`);

  console.log('\n=== Ready to seed database ===');
}
